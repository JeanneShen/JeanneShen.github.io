[{"title":"Hello World","url":"/2022/09/20/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"Stream","url":"/2023/02/19/GPU/Stream/","content":"<ul>\n<li><code>cudaMemcpy</code>顺序化数据传输和GPU计算：数据传输时，单向使用，GPU inactif状态；GPU计算是，CPU处于未激活状态</li>\n<li>一些cuda设备允许在设备和主机之间复制数据的同时执行内核(<em>Overlap</em>)<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">int dev_count;</span><br><span class=\"line\">cudaDeviceProp prop ;</span><br><span class=\"line\">cudaGetDeviceCount (&amp;dev_count );</span><br><span class=\"line\">for (int i = 0; i &lt; dev_count ; i ++) &#123;</span><br><span class=\"line\">    cudaGetDeviceProperties (&amp; prop , i);</span><br><span class=\"line\">     if (prop.deviceOverlap)  // 查看是否允许同时运行CPU和GPU</span><br><span class=\"line\">     ...</span><br></pre></td></tr></table></figure></li>\n<li>在<em>pipeline</em>中运行<ul>\n<li>将大的向量分成片段</li>\n<li>将相邻段的数据传输和计算叠加<br><a href=\"https://postimg.cc/87pn1z7g\"><img src=\"https://i.postimg.cc/g289Hwdj/image.png\" alt=\"pipeline\"></a></li>\n</ul>\n</li>\n<li>同步和异步：所有CUDA的调用相对于host要么同步要么异步<ul>\n<li>同步：放在等待运行队列，等待运行结束</li>\n<li>异步：放在等待运行队列，立即返回<br>Kernel 的启动是异步的，并自动与主机重叠(Overlap)</li>\n</ul>\n</li>\n<li>Cuda的<strong>流(Stream)</strong>:CUDA 支持使用流并行执行内核和 cudaMemcpy()<ul>\n<li>每个流都是Device上的操作队列<ul>\n<li>Hote将工作放在队列中，然后立即继续</li>\n<li>Device在资源可用时，在流上安排工作</li>\n</ul>\n</li>\n<li>不同流上的操作&#x2F;任务可以并行执行&#x2F;任务并行</li>\n<li>stream的执行顺序：Hote上代码的请求放在FIFO的队列中<ul>\n<li>队列由驱动程序和设备异步读取和处理。</li>\n<li>驱动程序确保队列中的操作按顺序处理。 例如，内存复制在内核启动之前终止等。</li>\n<li>为了实现复制和执行的同时进行，需要使用多个流</li>\n<li>CUDA 事件允许主机线程询问和同步各个队列（即流）<br><a href=\"https://postimg.cc/9RYL53cQ\"><img src=\"https://i.postimg.cc/63DgZwSn/image.png\" alt=\"stream\"></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"流的控制\"><a href=\"#流的控制\" class=\"headerlink\" title=\"流的控制\"></a>流的控制</h3><ul>\n<li>声明流控制：<code>cudaStream_t stream</code>;</li>\n<li>流的分配：<code>cudaStreamCreate(&amp;stream)</code>;</li>\n<li>取消流分：<code>cudaStreamDestroy(&amp;stream)</code>;</li>\n<li>流是GPU运行的第四个参数：<code>Kernel&lt;&lt;&lt;blocks, thread,smem,stream&gt;&gt;&gt;</code>;</li>\n<li>流在一些 API 调用中传递：<code>cudaMemcpyAsync(目标,来源,size,方向,stream)</code>;</li>\n</ul>\n<ol>\n<li>除非另有说明，否则所有调用都放在默认流中，通常称为“stream 0”。流0有特殊的同步规则：</li>\n</ol>\n<ul>\n<li>与所有流同步：流 0 中的操作不能与其他流重叠  <figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">  cudaStream_t stream1;</span><br><span class=\"line\">cudaStreamCreate (&amp;stream1 );</span><br><span class=\"line\">foo&lt;&lt;&lt; blocks,threads &gt;&gt;&gt;(); <span class=\"comment\">// 默认流</span></span><br><span class=\"line\">foo&lt;&lt;&lt; blocks,threads,<span class=\"number\">0</span>,stream1&gt;&gt;&gt;(); <span class=\"comment\">// 此时没有竞争</span></span><br><span class=\"line\">cudaStreamDestroy(stream1);</span><br></pre></td></tr></table></figure></li>\n<li>例外：分配具有非阻塞标志的流。用于与无法控制的bibliotheque竞争  <figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">  cudaStream_t stream1 ;</span><br><span class=\"line\">cudaStreamCreateWithFlags(&amp;stream1,cudaStreamNonBlocking); <span class=\"comment\">//声明特殊流</span></span><br><span class=\"line\">foo&lt;&lt;&lt;blocks,threads&gt;&gt;&gt;(); <span class=\"comment\">// 默认流</span></span><br><span class=\"line\">foo&lt;&lt;&lt;blocks,threads,<span class=\"number\">0</span>,stream1&gt;&gt;&gt;(); <span class=\"comment\">// 执行竞争</span></span><br><span class=\"line\">cudaStreamDestroy ( stream1 );</span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"2\">\n<li>同步</li>\n</ol>\n<ul>\n<li>同步所有：阻止hote只到cuda调用全部完成 <code>cudaDeviceSynchronize()</code></li>\n<li>同步hote和特定flux：<code>cudaStreamSynchronize(stream)</code></li>\n</ul>\n"},{"title":"Wraps","url":"/2023/02/19/GPU/Wraps/","content":"<p>Warp和Zero-overhead线程调度</p>\n<h1 id=\"1-Warp\"><a href=\"#1-Warp\" class=\"headerlink\" title=\"1. Warp\"></a>1. Warp</h1><h2 id=\"1-0-Scalability-invisible\"><a href=\"#1-0-Scalability-invisible\" class=\"headerlink\" title=\"1.0 Scalability (invisible)\"></a>1.0 Scalability (invisible)</h2><p>可拓展性和不可见</p>\n<p>Chaque bloc peut s’exécuter dans n’importe quel ordre par rapport aux autres. 每个bloc可以以相对于其他bloc以任何顺序运行。&#x3D;&#x3D;&gt;块与块间的执行不受影响？</p>\n<p>Le matériel est libre d’affecter des blocs à n’importe quel processeur à tout moment. 硬件可以在任何时候自由地将bloc分配给任何处理器。</p>\n<p>Un kernel s’adapte à n’importe quel nombre de processeurs parallèles.一个kernel适合任何数量的并行处理器。</p>\n<h2 id=\"1-1-Warp\"><a href=\"#1-1-Warp\" class=\"headerlink\" title=\"1.1 Warp\"></a>1.1 Warp</h2><h3 id=\"1-1-1-Unite-d’ordonnancement-Warp\"><a href=\"#1-1-1-Unite-d’ordonnancement-Warp\" class=\"headerlink\" title=\"1.1.1 Unité d’ordonnancement : Warp\"></a>1.1.1 Unité d’ordonnancement : Warp</h3><p>调度单元：Warp</p>\n<ul>\n<li>Chaque bloc est exécuté par un Warps de 32-threads (de valeurs consécutives[连续的] threadIdx : les thread 0 à 31 forment le premier warp, 32 à 63 le second, etc.)</li>\n<li>是SM里面的调度单元[ unités d’ordonnancement]<ul>\n<li>调度: 在计算机中是分配工作所需资源的方法。资源可以指虚拟的计算资源，如线程、进程或数据流；也可以指硬件资源，如处理器、网络连接或扩展卡。在操作系统中，调度程序是操作系统内核的组件，它选择进程在计算机处理器上的执行顺序</li>\n<li>SM:</li>\n</ul>\n</li>\n<li>在cuda编程里面没有实现</li>\n<li>一个warp线程是SIMD结构：Les threads d’un warp appliquent la <strong>même instruction</strong> à <strong>différentes parties des données</strong> et tous les threads d’un warp auront toujours le <strong>même temps d’exécution</strong>.</li>\n</ul>\n<p>Exemple de Warp</p>\n<p>Si 3 blocs sont attribués à un SM et que chaque bloc comporte 256 threads, combien de Warps y a-t-il dans un SM ?&#x3D;&#x3D;&gt;$256&#x2F;32*3&#x3D;24$</p>\n<h3 id=\"1-1-2-使用warp的好处\"><a href=\"#1-1-2-使用warp的好处\" class=\"headerlink\" title=\"1.1.2 使用warp的好处\"></a>1.1.2 使用warp的好处</h3><p>Permet d’exécuter efficacement <strong>les opérations avec des longues latences</strong>, comme les accès globaux à la mémoire (ou les instructions arithmétiques à virgule flottante, les branchements de pipeline, etc.) 允许有效地执行具有长延迟的操作，如全局内存访问（或浮点运算指令、流水线分支等）。</p>\n<ul>\n<li><p>Une instruction devant être exécutée par un warp qui doit attendre le résultat d’une opération avec une longue latence alors le warp n’est pas sélectionné pour l’exécution.</p>\n<p>一条要由warp执行的指令，必须等待具有长延迟的操作结果，那么该warp就不会被选择执行。</p>\n</li>\n<li><p>un autre warp qui n’attend plus de résultats sera sélectionné pour l’exécution.</p>\n<p> 另一个不再等待结果的warp将被选择执行。</p>\n</li>\n<li><p>si plus d’un warp est prêt à être exécuté, un mécanisme de priorité est utilisé pour en sélectionner un.</p>\n</li>\n</ul>\n<p>  如果有一个以上的warp准备被执行，则使用优先机制来选择其中一个。</p>\n<p>这种用其他线程的工作来填补操作的延迟的机制通常被称为 “延迟容忍 “或 “延迟隐藏”[ “latency tolerance” &#x2F; “latency hiding” ]</p>\n<h3 id=\"1-1-3-Ordonnancement-Zero-overhead-thread\"><a href=\"#1-1-3-Ordonnancement-Zero-overhead-thread\" class=\"headerlink\" title=\"1.1.3 Ordonnancement Zero-overhead thread\"></a>1.1.3 Ordonnancement Zero-overhead thread</h3><p>定义：如果有足够数量的warps，硬件就有可能在任何时候找到一个warps来执行，这允许在这些长延迟操作的情况下充分利用执行硬件。选择准备执行的warp可以避免在执行时间线上引入空闲或浪费的时间，这被称为零开销的线程调度[ l’ordonnancement Zero-overhead thread]。</p>\n<p>例子: Supposons qu’un device autorise[允许] jusqu’à 8 blocs et 1024 threads par SM et il permet jusqu’à 512 threads dans chaque bloc. Pour le traitement d’une image doit-on utiliser des blocs de 8×8, 16×16, ou 32×32 threads ?</p>\n<ul>\n<li>8*8 : 每个块中64个线程，需要1024&#x2F;64&#x3D;12个blocs，但每个SM只允许8个。</li>\n<li>16*16：每个块中256个线程，需要1024&#x2F;256&#x3D;4个blocs，正好。</li>\n<li>32*32：每个块中1024个线程超过512的限制了</li>\n</ul>\n<p>没有理解这个例子和零开销的线程调度的关系</p>\n<h1 id=\"2-Detour-sur-la-virgule-flottante\"><a href=\"#2-Detour-sur-la-virgule-flottante\" class=\"headerlink\" title=\"2. Détour sur la virgule flottante\"></a>2. Détour sur la virgule flottante</h1><h2 id=\"2-1-Le-format-标准-a-virgule-flottante\"><a href=\"#2-1-Le-format-标准-a-virgule-flottante\" class=\"headerlink\" title=\"2.1  Le format[标准] à virgule flottante\"></a>2.1  Le format[标准] à virgule flottante</h2><p>IEEE754-2008</p>\n<p>制定这个标准是为了：确保来自不同供应商的材料产生一致的结果</p>\n<p>二进制浮点数的组成:</p>\n<ul>\n<li>符号位<strong>S</strong>[0正1负]，指数<strong>E</strong>[大小，entier relatif, 通常有界]和尾数<strong>M</strong>[精度]</li>\n<li>$valeur &#x3D; (-1)^S·M·2^E\\ \\ \\ 1.0&lt;&#x3D;M&lt;10.0_B$，比如说$0.5_D&#x3D;1.0_B·2^{-1}$. 由于所有的尾数都是1.XX…的形式，我们可以省略 “1. “部分（这被称为隐含位）。比如说上面的$0.5_D$的尾数可以表示成00</li>\n</ul>\n<h2 id=\"2-2-Representation-de-l’exposant\"><a href=\"#2-2-Representation-de-l’exposant\" class=\"headerlink\" title=\"2.2 Représentation de l’exposant\"></a>2.2 Représentation de l’exposant</h2><p><strong>负数：</strong>正数的反码+1，比如-4 sur 8 bits:</p>\n<p> <img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.1aj6n7p8mqps.png\" alt=\"yaimage\"></p>\n<ul>\n<li>-0就是0：$ 1111 1111_B + 1&#x3D;0000 0000_B$</li>\n<li>减法是正负的加<ul>\n<li>举得例子有点怪：$3+(-4)&#x3D;0011_B+1100_B&#x3D;1111_B&#x3D;-1$, 就是说-1的二进制表示实际是1111[不过确实1的反码加1]</li>\n</ul>\n</li>\n</ul>\n<p>Pour un exposant E de n bits[对于一个n位的指数E]，on ajoute $2^{n−1} − 1$à la représentation de <strong>son complément à 2[它的2进制补码]</strong> (exemple pour une représentation de l’exposant sur 3 bits) .</p>\n<ul>\n<li>二进制数系统中，每个0或1就是一个位(bit)，位是数据存储的最小单位</li>\n<li>补码：正数补码即原码，负数补码反码+1</li>\n</ul>\n<p>En général, avec une <strong>mantisse normalisée</strong> et un <strong>exposant codé en excès</strong>, la valeur d’un nombre avec un exposant de n bits est : $(−1)S · 1.M · 2^{ (E−(2^{(n−1)}−1))}$</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.20awui5okaf4.png\" alt=\"yaimage\"></p>\n<p>这个在说n&#x3D;3然后E[指数]都给加个011？</p>\n<p><strong>Exemple sur un format 5-bit flottant</strong></p>\n<p>Supposons un format avec 1bit pour S, 2 bits pour E et 2 bits pour M</p>\n<p>$0.5_D&#x3D;1.00_B*2^{-1}&#x3D;0 \\ 00\\ 00\\ \\ \\ avec\\ S&#x3D;0,E&#x3D;00,M&#x3D;(1.)00$</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.31xkvxmijq00.png\" alt=\"yaimage\"></p>\n<p>这里又不对了，01怎么对11哦</p>\n<h2 id=\"2-3-Nombres-representables\"><a href=\"#2-3-Nombres-representables\" class=\"headerlink\" title=\"2.3 Nombres représentables\"></a>2.3 Nombres représentables</h2><ol>\n<li><p>Nombres représentables en 5 bits<img src=\"C:\\Users\\nili990221\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230109002338628.png\" alt=\"image-20230109002338628\"></p>\n<ul>\n<li><strong>指数的位数</strong>[E的bits]定义了可表示数的主要区间。主要的间隔[intervalles]是在2的幂之间</li>\n<li><strong>尾数</strong>位定义了每个主要区间的代表数字的数量。因此，尾数的位数决定了表述的<strong>准确性</strong></li>\n<li>这是一个理想的趋势，因为这些数字的绝对值越小，准确表达它们就越重要</li>\n<li>可表示的数字的密度增加的趋势，以及在接近0时表示数字的准确性增加的趋势，在紧邻0的地方并不适用。</li>\n</ul>\n</li>\n<li><p>Mise à zéro</p>\n<p>删除了几个接近零的可表示数，并将它们全部分组为0.0</p>\n<p><img src=\"C:\\Users\\nili990221\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230109002302967.png\" alt=\"image-20230109002302967\"></p>\n</li>\n<li><p>Nombres dénormalisés</p>\n<p>放宽了对非常接近0的数字的归一化要求,当E &#x3D; 0时，尾数M不再被认为是形式1.XX但是是0.XX，值为$0.M*2^{-2^{(n-1)}+2}$<img src=\"C:\\Users\\nili990221\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230109002324646.png\" alt=\"image-20230109002324646\"></p>\n</li>\n</ol>\n<h2 id=\"2-4-精度\"><a href=\"#2-4-精度\" class=\"headerlink\" title=\"2.4 精度\"></a>2.4 精度</h2><p>分类：</p>\n<ul>\n<li><p>单精度：32位，1个符号位，8个指数位，23个尾数位</p>\n</li>\n<li><p>双精度：64位，1个符号位，11个指数位，52个尾数位。在双精度中，表示数字的最大误差减少到单精度表示的1&#x2F;229。</p>\n</li>\n</ul>\n<p>Arrondi</p>\n<ul>\n<li>浮点算术运算的精度是由该运算引入的最大误差来衡量的。</li>\n<li>浮点算术中最常见的错误来源是，该操作产生的结果不能精确表示，因此必须舍入[arrondi]</li>\n<li>当值的保持必须在太多的位上表示才能准确时，就会发生舍入[arrondi]</li>\n</ul>\n<p>我们假设一个5-bits的加法运算</p>\n<ul>\n<li>$1.00_B<em>2^{-2}+1.00_B</em>2^1$&#x3D;&#x3D;&#x3D;&gt;$(0,00,01)+(0,10,00)$ &#x3D;&#x3D;&gt;这个尾数咋回事？</li>\n<li>Le matériel doit décaler les bits de la mantisse pour les aligner[对齐]，对齐后为$0.001_B<em>2^1+1.00_B</em>2^1&#x3D;1.001_B*2^1$,这在尾数M的2-bits中无法表示</li>\n<li>最多只能表示成$1.00_B<em>2^1$或者是$1.01_B</em>2^1$,但都差很多</li>\n</ul>\n<p>浮点运算不是严格意义上的关联性[ strictement associatives]，主要原因是，有时一个小数字与一个非常大的数字相加或相减后会消失：(Grand + Petit) + Petit 6&#x3D; Grand + (Petit + Petit)，例子：$1.00_B\\cdot2^0+1.00_B\\cdot2^0+1.00_B\\cdot2^{-2}+1.00_B\\cdot2^{-2}$</p>\n<ul>\n<li><p>时序<br>$$<br>\\begin{align*}\\label{2}<br>&amp;1.00_B · 2^0 + 1.00_B · 2^0 + 1.00_B · 2^{−2} + 1.00_B · 2^{−2}\\<br>&amp;&#x3D; 1.00_B · 2^1 + 1.00_B · 2^{−2} + 1.00_B · 2^{−2}\\<br>&amp;&#x3D; 1.00_B · 2^1 + 1.00_B · 2^{−2}(arrondi)\\<br>&amp;&#x3D; 1.00_B · 2^1(arrondi)\\<br>\\end{align*}<br>$$</p>\n</li>\n<li><p>并行<br>$$<br>\\begin{align*}\\label{1}<br>&amp;(1.00_B · 2^0 + 1.00_B · 2^0) + (1.00_B · 2^{−2} + 1.00_B · 2^{−2})\\<br>&amp;&#x3D; 1.00_B · 2^1 + 1.00_B · 2^{−1}\\<br>&amp;&#x3D; 1.01_B · 2^1<br>\\end{align*}<br>$$</p>\n</li>\n</ul>\n<p>一个解决方法是对其进行预排序[pré-trier]，比如上面那个例子<br>$$<br>\\begin{align*}\\label{0}<br>&amp;1.00_B · 2^{−2} + 1.00_B · 2^{−2}+1.00_B · 2^0 + 1.00_B · 2^0\\<br>&amp;&#x3D; 1.00_B · 2^{-1} + 1.00_B · 2^{0}+ 1.00_B · 2^{0}\\<br>&amp;&#x3D; 1.10_B · 2^0+ 1.00_B · 2^{0}\\<br>&amp;&#x3D;1.01_B\\cdot2^1<br>\\end{align*}<br>$$</p>\n"},{"title":"课程安排和GPU基本知识介绍","url":"/2023/02/19/GPU/%E4%BB%8B%E7%BB%8D/","content":"<p>作者：nililili（nililili7876.cn）</p>\n<h2 id=\"课程安排\"><a href=\"#课程安排\" class=\"headerlink\" title=\"课程安排\"></a>课程安排</h2><p>Généralités sur la programmation sur GPU</p>\n<ul>\n<li>programmation concurrente vs parallèle<ul>\n<li>concurrente: 并发，为了更好地管理多个任务的执行，快速转换执行多个任务，在同一处理器的同一时间间隔的不同时刻</li>\n<li>parallèle: 并行，多个事件在同一时刻发生，通常是在不同的处理器或核心上进行–&gt;GPU</li>\n</ul>\n</li>\n<li>principe et architecture des GPU</li>\n</ul>\n<p>Opérateurs, mémoires et structurations des données</p>\n<ul>\n<li>allocation des données[数据分配] et flot[流] de contrôle</li>\n<li>types de mémoire et localité des données[数据位置]</li>\n<li>opérations atomiques et synchronisation</li>\n</ul>\n<p>Etude de patterns d’algorithme pour la programmation parallèles<br>Exemple d’application : la bibliothèque CuDNN</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"Architecture-parallele\"><a href=\"#Architecture-parallele\" class=\"headerlink\" title=\"Architecture parallèle\"></a>Architecture parallèle</h3><h4 id=\"Taxonomie-de-Flynn\"><a href=\"#Taxonomie-de-Flynn\" class=\"headerlink\" title=\"Taxonomie de Flynn\"></a>Taxonomie de Flynn</h4><p>Taxonomie de Flynn[费林分类]：根据信息流把指令分为指令和数据两部分，然后在此基础上把计算机分为4种计算机类型：SISD、SIMD、MISD和MIMD</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.5mjrnp2vkq80.png\" alt=\"yaimage\"></p>\n<p><strong>SISD</strong>: Single instruction single data stream，在指令级别和内存级别<strong>不</strong>使用任何并行性[parallelisme]的计算机，对应<strong>冯诺依曼</strong>结构。</p>\n<p><strong>MISD</strong>: Multi-instruction single data stream，由多个计算机单元并行处理相同数据，在实践中很少出现</p>\n<p><strong>SIMD</strong>: 在内存级别使用并行设计</p>\n<p><strong>MIMD</strong>: 多个计算单元处理不同的数据，每一个具有单独的内存，这是现代CPU使用最广泛的并行结构</p>\n<h4 id=\"CPU-VS-GPU\"><a href=\"#CPU-VS-GPU\" class=\"headerlink\" title=\"CPU VS GPU\"></a>CPU VS GPU</h4><p><strong>CPU</strong>：面向延迟的设计&#x2F;Latency-oriented design</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.5o65o1gzu6o0.png\" alt=\"yaimage\"></p>\n<p>一些计组小回顾：</p>\n<ul>\n<li>ALU:算术逻辑单元，对数据执行算术和逻辑运算，又可以理解为<strong>执行指定</strong>的单元。</li>\n<li>CPU：对指令流和数据流在时间与空间上实施正确的<strong>控制</strong><ul>\n<li>指令控制：产生下一条指令在内存中的地址</li>\n<li>操作控制：产生各种操作信号送往相应的部件，以控制完成指令所要求的动作</li>\n<li>时序控制：以各操作信号实施时间上的控制，以保证计算机有条不紊的连续自动工作</li>\n</ul>\n</li>\n<li>Cache:缓存,存放少量经常使用的内存数据以便CPU能够快速检索这些数据</li>\n<li>DRAM（动态随机存储器）是一种常见的存储器类型，它能够在计算机的主存储器中临时存储数据。</li>\n</ul>\n<p>高效ALU: 减少操作延迟</p>\n<p>大Cache：将长内存访问转换为短缓存访问，减少延迟</p>\n<p>CPU的复杂的执行控制：分支预测，数据转发，减少延迟</p>\n<p><strong>GPU</strong>: 面向吞吐量的设计&#x2F;Throughput Oriented Design</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.6nci3vtsniw0.png\" alt=\"yaimage\"></p>\n<p>ALU：不是很高效，高延迟但很多，有许多pipeline可以实现吞吐量</p>\n<p>小Caches：加速内存的吞吐量</p>\n<p>控制简单：没有分支预测、数据转发但有很多线程的硬件</p>\n<p><strong>CPU VS GPU</strong>: CPU为了时序部分，GPU为了并行部分</p>\n<h4 id=\"Parallele\"><a href=\"#Parallele\" class=\"headerlink\" title=\"Parallèle\"></a>Parallèle</h4><p>任务平行和数据平行</p>\n<ul>\n<li>任务平行：拆分任务，任务的处理是独立[independants]的</li>\n<li>数据平行：将一个大的数据集划分为较小的块然后平行的处理数据，处理是相同的[identiques]</li>\n</ul>\n<p>时序、并行(parallel)和并发(concurrence)</p>\n<p>时序：操作等待前一个操作完成，这两个操作不是独立的</p>\n<p>并行：把每一个任务分配给每一个处理器独立完成。在同一时间点，任务一定是同时运行。</p>\n<p>并发：把任务在不同的时间点交给处理器进行处理。在同一时间点，任务并不一定会同时运行。关键是处理多个任务的能力，不一定要同时。</p>\n<h3 id=\"Remarques-sur-l’acceleration\"><a href=\"#Remarques-sur-l’acceleration\" class=\"headerlink\" title=\"Remarques sur l’accélération\"></a>Remarques sur l’accélération</h3><p>$$<br>S_{latence}&#x3D;\\frac{1}{1-p+\\frac ps}<br>$$</p>\n<p>s:执行的线程，p：受益于并行性的运行时间的百分比</p>\n<p>Conseil : lors de l’écriture d’un programme parallèle, il faut limiter autant que possible la partie sérielle. 在编写并行程序时，有必要限制尽可能的串行部分</p>\n<p>Conseil : un ordinateur parallèle doit être un excellent ordinateur sériel pour traiter le plus rapidement possible la partie sérielle. 并行计算机必须是优秀的串行计算机.</p>\n<h3 id=\"Domaines-d’application-du-calcul-parallele-heterogene\"><a href=\"#Domaines-d’application-du-calcul-parallele-heterogene\" class=\"headerlink\" title=\"Domaines d’application du calcul parallèle hétérogène\"></a>Domaines d’application du calcul parallèle hétérogène</h3><p>发展GPU应用的三个方法：使用库，汇编指令，编程</p>\n<h2 id=\"1-4-CUDA-C\"><a href=\"#1-4-CUDA-C\" class=\"headerlink\" title=\"1.4 CUDA-C\"></a>1.4 CUDA-C</h2><p>NVIDIA提供了cuda-C的编译器：nvcc&#x3D;&#x3D;&gt;为GPU编译代码，然后将其传递给编译器</p>\n<p><strong>Structure d’un programme C en CUDA</strong></p>\n<p>Host and Device: CPU是host，GPU是devices，一个cuda程序会在这两部分运行，C的代码默认是host运行，要让它在GPU运行的话得加关键字<code>__device__</code>,编译过程可参考下图：<img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.2pj1ni7zrny0.png\" alt=\"yaimage\"></p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.7fxncsypwic0.png\" alt=\"yaimage\"></p>\n<ul>\n<li><strong>thread：</strong><ul>\n<li>是一个顺序程序的执行过程</li>\n<li>线程组成：代码,它的执行点,变量和数据结构的值。可以用程序的API来创建和操作线程</li>\n<li>[网上讲的]线程是一种在一个进程中执行的任务，它是操作系统能够<strong>进行运算调度的最小单位</strong>。在一个进程中可以有多个线程，这些线程共享该进程中的资源，并且可以并发执行。这使得线程能够实现多任务处理，提高系统的效率。线程与进程的主要区别在于，进程是一个独立的执行单元，它有自己的内存空间和资源，而线程则是进程的一个执行流，它是进程中的一个执行单元。因此，在一个进程中，多个线程共享该进程的资源，而不同的进程之间是相互独立的。</li>\n</ul>\n</li>\n<li><strong>Exécution concurrente:</strong> 操作系统将计算资源分配给线程（如果线程多于资源，则通过从一个线程切换到另一个线程）。如果几个线程同时活动，这被称为并发执行。</li>\n</ul>\n"},{"title":"代码1","url":"/2023/02/19/GPU/%E4%BB%A3%E7%A0%811/","content":"<h3 id=\"hello\"><a href=\"#hello\" class=\"headerlink\" title=\"hello\"></a>hello</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\">__device__ <span class=\"type\">const</span> <span class=\"type\">char</span> *STR = <span class=\"string\">&quot;HELLO WORLD!&quot;</span>;</span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">char</span> STR_LENGTH = <span class=\"number\">12</span>;</span><br><span class=\"line\">__global__ <span class=\"type\">void</span> <span class=\"title function_\">hello</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%c\\n&quot;</span>, STR[threadIdx.x % STR_LENGTH]);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">void</span>)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> num_threads = STR_LENGTH;</span><br><span class=\"line\">        <span class=\"type\">int</span> num_blocks = <span class=\"number\">1</span>;</span><br><span class=\"line\">        hello&lt;&lt;&lt;num_blocks,num_threads&gt;&gt;&gt;();</span><br><span class=\"line\">        cudaDeviceSynchronize();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"properties\"><a href=\"#properties\" class=\"headerlink\" title=\"properties\"></a>properties</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">  <span class=\"type\">int</span> deviceCount;</span><br><span class=\"line\">  cudaGetDeviceCount(&amp;deviceCount);</span><br><span class=\"line\">  <span class=\"type\">int</span> device;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">for</span> (device = <span class=\"number\">0</span>; device &lt; deviceCount; ++device) &#123;</span><br><span class=\"line\">    cudaDeviceProp deviceProp;</span><br><span class=\"line\">    cudaGetDeviceProperties(&amp;deviceProp, device);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Device %d has compute capability %d.%d.\\n&quot;</span>, device,</span><br><span class=\"line\">           deviceProp.major, deviceProp.minor);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Device %d is : %s\\n&quot;</span>,device,deviceProp.name);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Clock running at %f MHz\\n&quot;</span>,deviceProp.clockRate/<span class=\"number\">1000.0</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Number of multiprocessor(s) :  %d\\n&quot;</span>,deviceProp.multiProcessorCount);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;maxGridSize :  %d x %d x %d\\n&quot;</span>,deviceProp.maxGridSize[<span class=\"number\">0</span>],deviceProp.maxGridSize[<span class=\"number\">1</span>],deviceProp.maxGridSize[<span class=\"number\">2</span>]);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;maxThreadPerBlock:  %d\\n&quot;</span>,deviceProp.maxThreadsPerBlock);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./properties</span><br><span class=\"line\">Device 0 has compute capability 5.3.</span><br><span class=\"line\">Device 0 is : NVIDIA Tegra X1</span><br><span class=\"line\">Clock running at 921.600000 MHz</span><br><span class=\"line\">Number of multiprocessor(s) :  1</span><br><span class=\"line\">maxGridSize :  2147483647 x 65535 x 65535</span><br><span class=\"line\">maxThreadPerBlock:  1024</span><br></pre></td></tr></table></figure>\n<h2 id=\"TD2\"><a href=\"#TD2\" class=\"headerlink\" title=\"TD2\"></a>TD2</h2><h3 id=\"addvec\"><a href=\"#addvec\" class=\"headerlink\" title=\"addvec\"></a>addvec</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N (2048*2048)</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> THREADS_PER_BLOCK 1024</span></span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vector_add</span><span class=\"params\">(<span class=\"type\">int</span> *a, <span class=\"type\">int</span> *b, <span class=\"type\">int</span> *c)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">/* insert code to calculate the index properly using blockIdx.x, blockDim.x, threadIdx.x */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> index = threadIdx.x + blockIdx.x*blockDim.x;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (index&lt;N) c[index] = a[index] + b[index];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> *a, *b, *c;</span><br><span class=\"line\">        <span class=\"type\">int</span> *d_a, *d_b, *d_c;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for device copies of a, b, c */</span></span><br><span class=\"line\">        cudaEvent_t start, stop;</span><br><span class=\"line\">        cudaEventCreate(&amp;start);</span><br><span class=\"line\">        cudaEventCreate(&amp;stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record start time */</span></span><br><span class=\"line\">        cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_a, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_b, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_c, size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for host copies of a, b, c and setup input values */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        a = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\">        b = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\">        c = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy inputs to device */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data to the device */</span></span><br><span class=\"line\">        cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\">        cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* launch the kernel on the GPU */</span></span><br><span class=\"line\">        <span class=\"comment\">/* insert the launch parameters to launch the kernel properly using blocks and threads */</span></span><br><span class=\"line\">        <span class=\"type\">int</span> nbBlocs = <span class=\"built_in\">ceil</span>(N/(<span class=\"type\">float</span>)THREADS_PER_BLOCK);</span><br><span class=\"line\">        vector_add&lt;&lt;&lt;nbBlocs,THREADS_PER_BLOCK&gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy result back to host */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data back to the host */</span></span><br><span class=\"line\">        cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>]);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>,N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">free</span>(a);</span><br><span class=\"line\">        <span class=\"built_in\">free</span>(b);</span><br><span class=\"line\">        <span class=\"built_in\">free</span>(c);</span><br><span class=\"line\">        cudaFree( d_a );</span><br><span class=\"line\">        cudaFree( d_b );</span><br><span class=\"line\">        cudaFree( d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\">/* record finish time */</span></span><br><span class=\"line\">        cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"comment\">/* wait GPU event */</span></span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">float</span> elapsedTime;</span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;[Classical]Time to execute %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record start time */</span></span><br><span class=\"line\">        cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_a, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_b, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_c, size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for host copies of a, b, c and setup input values */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        cudaHostAlloc( (<span class=\"type\">int</span> **) &amp;a, size,  cudaHostAllocDefault );</span><br><span class=\"line\">        cudaHostAlloc( (<span class=\"type\">int</span> **) &amp;b, size,  cudaHostAllocDefault );</span><br><span class=\"line\">        cudaHostAlloc( (<span class=\"type\">int</span> **) &amp;c, size,  cudaHostAllocDefault );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy inputs to device */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data to the device */</span></span><br><span class=\"line\">        cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\">        cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* launch the kernel on the GPU */</span></span><br><span class=\"line\">        <span class=\"comment\">/* insert the launch parameters to launch the kernel properly using blocks and threads */</span></span><br><span class=\"line\">        nbBlocs = <span class=\"built_in\">ceil</span>(N/(<span class=\"type\">float</span>)THREADS_PER_BLOCK);</span><br><span class=\"line\">        vector_add&lt;&lt;&lt;nbBlocs,THREADS_PER_BLOCK&gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy result back to host */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data back to the host */</span></span><br><span class=\"line\">        cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>]);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>,N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        cudaFreeHost(a);</span><br><span class=\"line\">        cudaFreeHost(b);</span><br><span class=\"line\">        cudaFreeHost(c);</span><br><span class=\"line\">        cudaFree( d_a );</span><br><span class=\"line\">        cudaFree( d_b );</span><br><span class=\"line\">        cudaFree( d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\">/* record finish time */</span></span><br><span class=\"line\">        cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"comment\">/* wait GPU event */</span></span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;[Pinned]Time to execute 2 %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record start time */</span></span><br><span class=\"line\">        cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for host copies of a, b, c and setup input values */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        cudaMallocManaged( (<span class=\"type\">int</span> **) &amp;a, size );</span><br><span class=\"line\">        cudaMallocManaged( (<span class=\"type\">int</span> **) &amp;a, size );</span><br><span class=\"line\">        cudaMallocManaged( (<span class=\"type\">int</span> **) &amp;a, size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* launch the kernel on the GPU */</span></span><br><span class=\"line\">        <span class=\"comment\">/* insert the launch parameters to launch the kernel properly using blocks and threads */</span></span><br><span class=\"line\">        nbBlocs = <span class=\"built_in\">ceil</span>(N/(<span class=\"type\">float</span>)THREADS_PER_BLOCK);</span><br><span class=\"line\">        vector_add&lt;&lt;&lt;nbBlocs,THREADS_PER_BLOCK&gt;&gt;&gt;( a, b, c );</span><br><span class=\"line\">    cudaDeviceSynchronize();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>]);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>,N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\">        cudaFree(a);</span><br><span class=\"line\">        cudaFree(b);</span><br><span class=\"line\">        cudaFree(c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record finish time */</span></span><br><span class=\"line\">        cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"comment\">/* wait GPU event */</span></span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;[Managed]Time to execute 3 %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventDestroy(stop);</span><br><span class=\"line\">        cudaEventDestroy(start);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125; <span class=\"comment\">/* end main */</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./addvec</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[4194303] = 8388606</span><br><span class=\"line\">[Classical]Time to execute 271.1 ms</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[4194303] = 8388606</span><br><span class=\"line\">[Pinned]Time to execute 2 1213.7 ms</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[4194303] = 4194303</span><br><span class=\"line\">[Managed]Time to execute 3 169.7 ms</span><br></pre></td></tr></table></figure>\n<h3 id=\"addVectWithoutKernel\"><a href=\"#addVectWithoutKernel\" class=\"headerlink\" title=\"addVectWithoutKernel\"></a>addVectWithoutKernel</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* experiment with N */</span></span><br><span class=\"line\"><span class=\"comment\">/* how large can it be? */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N (2048*2048)</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> THREADS_PER_BLOCK 512</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* declare and create CUDA events */</span></span><br><span class=\"line\">        cudaEvent_t start, stop;</span><br><span class=\"line\">        cudaEventCreate(&amp;start);</span><br><span class=\"line\">        cudaEventCreate(&amp;stop);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> *a, *b, *c;</span><br><span class=\"line\">        <span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for host copies of a, b, c and setup input values */</span></span><br><span class=\"line\">        a = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\">        b = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\">        c = (<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>( size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record start time */</span></span><br><span class=\"line\">        cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* insert the launch parameters to launch the kernel properly using blocks and threads */</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> index = <span class=\"number\">0</span>; index &lt; N; index++)</span><br><span class=\"line\">                c[index] = a[index] + b[index];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* record finish time */</span></span><br><span class=\"line\">        cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"comment\">/* wait GPU event */</span></span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* compute and print ellapsed time between start and stop */</span></span><br><span class=\"line\">        <span class=\"type\">float</span> elapsedTime;</span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Time to execute %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\">        cudaEventDestroy(start);</span><br><span class=\"line\">        cudaEventDestroy(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\">        <span class=\"built_in\">free</span>(a);</span><br><span class=\"line\">        <span class=\"built_in\">free</span>(b);</span><br><span class=\"line\">        <span class=\"built_in\">free</span>(c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125; <span class=\"comment\">/* end main */</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./addVecProp</span><br><span class=\"line\">Time to execute 47.7 ms</span><br></pre></td></tr></table></figure>\n<h2 id=\"TD3\"><a href=\"#TD3\" class=\"headerlink\" title=\"TD3\"></a>TD3</h2><h3 id=\"julia-bmp\"><a href=\"#julia-bmp\" class=\"headerlink\" title=\"julia_bmp\"></a>julia_bmp</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;bitmap_image.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> DIM 1000</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">cuComplex</span> &#123;</span></span><br><span class=\"line\">   <span class=\"type\">float</span>   r;</span><br><span class=\"line\">   <span class=\"type\">float</span>   i;</span><br><span class=\"line\">   cuComplex( <span class=\"type\">float</span> a, <span class=\"type\">float</span> b ) : r(a), i(b)  &#123;&#125;</span><br><span class=\"line\">   <span class=\"type\">float</span> <span class=\"title function_\">magnitude2</span><span class=\"params\">( <span class=\"type\">void</span> )</span> &#123; <span class=\"keyword\">return</span> r * r + i * i; &#125;</span><br><span class=\"line\">   cuComplex operator*(<span class=\"type\">const</span> cuComplex&amp; a) &#123;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> cuComplex(r*a.r - i*a.i, i*a.r + r*a.i);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   cuComplex operator+(<span class=\"type\">const</span> cuComplex&amp; a) &#123;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> cuComplex(r+a.r, i+a.i);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">julia</span><span class=\"params\">( <span class=\"type\">int</span> x, <span class=\"type\">int</span> y )</span> &#123;</span><br><span class=\"line\">   <span class=\"type\">const</span> <span class=\"type\">float</span> scale = <span class=\"number\">1.5</span>;</span><br><span class=\"line\">   <span class=\"type\">float</span> jx = scale * (<span class=\"type\">float</span>)(DIM/<span class=\"number\">2</span> - x)/(DIM/<span class=\"number\">2</span>);</span><br><span class=\"line\">   <span class=\"type\">float</span> jy = scale * (<span class=\"type\">float</span>)(DIM/<span class=\"number\">2</span> - y)/(DIM/<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">   cuComplex <span class=\"title function_\">c</span><span class=\"params\">(<span class=\"number\">-0.8</span>, <span class=\"number\">0.156</span>)</span>;</span><br><span class=\"line\">   cuComplex <span class=\"title function_\">a</span><span class=\"params\">(jx, jy)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">200</span>; i++) &#123;</span><br><span class=\"line\">       a = a * a + c;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (a.magnitude2() &gt; <span class=\"number\">1000</span>)</span><br><span class=\"line\">           <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">kernel</span><span class=\"params\">( <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *ptr )</span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> y=<span class=\"number\">0</span>; y&lt;DIM; y++) &#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> x=<span class=\"number\">0</span>; x&lt;DIM; x++) &#123;</span><br><span class=\"line\">           <span class=\"type\">int</span> offset = x + y * DIM;</span><br><span class=\"line\"></span><br><span class=\"line\">           <span class=\"type\">int</span> juliaValue = julia( x, y );</span><br><span class=\"line\">           ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">0</span>] = <span class=\"number\">255</span> * juliaValue;</span><br><span class=\"line\">           ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">           ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">2</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">( <span class=\"type\">void</span> )</span> &#123;</span><br><span class=\"line\">   cudaEvent_t start, stop;</span><br><span class=\"line\"> cudaEventCreate(&amp;start);</span><br><span class=\"line\"> cudaEventCreate(&amp;stop);</span><br><span class=\"line\">   <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *ptr = (<span class=\"type\">unsigned</span> <span class=\"type\">char</span> *)<span class=\"built_in\">malloc</span>(DIM*DIM*<span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\">   kernel( ptr );</span><br><span class=\"line\">   cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\"> cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"type\">float</span> elapsedTime;</span><br><span class=\"line\"> cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\"> <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Time to compute %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaEventDestroy(start);</span><br><span class=\"line\">   cudaEventDestroy(stop);</span><br><span class=\"line\"> <span class=\"comment\">// Write BMP image</span></span><br><span class=\"line\">   bitmap_image <span class=\"title function_\">img</span><span class=\"params\">(DIM,DIM)</span>;</span><br><span class=\"line\">   img.clear();</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> y = DIM<span class=\"number\">-1</span>; y &gt;= <span class=\"number\">0</span>; y--)</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> x = DIM<span class=\"number\">-1</span>; x &gt;= <span class=\"number\">0</span>; x--)</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">           <span class=\"type\">int</span> offset = x + y * DIM;</span><br><span class=\"line\">           img.set_pixel(x, y, ptr[offset*<span class=\"number\">3</span>], ptr[offset*<span class=\"number\">3</span>+<span class=\"number\">1</span>], ptr[offset*<span class=\"number\">3</span>+<span class=\"number\">2</span>]);            </span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   img.save_image(<span class=\"string\">&quot;test.bmp&quot;</span>);</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./julia</span><br><span class=\"line\">Time to compute 1789.2 ms</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://postimg.cc/bS11nkFV\"><img src=\"https://i.postimg.cc/J02x9Q38/image.png\" alt=\"image.png\"></a></p>\n<h3 id=\"julia-bmp-gpu\"><a href=\"#julia-bmp-gpu\" class=\"headerlink\" title=\"julia_bmp_gpu\"></a>julia_bmp_gpu</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"> <span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;bitmap_image.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> DIM 1000</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">cuComplex</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">float</span>   r;</span><br><span class=\"line\">    <span class=\"type\">float</span>   i;</span><br><span class=\"line\">    __device__</span><br><span class=\"line\">    <span class=\"title function_\">cuComplex</span><span class=\"params\">( <span class=\"type\">float</span> a, <span class=\"type\">float</span> b )</span> : <span class=\"title function_\">r</span><span class=\"params\">(a)</span>, <span class=\"title function_\">i</span><span class=\"params\">(b)</span>  &#123;&#125;</span><br><span class=\"line\">    __device__</span><br><span class=\"line\">    <span class=\"type\">float</span> <span class=\"title function_\">magnitude2</span><span class=\"params\">( <span class=\"type\">void</span> )</span> &#123; <span class=\"keyword\">return</span> r * r + i * i; &#125;</span><br><span class=\"line\">    __device__</span><br><span class=\"line\">    cuComplex operator*(<span class=\"type\">const</span> cuComplex&amp; a) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> cuComplex(r*a.r - i*a.i, i*a.r + r*a.i);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    __device__</span><br><span class=\"line\">    cuComplex operator+(<span class=\"type\">const</span> cuComplex&amp; a) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> cuComplex(r+a.r, i+a.i);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">__device__</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">julia</span><span class=\"params\">( <span class=\"type\">int</span> x, <span class=\"type\">int</span> y )</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">const</span> <span class=\"type\">float</span> scale = <span class=\"number\">1.5</span>;</span><br><span class=\"line\">    <span class=\"type\">float</span> jx = scale * (<span class=\"type\">float</span>)(DIM/<span class=\"number\">2</span> - x)/(DIM/<span class=\"number\">2</span>);</span><br><span class=\"line\">    <span class=\"type\">float</span> jy = scale * (<span class=\"type\">float</span>)(DIM/<span class=\"number\">2</span> - y)/(DIM/<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    cuComplex <span class=\"title function_\">c</span><span class=\"params\">(<span class=\"number\">-0.8</span>, <span class=\"number\">0.156</span>)</span>;</span><br><span class=\"line\">    cuComplex <span class=\"title function_\">a</span><span class=\"params\">(jx, jy)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">200</span>; i++) &#123;</span><br><span class=\"line\">        a = a * a + c;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (a.magnitude2() &gt; <span class=\"number\">1000</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">kernel</span><span class=\"params\">( <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *ptr )</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> x = threadIdx.x + blockDim.x*blockIdx.x;</span><br><span class=\"line\">    <span class=\"type\">int</span> y = threadIdx.y + blockDim.y*blockIdx.y;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &lt; DIM &amp;&amp; y &lt; DIM)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> offset = x + y * DIM;</span><br><span class=\"line\">        <span class=\"type\">int</span> juliaValue = julia( x, y );</span><br><span class=\"line\">        ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">0</span>] = <span class=\"number\">255</span> * juliaValue;</span><br><span class=\"line\">        ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        ptr[offset*<span class=\"number\">3</span> + <span class=\"number\">2</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">( <span class=\"type\">void</span> )</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> size = DIM*DIM*<span class=\"number\">3</span>;</span><br><span class=\"line\">    cudaEvent_t start, stop;</span><br><span class=\"line\">        cudaEventCreate(&amp;start);</span><br><span class=\"line\">        cudaEventCreate(&amp;stop);</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *ptr = (<span class=\"type\">unsigned</span> <span class=\"type\">char</span> *)<span class=\"built_in\">malloc</span>(size);</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *image;</span><br><span class=\"line\">    cudaMalloc((<span class=\"type\">void</span> **) &amp;image, size);</span><br><span class=\"line\"></span><br><span class=\"line\">    dim3 <span class=\"title function_\">THREADS_PER_BLOCK</span><span class=\"params\">(<span class=\"number\">16</span>,<span class=\"number\">16</span>,<span class=\"number\">1</span>)</span>;</span><br><span class=\"line\">    dim3 <span class=\"title function_\">BLOCK_NUMBER</span><span class=\"params\">((DIM<span class=\"number\">-1</span>)/<span class=\"number\">16</span>+<span class=\"number\">1</span>,(DIM<span class=\"number\">-1</span>)/<span class=\"number\">16</span>+<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\">    kernel&lt;&lt;&lt;BLOCK_NUMBER,THREADS_PER_BLOCK&gt;&gt;&gt;( image );</span><br><span class=\"line\">    cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">float</span> elapsedTime;</span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Time to compute %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemcpy(ptr, image, size, cudaMemcpyDeviceToHost);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventDestroy(start);</span><br><span class=\"line\">    cudaEventDestroy(stop);</span><br><span class=\"line\">        <span class=\"comment\">// Write BMP image</span></span><br><span class=\"line\">    bitmap_image <span class=\"title function_\">img</span><span class=\"params\">(DIM,DIM)</span>;</span><br><span class=\"line\">    img.clear();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> y = DIM<span class=\"number\">-1</span>; y &gt;= <span class=\"number\">0</span>; y--)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"type\">int</span> x = DIM<span class=\"number\">-1</span>; x &gt;= <span class=\"number\">0</span>; x--)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> offset = x + y * DIM;</span><br><span class=\"line\">            img.set_pixel(x, y, ptr[offset*<span class=\"number\">3</span>], ptr[offset*<span class=\"number\">3</span>+<span class=\"number\">1</span>], ptr[offset*<span class=\"number\">3</span>+<span class=\"number\">2</span>]);            </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    img.save_image(<span class=\"string\">&quot;test.bmp&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaFree(image);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(ptr);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./julia_gpu</span><br><span class=\"line\">Time to compute 52.3 ms</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://postimg.cc/cvCryfBq\"><img src=\"https://i.postimg.cc/wjcJNVtB/image.png\" alt=\"image.png\"></a></p>\n<h3 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;assert.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;start); \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;stop);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventSynchronize(stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">&#123;float elapsedTime;\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">printf(<span class=\"string\">&quot;Time to %s %3.1f ms\\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//ComputeC=A*B</span></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">sgemm</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span> &#123;</span><br><span class=\"line\">   <span class=\"type\">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class=\"line\">   <span class=\"type\">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"keyword\">if</span> (row &lt; numARows &amp;&amp; col &lt; numBColumns) &#123;</span><br><span class=\"line\">       <span class=\"type\">float</span> sum = <span class=\"number\">0</span>;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> ii = <span class=\"number\">0</span>; ii &lt; numAColumns; ii++) &#123;</span><br><span class=\"line\">           sum += A[row * numAColumns + ii] * B[ii * numBColumns + col];</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       C[row * numBColumns + col] = sum;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   CREATE_CUDAEVENT</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"type\">int</span> numARows = atoi(argv[<span class=\"number\">1</span>]); <span class=\"comment\">// number of rows in the matrix A</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numAColumns = atoi(argv[<span class=\"number\">2</span>]); <span class=\"comment\">// number of columns in the matrix A</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numBRows = atoi(argv[<span class=\"number\">3</span>]); <span class=\"comment\">// number of rows in the matrix B</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numBColumns = atoi(argv[<span class=\"number\">4</span>]); <span class=\"comment\">// number of columns in the matrix B</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numCRows = numARows; <span class=\"comment\">// number of rows in the matrix C</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numCColumns = numBColumns; <span class=\"comment\">// number of columns in the matrix C</span></span><br><span class=\"line\">   assert(numAColumns == numBRows);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"type\">float</span> *A = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numARows*numAColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *B = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numBRows*numBColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *C = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *hostC = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// Initialize matrices on the host</span></span><br><span class=\"line\">   initMatrix(A, numARows, numAColumns);</span><br><span class=\"line\">   initMatrix(B, numBRows, numBColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">   START_CUDAEVENT</span><br><span class=\"line\">   <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(A, B, C, numARows, numAColumns, numBRows, numBColumns)</span>;</span><br><span class=\"line\">   STOP_AND_PRINT_CUDAEVENT(compute CPU)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// CUDA PART</span></span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceA;</span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceB;</span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceC;</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceA, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceB, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceC, numCRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemcpy(deviceA, A, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\">   cudaMemcpy(deviceB, B, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemset(deviceC, <span class=\"number\">0</span>, numCRows * numCColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   dim3 <span class=\"title function_\">blockDim</span><span class=\"params\">(<span class=\"number\">16</span>, <span class=\"number\">16</span>)</span>;</span><br><span class=\"line\">   dim3 <span class=\"title function_\">gridDim</span><span class=\"params\">(<span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numBColumns) / blockDim.x), <span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numARows) / blockDim.y))</span>;</span><br><span class=\"line\">   START_CUDAEVENT</span><br><span class=\"line\">   sgemm&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numBColumns);</span><br><span class=\"line\">   STOP_AND_PRINT_CUDAEVENT(compute GPU)</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemcpy(hostC, deviceC, numARows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyDeviceToHost);</span><br><span class=\"line\">   <span class=\"comment\">// END CUDA PART</span></span><br><span class=\"line\"></span><br><span class=\"line\">   compareMatrix(C, hostC, numCRows, numCColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaFree(deviceA);</span><br><span class=\"line\">   cudaFree(deviceB);</span><br><span class=\"line\">   cudaFree(deviceC);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"built_in\">free</span>(A);</span><br><span class=\"line\">   <span class=\"built_in\">free</span>(B);</span><br><span class=\"line\">   <span class=\"built_in\">free</span>(C);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;numRows; i++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j=<span class=\"number\">0</span>; j&lt;numCols; j++)&#123;</span><br><span class=\"line\">           m[i*numCols+j] = <span class=\"built_in\">sin</span>(i*numCols+j);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C,</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns,</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns</span></span><br><span class=\"line\"><span class=\"params\">)</span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numARows; row++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numBColumns; col++)&#123;   </span><br><span class=\"line\">           C[row * numBColumns + col] = <span class=\"number\">0.0</span>;         </span><br><span class=\"line\">           <span class=\"keyword\">for</span> (<span class=\"type\">int</span> n = <span class=\"number\">0</span>; n &lt; numAColumns; n++)&#123;</span><br><span class=\"line\">               C[row * numBColumns + col] += A[row * numAColumns + n] * B[n * numBColumns + col];</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>&#123;</span><br><span class=\"line\">   <span class=\"type\">float</span> sum = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">   <span class=\"type\">float</span> max = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">   <span class=\"type\">float</span> min = <span class=\"number\">10.0</span>;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numRows; row++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numColumns; col++)&#123;    </span><br><span class=\"line\">           <span class=\"type\">float</span> diff = A[row*numColumns+col] - B[row*numColumns+col];</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (diff &gt; max) max = diff;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (diff &lt; min) min = diff;</span><br><span class=\"line\">           sum += diff;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;mean: &quot;</span> &lt;&lt; sum / (numRows*numColumns) &lt;&lt; <span class=\"string\">&quot; max: &quot;</span> &lt;&lt; max &lt;&lt; <span class=\"string\">&quot; min: &quot;</span> &lt;&lt; min &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./matrix 20 40 40 30</span><br><span class=\"line\">Time to compute CPU 0.5 ms</span><br><span class=\"line\">Time to compute GPU 0.1 ms</span><br><span class=\"line\">mean: -3.79204e-09 max: 9.53674e-07 min: -7.15256e-07</span><br></pre></td></tr></table></figure>\n<h2 id=\"TD6\"><a href=\"#TD6\" class=\"headerlink\" title=\"TD6\"></a>TD6</h2><p>###同步的重要性</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">#include &lt;iostream&gt;</span><br><span class=\"line\">#include &quot;bitmap_image.hpp&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#define DIM 1024</span><br><span class=\"line\">#define PI 3.1415926535897932f</span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\">void kernel( unsigned char *ptr ) &#123;</span><br><span class=\"line\">    // map from threadIdx/BlockIdx to pixel position</span><br><span class=\"line\">    int x = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class=\"line\">    int y = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class=\"line\">    int offset = x + y * blockDim.x * gridDim.x;</span><br><span class=\"line\"></span><br><span class=\"line\">    __shared__ float shared[16][16];</span><br><span class=\"line\"></span><br><span class=\"line\">    // now calculate the value at that position</span><br><span class=\"line\">    const float period = 128.0f;</span><br><span class=\"line\"></span><br><span class=\"line\">    __syncthreads(); // 加上同步命令，得到排列整齐的图片.没有的话是混乱的</span><br><span class=\"line\">    shared[threadIdx.x][threadIdx.y] =</span><br><span class=\"line\">            255 * (sinf(x*2.0f*PI/ period) + 1.0f) *</span><br><span class=\"line\">                  (sinf(y*2.0f*PI/ period) + 1.0f) / 4.0f;</span><br><span class=\"line\"></span><br><span class=\"line\">    __syncthreads(); // 加上同步命令，得到整齐排列的图片</span><br><span class=\"line\">    ptr[offset*3 + 0] = shared[15-threadIdx.x][15-threadIdx.y];</span><br><span class=\"line\">    ptr[offset*3 + 1] = 0;</span><br><span class=\"line\">    ptr[offset*3 + 2] = 255;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">int main( void ) &#123;</span><br><span class=\"line\">   int size = DIM*DIM*3*sizeof(unsigned char);</span><br><span class=\"line\">   unsigned char *h_ptr = (unsigned char *)malloc(size);</span><br><span class=\"line\">   unsigned char *d_ptr;</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMalloc( (void**)&amp;d_ptr, size );</span><br><span class=\"line\"></span><br><span class=\"line\">   dim3    grids(DIM/16,DIM/16);</span><br><span class=\"line\">   dim3    threads(16,16);</span><br><span class=\"line\">   kernel&lt;&lt;&lt;grids,threads&gt;&gt;&gt;( d_ptr );</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemcpy( h_ptr, d_ptr, size, cudaMemcpyDeviceToHost );</span><br><span class=\"line\"></span><br><span class=\"line\">   bitmap_image img(DIM,DIM);</span><br><span class=\"line\">   img.clear();</span><br><span class=\"line\">   for (int y = DIM-1; y &gt;= 0; y--)</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   for (int x = DIM-1; x &gt;= 0; x--)</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">           int offset = x + y * DIM;</span><br><span class=\"line\">           img.set_pixel(x, y, h_ptr[offset*3], h_ptr[offset*3+1], h_ptr[offset*3+2]);            </span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   img.save_image(&quot;test.bmp&quot;);</span><br><span class=\"line\">   return 0;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://postimg.cc/Fd5bDhkB\"><img src=\"https://i.postimg.cc/nzMdVzFp/image.png\" alt=\"image.png\"></a><br><a href=\"https://postimg.cc/B8xPZHtm\"><img src=\"https://i.postimg.cc/g07VDKpb/image.png\" alt=\"image.png\"></a></p>\n<h3 id=\"共享内存应用于矩阵乘法\"><a href=\"#共享内存应用于矩阵乘法\" class=\"headerlink\" title=\"共享内存应用于矩阵乘法\"></a>共享内存应用于矩阵乘法</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"> <span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"> <span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;assert.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"> using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;start); \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;stop);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventSynchronize(stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">&#123;float elapsedTime;\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">printf(<span class=\"string\">&quot;Time to %s %3.1f ms\\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> TILE_WIDTH 16</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//ComputeC=A*B</span></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">sgemm</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span> &#123;</span><br><span class=\"line\">    __shared__ <span class=\"type\">float</span> ds_M[TILE_WIDTH][TILE_WIDTH];</span><br><span class=\"line\">    __shared__ <span class=\"type\">float</span> ds_N[TILE_WIDTH][TILE_WIDTH];</span><br><span class=\"line\">    <span class=\"type\">int</span> bx = blockIdx.x, by = blockIdx.y, tx = threadIdx.x, ty = threadIdx.y,</span><br><span class=\"line\">    row = by * TILE_WIDTH + ty, col = bx * TILE_WIDTH + tx;</span><br><span class=\"line\">    <span class=\"type\">float</span> Pvalue = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> m = <span class=\"number\">0</span>; m &lt; (numAColumns - <span class=\"number\">1</span>) / TILE_WIDTH + <span class=\"number\">1</span>; ++m) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (row &lt; numARows &amp;&amp; m * TILE_WIDTH + tx &lt; numAColumns)</span><br><span class=\"line\">            ds_M[ty][tx] = A[row * numAColumns + m * TILE_WIDTH + tx];</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            ds_M[ty][tx] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (col &lt; numBColumns &amp;&amp; m * TILE_WIDTH + ty &lt; numBRows)</span><br><span class=\"line\">            ds_N[ty][tx] = B[(m * TILE_WIDTH + ty) * numBColumns + col];</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            ds_N[ty][tx] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        __syncthreads();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">0</span>; k &lt; TILE_WIDTH; ++k)</span><br><span class=\"line\">            Pvalue += ds_M[ty][k] * ds_N[k][tx];</span><br><span class=\"line\">        __syncthreads();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (row &lt; numARows &amp;&amp; col &lt; numBColumns)</span><br><span class=\"line\">        C[row * numBColumns + col] = Pvalue;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span><br><span class=\"line\"><span class=\"comment\">    int col = blockIdx.x * blockDim.x + threadIdx.x;</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    if (row &lt; numARows &amp;&amp; col &lt; numBColumns) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">        float sum = 0;</span></span><br><span class=\"line\"><span class=\"comment\">        for (int ii = 0; ii &lt; numAColumns; ii++) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">            sum += A[row * numAColumns + ii] * B[ii * numBColumns + col];</span></span><br><span class=\"line\"><span class=\"comment\">        &#125;</span></span><br><span class=\"line\"><span class=\"comment\">        C[row * numBColumns + col] = sum;</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;*/</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    CREATE_CUDAEVENT</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> numARows = atoi(argv[<span class=\"number\">1</span>]); <span class=\"comment\">// number of rows in the matrix A</span></span><br><span class=\"line\">    <span class=\"type\">int</span> numAColumns = atoi(argv[<span class=\"number\">2</span>]); <span class=\"comment\">// number of columns in the matrix A</span></span><br><span class=\"line\">    <span class=\"type\">int</span> numBRows = atoi(argv[<span class=\"number\">3</span>]); <span class=\"comment\">// number of rows in the matrix B</span></span><br><span class=\"line\">    <span class=\"type\">int</span> numBColumns = atoi(argv[<span class=\"number\">4</span>]); <span class=\"comment\">// number of columns in the matrix B</span></span><br><span class=\"line\">    <span class=\"type\">int</span> numCRows = numARows; <span class=\"comment\">// number of rows in the matrix C</span></span><br><span class=\"line\">    <span class=\"type\">int</span> numCColumns = numBColumns; <span class=\"comment\">// number of columns in the matrix C</span></span><br><span class=\"line\">    assert(numAColumns == numBRows);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">float</span> *A = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numARows*numAColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">    <span class=\"type\">float</span> *B = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numBRows*numBColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">    <span class=\"type\">float</span> *C = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">    <span class=\"type\">float</span> *hostC = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Initialize matrices on the host</span></span><br><span class=\"line\">    initMatrix(A, numARows, numAColumns);</span><br><span class=\"line\">    initMatrix(B, numBRows, numBColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">    START_CUDAEVENT</span><br><span class=\"line\">    <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(A, B, C, numARows, numAColumns, numBRows, numBColumns)</span>;</span><br><span class=\"line\">    STOP_AND_PRINT_CUDAEVENT(compute CPU)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// CUDA PART</span></span><br><span class=\"line\">    <span class=\"type\">float</span> *deviceA;</span><br><span class=\"line\">    <span class=\"type\">float</span> *deviceB;</span><br><span class=\"line\">    <span class=\"type\">float</span> *deviceC;</span><br><span class=\"line\">    cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceA, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">    cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceB, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">    cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceC, numCRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemcpy(deviceA, A, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\">    cudaMemcpy(deviceB, B, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemset(deviceC, <span class=\"number\">0</span>, numCRows * numCColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    dim3 <span class=\"title function_\">blockDim</span><span class=\"params\">(<span class=\"number\">16</span>, <span class=\"number\">16</span>)</span>;</span><br><span class=\"line\">    dim3 <span class=\"title function_\">gridDim</span><span class=\"params\">(<span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numBColumns) / blockDim.x), <span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numARows) / blockDim.y))</span>;</span><br><span class=\"line\">    START_CUDAEVENT</span><br><span class=\"line\">    sgemm&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numBColumns);</span><br><span class=\"line\">    STOP_AND_PRINT_CUDAEVENT(compute GPU)</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemcpy(hostC, deviceC, numARows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyDeviceToHost);</span><br><span class=\"line\">    <span class=\"comment\">// END CUDA PART</span></span><br><span class=\"line\"></span><br><span class=\"line\">    compareMatrix(C, hostC, numCRows, numCColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaFree(deviceA);</span><br><span class=\"line\">    cudaFree(deviceB);</span><br><span class=\"line\">    cudaFree(deviceC);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(A);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(B);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(C);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;numRows; i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j=<span class=\"number\">0</span>; j&lt;numCols; j++)&#123;</span><br><span class=\"line\">            m[i*numCols+j] = <span class=\"built_in\">sin</span>(i*numCols+j);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">    <span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C,</span></span><br><span class=\"line\"><span class=\"params\">    <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns,</span></span><br><span class=\"line\"><span class=\"params\">    <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns</span></span><br><span class=\"line\"><span class=\"params\">)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numARows; row++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numBColumns; col++)&#123;   </span><br><span class=\"line\">            C[row * numBColumns + col] = <span class=\"number\">0.0</span>;         </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> n = <span class=\"number\">0</span>; n &lt; numAColumns; n++)&#123;</span><br><span class=\"line\">                C[row * numBColumns + col] += A[row * numAColumns + n] * B[n * numBColumns + col];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">float</span> sum = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">    <span class=\"type\">float</span> max = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">    <span class=\"type\">float</span> min = <span class=\"number\">10.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numRows; row++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numColumns; col++)&#123;    </span><br><span class=\"line\">            <span class=\"type\">float</span> diff = A[row*numColumns+col] - B[row*numColumns+col];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (diff &gt; max) max = diff;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (diff &lt; min) min = diff;</span><br><span class=\"line\">            sum += diff;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;mean: &quot;</span> &lt;&lt; sum / (numRows*numColumns) &lt;&lt; <span class=\"string\">&quot; max: &quot;</span> &lt;&lt; max &lt;&lt; <span class=\"string\">&quot; min: &quot;</span> &lt;&lt; min &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./matrixshare 20 40 40 30</span><br><span class=\"line\">Time to compute CPU 0.5 ms</span><br><span class=\"line\">Time to compute GPU 0.1 ms</span><br><span class=\"line\">mean: -3.79204e-09 max: 9.53674e-07 min: -7.15256e-07</span><br></pre></td></tr></table></figure>\n"},{"title":"代码2","url":"/2023/02/19/GPU/%E4%BB%A3%E7%A0%812/","content":"<h3 id=\"比较不同内存的矩阵加法\"><a href=\"#比较不同内存的矩阵加法\" class=\"headerlink\" title=\"比较不同内存的矩阵加法\"></a>比较不同内存的矩阵加法</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;assert.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;start); \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;stop);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventSynchronize(stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">&#123;float elapsedTime;\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">printf(<span class=\"string\">&quot;Time to %s %3.1f ms\\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> TILE_WIDTH 16</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//ComputeC=A*B</span></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">sgemm</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C, <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns, <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns)</span> &#123;</span><br><span class=\"line\">   __shared__ <span class=\"type\">float</span> ds_M[TILE_WIDTH][TILE_WIDTH];</span><br><span class=\"line\">   __shared__ <span class=\"type\">float</span> ds_N[TILE_WIDTH][TILE_WIDTH];</span><br><span class=\"line\">   <span class=\"type\">int</span> bx = blockIdx.x, by = blockIdx.y, tx = threadIdx.x, ty = threadIdx.y,</span><br><span class=\"line\">   row = by * TILE_WIDTH + ty, col = bx * TILE_WIDTH + tx;</span><br><span class=\"line\">   <span class=\"type\">float</span> Pvalue = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> m = <span class=\"number\">0</span>; m &lt; (numAColumns - <span class=\"number\">1</span>) / TILE_WIDTH + <span class=\"number\">1</span>; ++m) &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (row &lt; numARows &amp;&amp; m * TILE_WIDTH + tx &lt; numAColumns)</span><br><span class=\"line\">           ds_M[ty][tx] = A[row * numAColumns + m * TILE_WIDTH + tx];</span><br><span class=\"line\">       <span class=\"keyword\">else</span></span><br><span class=\"line\">           ds_M[ty][tx] = <span class=\"number\">0</span>;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (col &lt; numBColumns &amp;&amp; m * TILE_WIDTH + ty &lt; numBRows)</span><br><span class=\"line\">           ds_N[ty][tx] = B[(m * TILE_WIDTH + ty) * numBColumns + col];</span><br><span class=\"line\">       <span class=\"keyword\">else</span></span><br><span class=\"line\">           ds_N[ty][tx] = <span class=\"number\">0</span>;</span><br><span class=\"line\">       __syncthreads();</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">0</span>; k &lt; TILE_WIDTH; ++k)</span><br><span class=\"line\">           Pvalue += ds_M[ty][k] * ds_N[k][tx];</span><br><span class=\"line\">       __syncthreads();</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (row &lt; numARows &amp;&amp; col &lt; numBColumns)</span><br><span class=\"line\">       C[row * numBColumns + col] = Pvalue;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   CREATE_CUDAEVENT</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"type\">int</span> numARows = atoi(argv[<span class=\"number\">1</span>]); <span class=\"comment\">// number of rows in the matrix A</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numAColumns = atoi(argv[<span class=\"number\">2</span>]); <span class=\"comment\">// number of columns in the matrix A</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numBRows = atoi(argv[<span class=\"number\">3</span>]); <span class=\"comment\">// number of rows in the matrix B</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numBColumns = atoi(argv[<span class=\"number\">4</span>]); <span class=\"comment\">// number of columns in the matrix B</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numCRows = numARows; <span class=\"comment\">// number of rows in the matrix C</span></span><br><span class=\"line\">   <span class=\"type\">int</span> numCColumns = numBColumns; <span class=\"comment\">// number of columns in the matrix C</span></span><br><span class=\"line\">   assert(numAColumns == numBRows);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"type\">float</span> *A = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numARows*numAColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *B = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numBRows*numBColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *C = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   <span class=\"type\">float</span> *hostC = (<span class=\"type\">float</span> *)<span class=\"built_in\">malloc</span>(numCRows*numCColumns*<span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// Initialize matrices on the host</span></span><br><span class=\"line\">   initMatrix(A, numARows, numAColumns);</span><br><span class=\"line\">   initMatrix(B, numBRows, numBColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">   START_CUDAEVENT</span><br><span class=\"line\">   <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(A, B, C, numARows, numAColumns, numBRows, numBColumns)</span>;</span><br><span class=\"line\">   STOP_AND_PRINT_CUDAEVENT(compute CPU)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// CUDA PART</span></span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceA;</span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceB;</span><br><span class=\"line\">   <span class=\"type\">float</span> *deviceC;</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceA, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceB, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\">   cudaMalloc((<span class=\"type\">void</span> **)&amp;deviceC, numCRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemcpy(deviceA, A, numARows * numAColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\">   cudaMemcpy(deviceB, B, numBRows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemset(deviceC, <span class=\"number\">0</span>, numCRows * numCColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">   dim3 <span class=\"title function_\">blockDim</span><span class=\"params\">(<span class=\"number\">16</span>, <span class=\"number\">16</span>)</span>;</span><br><span class=\"line\">   dim3 <span class=\"title function_\">gridDim</span><span class=\"params\">(<span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numBColumns) / blockDim.x), <span class=\"built_in\">ceil</span>(((<span class=\"type\">float</span>)numARows) / blockDim.y))</span>;</span><br><span class=\"line\">   START_CUDAEVENT</span><br><span class=\"line\">   sgemm&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numBColumns);</span><br><span class=\"line\">   STOP_AND_PRINT_CUDAEVENT(compute GPU)</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaMemcpy(hostC, deviceC, numARows * numBColumns * <span class=\"keyword\">sizeof</span>(<span class=\"type\">float</span>), cudaMemcpyDeviceToHost);</span><br><span class=\"line\">   <span class=\"comment\">// END CUDA PART</span></span><br><span class=\"line\"></span><br><span class=\"line\">   compareMatrix(C, hostC, numCRows, numCColumns);</span><br><span class=\"line\"></span><br><span class=\"line\">   cudaFree(deviceA);</span><br><span class=\"line\">   cudaFree(deviceB);</span><br><span class=\"line\">   cudaFree(deviceC);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"built_in\">free</span>(A);</span><br><span class=\"line\">   <span class=\"built_in\">free</span>(B);</span><br><span class=\"line\">   <span class=\"built_in\">free</span>(C);</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *m, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numCols)</span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;numRows; i++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j=<span class=\"number\">0</span>; j&lt;numCols; j++)&#123;</span><br><span class=\"line\">           m[i*numCols+j] = <span class=\"built_in\">sin</span>(i*numCols+j);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">computeMatrixMulCPU</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">float</span> *C,</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">int</span> numARows, <span class=\"type\">int</span> numAColumns,</span></span><br><span class=\"line\"><span class=\"params\">   <span class=\"type\">int</span> numBRows, <span class=\"type\">int</span> numBColumns</span></span><br><span class=\"line\"><span class=\"params\">)</span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numARows; row++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numBColumns; col++)&#123;   </span><br><span class=\"line\">           C[row * numBColumns + col] = <span class=\"number\">0.0</span>;         </span><br><span class=\"line\">           <span class=\"keyword\">for</span> (<span class=\"type\">int</span> n = <span class=\"number\">0</span>; n &lt; numAColumns; n++)&#123;</span><br><span class=\"line\">               C[row * numBColumns + col] += A[row * numAColumns + n] * B[n * numBColumns + col];</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">compareMatrix</span><span class=\"params\">(<span class=\"type\">float</span> *A, <span class=\"type\">float</span> *B, <span class=\"type\">int</span> numRows, <span class=\"type\">int</span> numColumns)</span>&#123;</span><br><span class=\"line\">   <span class=\"type\">float</span> sum = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">   <span class=\"type\">float</span> max = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">   <span class=\"type\">float</span> min = <span class=\"number\">10.0</span>;</span><br><span class=\"line\">   <span class=\"keyword\">for</span> (<span class=\"type\">int</span> row = <span class=\"number\">0</span>; row &lt; numRows; row++)&#123;</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> col = <span class=\"number\">0</span>; col &lt; numColumns; col++)&#123;    </span><br><span class=\"line\">           <span class=\"type\">float</span> diff = A[row*numColumns+col] - B[row*numColumns+col];</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (diff &gt; max) max = diff;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (diff &lt; min) min = diff;</span><br><span class=\"line\">           sum += diff;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;mean: &quot;</span> &lt;&lt; sum / (numRows*numColumns) &lt;&lt; <span class=\"string\">&quot; max: &quot;</span> &lt;&lt; max &lt;&lt; <span class=\"string\">&quot; min: &quot;</span> &lt;&lt; min &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./addvec_answer3</span><br><span class=\"line\">&gt;&gt;&gt; Results <span class=\"keyword\">for</span> MemCopy</span><br><span class=\"line\">Time to [Classical] host allocation 0.1 ms</span><br><span class=\"line\">Time to [Classical] Initialize 117.5 ms</span><br><span class=\"line\">Time to [Classical] device allocation 29.5 ms</span><br><span class=\"line\">Time to [Classical] MemCopy Host to Device 64.8 ms</span><br><span class=\"line\">Time to [Classical] execution 14.8 ms</span><br><span class=\"line\">Time to [Classical] MemCopy Device to Host 26.2 ms</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt; Results <span class=\"keyword\">for</span> Pinned Memory</span><br><span class=\"line\">Time to [Pinned] host allocation 32.5 ms</span><br><span class=\"line\">Time to [Pinned] initialize 1083.4 ms</span><br><span class=\"line\">Time to [Pinned] device allocation 35.6 ms</span><br><span class=\"line\">Time to [Pinned] MemCopy Host to Device 34.3 ms</span><br><span class=\"line\">Time to [Pinned] execution 4.8 ms</span><br><span class=\"line\">Time to [Pinned] MemCopy Device to Host 3.3 ms</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt; Results <span class=\"keyword\">for</span> Unified Memory</span><br><span class=\"line\">Time to [Unified Memory] memory allocation 33.8 ms</span><br><span class=\"line\">Time to [Unified Memory] initialize 115.5 ms</span><br><span class=\"line\">Time to [Unified Memory] MemCopy Host to Device 0.0 ms</span><br><span class=\"line\">Time to [Unified Memory] execution 11.3 ms</span><br><span class=\"line\">Time to [Unified Memory] MemCopy Device to Host 0.0 ms</span><br></pre></td></tr></table></figure>\n<h2 id=\"TD8-直方图统计\"><a href=\"#TD8-直方图统计\" class=\"headerlink\" title=\"TD8 直方图统计\"></a>TD8 直方图统计</h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;text.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> NB_ASCII_CHAR 128</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">int</span> threadsPerBlock = <span class=\"number\">256</span>;</span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">int</span> BlockNumber = <span class=\"number\">16</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">histo_kernel</span><span class=\"params\">( <span class=\"type\">char</span> *buffer , <span class=\"type\">long</span> size , <span class=\"type\">int</span> *histo )</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"type\">int</span> i = threadIdx . x + blockIdx . x * blockDim . x ;</span><br><span class=\"line\"><span class=\"comment\">// stride is total number of threads</span></span><br><span class=\"line\"><span class=\"type\">int</span> stride = blockDim . x * gridDim . x ;</span><br><span class=\"line\"><span class=\"comment\">// All threads handle blockDim . x * gridDim . x</span></span><br><span class=\"line\"><span class=\"comment\">// consecutive elements</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> ( i &lt; size ) &#123;</span><br><span class=\"line\">        atomicAdd (&amp;(histo[buffer[i]]) , <span class=\"number\">1</span>) ;</span><br><span class=\"line\">        i += stride ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">histo_kernel_shared</span><span class=\"params\">( <span class=\"type\">char</span> *buffer , <span class=\"type\">long</span> size , <span class=\"type\">int</span> *histo)</span>&#123;</span><br><span class=\"line\">__shared__  <span class=\"type\">unsigned</span> <span class=\"type\">int</span> histo_private[NB_ASCII_CHAR];</span><br><span class=\"line\"><span class=\"keyword\">if</span> ( threadIdx.x &lt; NB_ASCII_CHAR) histo_private[threadIdx.x] = <span class=\"number\">0</span>;</span><br><span class=\"line\">__syncthreads () ;</span><br><span class=\"line\"><span class=\"type\">int</span> i = threadIdx.x + blockIdx.x * blockDim.x ;</span><br><span class=\"line\"><span class=\"comment\">// stride is total number of threads</span></span><br><span class=\"line\"><span class=\"type\">int</span> stride = blockDim.x * gridDim.x ;</span><br><span class=\"line\"><span class=\"keyword\">while</span> ( i &lt; size ) &#123;</span><br><span class=\"line\">    atomicAdd ( &amp;( histo_private[buffer[i]]) , <span class=\"number\">1</span>) ;</span><br><span class=\"line\">    i += stride ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// wait for all other threads in the block to finish</span></span><br><span class=\"line\">__syncthreads () ;</span><br><span class=\"line\"><span class=\"keyword\">if</span> ( threadIdx.x &lt; NB_ASCII_CHAR) &#123;</span><br><span class=\"line\">atomicAdd (&amp;( histo [ threadIdx.x ]) , histo_private[ threadIdx.x] ) ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">( <span class=\"type\">void</span> )</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> len = <span class=\"built_in\">strlen</span>(h_str);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;len:%d\\n&quot;</span>, len);</span><br><span class=\"line\">    <span class=\"type\">int</span> size = len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">char</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">char</span> *d_str;</span><br><span class=\"line\">    <span class=\"type\">int</span> *h_histo, *d_histo;</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEvent_t start, stop;</span><br><span class=\"line\">    cudaEventCreate( &amp;start );</span><br><span class=\"line\">    cudaEventCreate( &amp;stop );</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// GPU computation</span></span><br><span class=\"line\">    h_histo = (<span class=\"type\">int</span>*)<span class=\"built_in\">malloc</span>( len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>) );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMalloc( (<span class=\"type\">void</span>**)&amp;d_str, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">char</span>) );</span><br><span class=\"line\">    cudaMalloc( (<span class=\"type\">void</span>**)&amp;d_histo, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>) );</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord( start, <span class=\"number\">0</span> );</span><br><span class=\"line\">    cudaMemcpy( d_str, h_str, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">char</span>), cudaMemcpyHostToDevice );</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    histo_kernel_shared&lt;&lt;&lt;BlockNumber,threadsPerBlock&gt;&gt;&gt;( d_str, size, d_histo );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemcpy( h_histo, d_histo, NB_ASCII_CHAR*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>), cudaMemcpyDeviceToHost );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord( stop, <span class=\"number\">0</span> );</span><br><span class=\"line\">    cudaEventSynchronize( stop );</span><br><span class=\"line\">    <span class=\"type\">float</span> elapsedTime;</span><br><span class=\"line\">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Total time for  GPU computation with shared memory was %f ms\\n&quot;</span>, elapsedTime );</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class=\"line\">    <span class=\"comment\">//     std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; h_histo[bean] &lt;&lt; std::endl;</span></span><br><span class=\"line\">    <span class=\"comment\">// &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    cudaFree(d_histo);</span><br><span class=\"line\">    cudaFree(d_str);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_histo);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// GPU computation with shared memory</span></span><br><span class=\"line\"></span><br><span class=\"line\">    h_histo = (<span class=\"type\">int</span>*)<span class=\"built_in\">malloc</span>( len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>) );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMalloc( (<span class=\"type\">void</span>**)&amp;d_str, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">char</span>) );</span><br><span class=\"line\">    cudaMalloc( (<span class=\"type\">void</span>**)&amp;d_histo, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>) );</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord( start, <span class=\"number\">0</span> );</span><br><span class=\"line\">    cudaMemcpy( d_str, h_str, len*<span class=\"keyword\">sizeof</span>(<span class=\"type\">char</span>), cudaMemcpyHostToDevice );</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    histo_kernel&lt;&lt;&lt;BlockNumber,threadsPerBlock&gt;&gt;&gt;( d_str, size, d_histo );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaMemcpy( h_histo, d_histo, NB_ASCII_CHAR*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>), cudaMemcpyDeviceToHost );</span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord( stop, <span class=\"number\">0</span> );</span><br><span class=\"line\">    cudaEventSynchronize( stop );</span><br><span class=\"line\">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Total time for naive GPU computation was %f ms\\n&quot;</span>, elapsedTime );</span><br><span class=\"line\">    <span class=\"comment\">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class=\"line\">    <span class=\"comment\">//     std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; h_histo[bean] &lt;&lt; std::endl;</span></span><br><span class=\"line\">    <span class=\"comment\">// &#125;</span></span><br><span class=\"line\">    cudaFree(d_histo);</span><br><span class=\"line\">    cudaFree(d_str);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_histo);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// CPU computation</span></span><br><span class=\"line\"></span><br><span class=\"line\">    cudaEventRecord( start, <span class=\"number\">0</span> );</span><br><span class=\"line\"></span><br><span class=\"line\">    u_int histo[NB_ASCII_CHAR] = &#123;<span class=\"number\">0</span>&#125;;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; len; i++)&#123;</span><br><span class=\"line\">            histo[h_str[i]]++;</span><br><span class=\"line\">    &#125;    </span><br><span class=\"line\">    <span class=\"comment\">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class=\"line\">    <span class=\"comment\">//     std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; histo[bean] &lt;&lt; std::endl;</span></span><br><span class=\"line\">    <span class=\"comment\">// &#125;</span></span><br><span class=\"line\">    cudaEventRecord( stop, <span class=\"number\">0</span> );</span><br><span class=\"line\">    cudaEventSynchronize( stop );</span><br><span class=\"line\">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Total time for CPU computation was %f ms\\n&quot;</span>, elapsedTime );</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./hist_gpu</span><br><span class=\"line\">len:179709</span><br><span class=\"line\">Total time <span class=\"keyword\">for</span>  GPU computation with shared memory was 1.082448 ms</span><br><span class=\"line\">Total time <span class=\"keyword\">for</span> naive GPU computation was 2.703802 ms</span><br><span class=\"line\">Total time <span class=\"keyword\">for</span> CPU computation was 5.037396 ms</span><br></pre></td></tr></table></figure>\n<h2 id=\"TD9\"><a href=\"#TD9\" class=\"headerlink\" title=\"TD9\"></a>TD9</h2><h3 id=\"Pinned内存计算向量加法\"><a href=\"#Pinned内存计算向量加法\" class=\"headerlink\" title=\"Pinned内存计算向量加法\"></a>Pinned内存计算向量加法</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vector_add</span><span class=\"params\">(<span class=\"type\">int</span> *a, <span class=\"type\">int</span> *b, <span class=\"type\">int</span> *c, <span class=\"type\">int</span> n)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">/* insert code to calculate the index properly using blockIdx.x, blockDim.x, threadIdx.x */</span></span><br><span class=\"line\">        <span class=\"type\">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(index &lt; n) c[index] = a[index] + b[index];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* experiment with N */</span></span><br><span class=\"line\"><span class=\"comment\">/* how large can it be? */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N (2048*2048*10)</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> THREADS_PER_BLOCK 512</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* declare and create CUDA events */</span></span><br><span class=\"line\">        cudaEvent_t start, stop, mem_start, mem_stop;</span><br><span class=\"line\">        cudaEventCreate(&amp;start);</span><br><span class=\"line\">        cudaEventCreate(&amp;stop);</span><br><span class=\"line\">        cudaEventCreate(&amp;mem_start);</span><br><span class=\"line\">        cudaEventCreate(&amp;mem_stop);</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"type\">int</span> *a, *b, *c;</span><br><span class=\"line\">        <span class=\"type\">int</span> *d_a, *d_b, *d_c;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for device copies of a, b, c */</span></span><br><span class=\"line\">        cudaMalloc( &amp;d_a, size );</span><br><span class=\"line\">        cudaMalloc( &amp;d_b, size );</span><br><span class=\"line\">        cudaMalloc( &amp;d_c, size );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* allocate space for host copies of a, b, c and setup input values */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//a = (int *)malloc( size );</span></span><br><span class=\"line\">        cudaHostAlloc(&amp;a,size,cudaHostAllocDefault);</span><br><span class=\"line\">        <span class=\"comment\">//b = (int *)malloc( size );</span></span><br><span class=\"line\">        cudaHostAlloc(&amp;b,size,cudaHostAllocDefault);</span><br><span class=\"line\">        <span class=\"comment\">//c = (int *)malloc( size );</span></span><br><span class=\"line\">        cudaHostAlloc(&amp;c,size,cudaHostAllocDefault);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventRecord(mem_start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy inputs to device */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data to the device */</span></span><br><span class=\"line\">        cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\">        cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> blocks = <span class=\"built_in\">ceil</span>(N / ((<span class=\"type\">float</span>) THREADS_PER_BLOCK));</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventRecord(start, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* launch the kernel on the GPU */</span></span><br><span class=\"line\">        <span class=\"comment\">/* insert the launch parameters to launch the kernel properly using blocks and threads */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        vector_add&lt;&lt;&lt;blocks, THREADS_PER_BLOCK&gt;&gt;&gt;( d_a, d_b, d_c, N );</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventRecord(stop, <span class=\"number\">0</span>);</span><br><span class=\"line\">        cudaEventSynchronize(stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* copy result back to host */</span></span><br><span class=\"line\">        <span class=\"comment\">/* fix the parameters needed to copy data back to the host */</span></span><br><span class=\"line\">        cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaEventRecord(mem_stop, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>] );</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>,N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>] );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* compute and print ellapsed time between start and stop */</span></span><br><span class=\"line\">        <span class=\"type\">float</span> elapsedTime, elapsedTotalTime;</span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class=\"line\">        cudaEventElapsedTime(&amp;elapsedTotalTime, mem_start, mem_stop);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Time to load memory %3.1f ms\\n&quot;</span>, elapsedTotalTime - elapsedTime);</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Time to execute %3.1f ms\\n&quot;</span>, elapsedTime);</span><br><span class=\"line\">        cudaEventDestroy(start);</span><br><span class=\"line\">        cudaEventDestroy(stop);</span><br><span class=\"line\">        cudaEventDestroy(mem_start);</span><br><span class=\"line\">        cudaEventDestroy(mem_stop);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/*free(a);</span></span><br><span class=\"line\"><span class=\"comment\">        free(b);</span></span><br><span class=\"line\"><span class=\"comment\">        free(c);*/</span></span><br><span class=\"line\">        cudaFree( d_a );</span><br><span class=\"line\">        cudaFree( d_b );</span><br><span class=\"line\">        cudaFree( d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125; <span class=\"comment\">/* end main */</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ ./addvec</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[41943039] = 83886078</span><br><span class=\"line\">Time to load memory 234.5 ms</span><br><span class=\"line\">Time to execute 35.0 ms</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"流计算向量加法\"><a href=\"#流计算向量加法\" class=\"headerlink\" title=\"流计算向量加法\"></a>流计算向量加法</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> N (2048*2048)</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> THREADS_PER_BLOCK 512</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> NB_STREAMS 4</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> SEGMENT_SIZE (1024*128)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;start); \\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventCreate(&amp;stop);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventSynchronize(stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">&#123;float elapsedTime;\\</span></span><br><span class=\"line\"><span class=\"meta\">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\\</span></span><br><span class=\"line\"><span class=\"meta\">printf(<span class=\"string\">&quot;Time to %s %3.1f ms\\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vector_add</span><span class=\"params\">(<span class=\"type\">int</span> *a, <span class=\"type\">int</span> *b, <span class=\"type\">int</span> *c)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class=\"line\">        c[index] = a[index] + b[index];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">stream_addition</span><span class=\"params\">(<span class=\"type\">int</span> *a, <span class=\"type\">int</span> *b, <span class=\"type\">int</span> *c)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"comment\">/*&lt; Add your code here, you can use the kernel without change it &gt;*/</span></span><br><span class=\"line\"><span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> )/NB_STREAMS;</span><br><span class=\"line\"></span><br><span class=\"line\">cudaStream_t stream1 , stream2 , stream3, stream4;</span><br><span class=\"line\">cudaStreamCreate (&amp; stream1 ) ;</span><br><span class=\"line\">cudaStreamCreate (&amp; stream2 ) ;</span><br><span class=\"line\">cudaStreamCreate (&amp; stream3 ) ;</span><br><span class=\"line\">cudaStreamCreate (&amp; stream4 ) ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> *d_a1, *d_b1, *d_c1;</span><br><span class=\"line\"><span class=\"type\">int</span> *d_a2, *d_b2, *d_c2;</span><br><span class=\"line\"><span class=\"type\">int</span> *d_a3, *d_b3, *d_c3;</span><br><span class=\"line\"><span class=\"type\">int</span> *d_a4, *d_b4, *d_c4;</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMalloc( &amp;d_a1, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_b1, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_c1, size );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMalloc( &amp;d_a2, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_b2, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_c2, size );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMalloc( &amp;d_a3, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_b3, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_c3, size );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMalloc( &amp;d_a4, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_b4, size );</span><br><span class=\"line\">cudaMalloc( &amp;d_c4, size );</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> ( <span class=\"type\">int</span> i =<span class=\"number\">0</span>; i &lt; N ; i += SEGMENT_SIZE*NB_STREAMS) &#123;</span><br><span class=\"line\">cudaMemcpyAsync( d_a1 , a +i , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ), cudaMemcpyHostToDevice, stream1 ) ;</span><br><span class=\"line\">cudaMemcpyAsync( d_b1 , b +i , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ), cudaMemcpyHostToDevice, stream1 ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMemcpyAsync( d_a2 , a +i + SEGMENT_SIZE , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ),cudaMemcpyHostToDevice,  stream2 ) ;</span><br><span class=\"line\">cudaMemcpyAsync( d_b2 , b +i + SEGMENT_SIZE, SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyHostToDevice, stream2 ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMemcpyAsync( d_a3 , a +i + <span class=\"number\">2</span>*SEGMENT_SIZE, SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyHostToDevice, stream3 ) ;</span><br><span class=\"line\">cudaMemcpyAsync( d_b3 , b +i + <span class=\"number\">2</span>* SEGMENT_SIZE, SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyHostToDevice, stream3 ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMemcpyAsync( d_a4 , a +i + <span class=\"number\">3</span>*SEGMENT_SIZE , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyHostToDevice, stream4 ) ;</span><br><span class=\"line\">cudaMemcpyAsync( d_b4 , b +i + <span class=\"number\">3</span>*SEGMENT_SIZE , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyHostToDevice, stream4 ) ;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class=\"number\">0</span>,stream1&gt;&gt;&gt;( d_a1, d_b1, d_c1);</span><br><span class=\"line\">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class=\"number\">0</span>,stream2&gt;&gt;&gt;( d_a2, d_b2, d_c2);</span><br><span class=\"line\">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class=\"number\">0</span>,stream3&gt;&gt;&gt;( d_a3, d_b3, d_c3);</span><br><span class=\"line\">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class=\"number\">0</span>,stream4&gt;&gt;&gt;( d_a4, d_b4, d_c4);</span><br><span class=\"line\"></span><br><span class=\"line\">cudaMemcpyAsync( c + i, d_c1, SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) , cudaMemcpyDeviceToHost,stream1 );</span><br><span class=\"line\">cudaMemcpyAsync( c +i+SEGMENT_SIZE , d_c2 , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) , cudaMemcpyDeviceToHost,stream2 );</span><br><span class=\"line\">cudaMemcpyAsync( c+i+<span class=\"number\">2</span>*SEGMENT_SIZE, d_c3 , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyDeviceToHost, stream3 );</span><br><span class=\"line\">cudaMemcpyAsync( c+i+<span class=\"number\">3</span>*SEGMENT_SIZE, d_c4 , SEGMENT_SIZE * <span class=\"keyword\">sizeof</span> ( <span class=\"type\">int</span> ) ,cudaMemcpyDeviceToHost, stream4 );</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">cudaDeviceSynchronize();</span><br><span class=\"line\">cudaFree( d_a1 );</span><br><span class=\"line\">cudaFree( d_b1 );</span><br><span class=\"line\">cudaFree( d_c1 );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaFree( d_a2 );</span><br><span class=\"line\">cudaFree( d_b2 );</span><br><span class=\"line\">cudaFree( d_c2 );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaFree( d_a3 );</span><br><span class=\"line\">cudaFree( d_b3 );</span><br><span class=\"line\">cudaFree( d_c3 );</span><br><span class=\"line\"></span><br><span class=\"line\">cudaFree( d_a4 );</span><br><span class=\"line\">cudaFree( d_b4 );</span><br><span class=\"line\">cudaFree( d_c4 );</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">addition</span><span class=\"params\">(<span class=\"type\">int</span> *a, <span class=\"type\">int</span> *b, <span class=\"type\">int</span> *c)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        CREATE_CUDAEVENT</span><br><span class=\"line\">        <span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> );</span><br><span class=\"line\">        <span class=\"type\">int</span> *d_a, *d_b, *d_c;</span><br><span class=\"line\"></span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_a, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_b, size );</span><br><span class=\"line\">        cudaMalloc( (<span class=\"type\">void</span> **) &amp;d_c, size );</span><br><span class=\"line\"></span><br><span class=\"line\">        START_CUDAEVENT</span><br><span class=\"line\">        <span class=\"title function_\">cudaMemcpy</span><span class=\"params\">( d_a, a, size, cudaMemcpyHostToDevice )</span>;</span><br><span class=\"line\">        cudaMemcpy( d_b, b, size, cudaMemcpyHostToDevice );</span><br><span class=\"line\">        STOP_AND_PRINT_CUDAEVENT(<span class=\"built_in\">memcpy</span> h2d)</span><br><span class=\"line\"></span><br><span class=\"line\">        START_CUDAEVENT</span><br><span class=\"line\">        vector_add&lt;&lt;&lt; (N + (THREADS_PER_BLOCK<span class=\"number\">-1</span>)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class=\"line\">        STOP_AND_PRINT_CUDAEVENT(computation)</span><br><span class=\"line\"></span><br><span class=\"line\">        START_CUDAEVENT</span><br><span class=\"line\">        <span class=\"title function_\">cudaMemcpy</span><span class=\"params\">( c, d_c, size, cudaMemcpyDeviceToHost )</span>;</span><br><span class=\"line\">        STOP_AND_PRINT_CUDAEVENT(<span class=\"built_in\">memcpy</span> d2h)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\">        cudaFree( d_a );</span><br><span class=\"line\">        cudaFree( d_b );</span><br><span class=\"line\">        cudaFree( d_c );</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> *a, *b, *c;</span><br><span class=\"line\">        <span class=\"type\">int</span> size = N * <span class=\"keyword\">sizeof</span>( <span class=\"type\">int</span> );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Pinned memory */</span></span><br><span class=\"line\">        cudaHostAlloc((<span class=\"type\">void</span> **) &amp;a, size, cudaHostAllocDefault);</span><br><span class=\"line\">        cudaHostAlloc((<span class=\"type\">void</span> **) &amp;b, size, cudaHostAllocDefault);</span><br><span class=\"line\">        cudaHostAlloc((<span class=\"type\">void</span> **) &amp;c, size, cudaHostAllocDefault);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                a[i] = b[i] = i;</span><br><span class=\"line\">                c[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Addition with default stream\\n&quot;</span>);</span><br><span class=\"line\">        addition(a, b, c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>] );</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>,N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>] );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i++ )</span><br><span class=\"line\">        &#123;        c[i] = <span class=\"number\">0</span>;         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/*&lt; Add a call to your function with streams &gt;*/</span></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Addition with streams\\n&quot;</span>);</span><br><span class=\"line\">        stream_addition(a, b, c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[0] = %d\\n&quot;</span>,c[<span class=\"number\">0</span>] );</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>, N<span class=\"number\">-1</span>, c[N<span class=\"number\">-1</span>] );</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>, N/<span class=\"number\">4</span><span class=\"number\">-1</span>, c[N/<span class=\"number\">4</span><span class=\"number\">-1</span>] );</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>( <span class=\"string\">&quot;c[%d] = %d\\n&quot;</span>, <span class=\"number\">6</span>, c[<span class=\"number\">6</span>] );</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* clean up */</span></span><br><span class=\"line\">        cudaFreeHost(a);</span><br><span class=\"line\">        cudaFreeHost(b);</span><br><span class=\"line\">        cudaFreeHost(c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./stream</span><br><span class=\"line\">Addition with default stream</span><br><span class=\"line\">Time to memcpy h2d 32.3 ms</span><br><span class=\"line\">Time to computation 18.6 ms</span><br><span class=\"line\">Time to memcpy d2h 17.1 ms</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[4194303] = 8388606</span><br><span class=\"line\">Addition with streams</span><br><span class=\"line\">c[0] = 0</span><br><span class=\"line\">c[4194303] = 8388606</span><br><span class=\"line\">c[1048575] = 2097150</span><br><span class=\"line\">c[6] = 12</span><br></pre></td></tr></table></figure>\n"},{"title":"共享内存和同步","url":"/2023/02/19/GPU/%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E4%B8%8E%E5%90%8C%E6%AD%A5/","content":"<h2 id=\"共享内存和同步\"><a href=\"#共享内存和同步\" class=\"headerlink\" title=\"共享内存和同步\"></a>共享内存和同步</h2><h3 id=\"共享内存\"><a href=\"#共享内存\" class=\"headerlink\" title=\"共享内存\"></a>共享内存</h3><p>是一种特殊类型的内存，其内容在内核的源代码中明确定义和使用。 它具有以下特点：</p>\n<ul>\n<li>出现在每一个SM</li>\n<li>以比全局内存更高的速度访问（在延迟和吞吐量方面）</li>\n<li>范围仅限于构成块的线程</li>\n<li>块生命周期，一旦线程完成执行，内容就会消失</li>\n<li>通过加载（load）和存储（store）指令访问</li>\n<li>一种“暂存器”类型的存储器</li>\n<li>限制大小 — Jetson Nano：“每个 SM 的最大共享内存量 64 KB”和“每个线程块的最大共享内存量 48 KB”</li>\n</ul>\n<ol>\n<li>声明：<br><a href=\"https://postimg.cc/1fRkJByQ\"><img src=\"https://i.postimg.cc/YqQt2DVh/image.png\" alt=\"image.png\"></a></li>\n</ol>\n<h3 id=\"同步\"><a href=\"#同步\" class=\"headerlink\" title=\"同步\"></a>同步</h3><p>调用__syncthreads保证块中的每个线程都在 __syncthreads() 之前完成其指令。在共享内存加载数据指令后使用，保证数据加载完成；用在计算部分之后，保证所有threads完成计算。</p>\n<h3 id=\"瓦片大小\"><a href=\"#瓦片大小\" class=\"headerlink\" title=\"瓦片大小\"></a>瓦片大小</h3><ul>\n<li><p>每个线程块必须有多个线程：</p>\n<p>16 的 TILE_WIDTH 给出 16 × 16 &#x3D; 256 个线程</p>\n<p>32 的 TILE_WIDTH 给出 32 × 32 &#x3D; 1024 个线程</p>\n<ul>\n<li>对于 16，在每个阶段，每个块从全局内存执行 2 × 256 &#x3D; 512 次浮点负载，进行 256 × (2 × 16) &#x3D; 8192 次乘法&#x2F;加法操作。 （每次内存加载 16 次浮点运算）</li>\n<li>对于 32，在每个阶段，每个块执行 2 × 1024 &#x3D; 2048 次从全局内存加载浮点数，进行 1024 × (2 × 32) &#x3D; 65536 次乘法&#x2F;加法操作。（每次内存加载 32 次浮点运算）</li>\n</ul>\n</li>\n</ul>\n<p>链接：计算能力（具有 64 KB 共享内存的 SM）</p>\n<p>共享内存的大小取决于实现！</p>\n<ul>\n<li>对于TILE_WIDTH&#x3D;16，每个线程块使用2<em>256</em>4B&#x3D;2KB的共享内存<ul>\n<li>对于 64 KB 的共享内存，最多可以有 32 个执行操作的线程块</li>\n<li>这允许最多 32*512 &#x3D; 16,384 个待处理负载（每个线程 2 个，每个块 256 个线程）</li>\n</ul>\n</li>\n<li>对于TILE_WIDTH&#x3D;32，每个线程块使用2<em>32</em>32*4B&#x3D;8KB的共享内存<ul>\n<li>对于 64 KB 的共享内存，最多可以有 8 个执行操作的线程块</li>\n<li>这允许最多 32*2048 &#x3D; 65536 个待处理负载（每个线程 2 个，每个块 1024 个线程）</li>\n</ul>\n</li>\n</ul>\n<p>每个<code>__syncthread()</code> 可以减少一个块的活动线程数：更多的线程块可能是有益的</p>\n<ul>\n<li>优点：降低内存带宽对并行内核性能的限制作用</li>\n<li>步骤：<ul>\n<li>识别由多个线程使用的全局内存块</li>\n<li>将图块从全局内存加载到 SM 上的内存中</li>\n<li>使用屏障同步确保所有线程都完成加载内存</li>\n<li>有多个线程从共享内存访问他们的数据</li>\n<li>使用屏障同步来确保所有线程都完成了它们的计算</li>\n<li>移动到下一个图块</li>\n</ul>\n</li>\n<li>矩阵乘法的例子：对于瓦片要求的数据超过矩阵时，定为0<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">MatrixMulKernel</span><span class=\"params\">(<span class=\"type\">float</span> *M, <span class=\"type\">float</span> *N,<span class=\"type\">float</span> *P,<span class=\"type\">int</span> width )</span><span class=\"comment\">// square matrix</span></span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     <span class=\"keyword\">for</span>(<span class=\"type\">int</span> p = <span class=\"number\">0</span>; p &lt; (Width <span class=\"number\">-1</span>) / TILE_WIDTH + <span class=\"number\">1</span>; ++p) &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span>( row &lt; Width &amp;&amp; t * TILE_WIDTH +tx &lt; width )&#123;</span><br><span class=\"line\">             ds_M [ty ][ tx]= M[ row * Width + p* TILE_WIDTH +tx ];</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         <span class=\"keyword\">else</span></span><br><span class=\"line\">             &#123; ds_M [ty ][ tx] = <span class=\"number\">0.0</span>; &#125;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (p* TILE_WIDTH +ty &lt; Width &amp;&amp; col &lt; width ) &#123;</span><br><span class=\"line\">             ds_N [ty ][ tx] = N[(t* TILE_WIDTH +ty)* Width + col ];</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         <span class=\"keyword\">else</span></span><br><span class=\"line\">             &#123; ds_M [ty ][ tx] = <span class=\"number\">0.0</span>; &#125;</span><br><span class=\"line\">         __syncthreads ();</span><br><span class=\"line\">         <span class=\"keyword\">if</span>( row &lt; width &amp;&amp; col &lt; width ) &#123;</span><br><span class=\"line\">             <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; TILE_WIDTH ; ++i) &#123;</span><br><span class=\"line\">                 Pvalue += ds_M [ty ][i] * ds_N [i][ tx ];</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         __syncthreads ();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> ( row &lt; width &amp;&amp; col &lt; width ) &#123;</span><br><span class=\"line\">          P[ row * Width + col ] = Pvalue ;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"内存管理","url":"/2023/02/19/GPU/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","content":"<ul>\n<li>内存等级：<ul>\n<li>兆字节(mégaoctet)的易失性高速缓存：快速且昂贵</li>\n<li>千兆字节(gigaoctet)的易失性存储器访问：快速和平均价格</li>\n<li>太字节(teraoctet)的非易失性大容量存储：缓慢且廉价</li>\n</ul>\n</li>\n</ul>\n<p><strong>内存管理器</strong>是操作系统的实体，它管理内存层次结构以及它们之间的协调。</p>\n<ul>\n<li><p>内存结构与代码联系<br><a href=\"https://postimg.cc/fVpfSz8P\"><img src=\"https://i.postimg.cc/wBTfP3L9/image.png\" alt=\"image.png\"></a></p>\n</li>\n<li><p>地址空间：</p>\n<ul>\n<li>地址空间是内存中可供程序或进程使用的有效地址范围，即它是程序或进程可以访问的内存。</li>\n<li>每个程序都有自己的空间，因此一个程序的地址 42 与另一个程序的地址 42 对应于不同的内存位置。</li>\n<li>地址空间是程序使用的内存的一种抽象</li>\n</ul>\n</li>\n<li><p>动态重新分配</p>\n<ul>\n<li>添加了两个寄存器：基址寄存器和限制寄存器。</li>\n<li>在运行时，操作系统将程序开始和结束的物理地址加载到这两个寄存器中。</li>\n<li>我们将基地址添加到所有内存引用中（并检查该地址是否低于限制地址）。<blockquote>\n<p>默认：需要进行加法和比较两个操作. (比较速度快，但是加法速度慢-因为有进位)</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>如果物理内存不足以容纳所有进程怎么办？</p>\n<ul>\n<li>va-et-vient (swapping)<ul>\n<li>预留额外的内存(tas et pile)</li>\n<li>内存压缩技术（很少使用）</li>\n<li>空闲内存管理：结构（表格，列表）和算法</li>\n</ul>\n</li>\n<li>mémoire virtuelle<ul>\n<li>每个程序都有自己的地址空间，分为小实体，les pages</li>\n<li>页面被映射到物理内存，但不一定所有页面都可以运行程序</li>\n<li>虚拟寻址和物理寻址之间的对应是由硬件（内存管理单元，<strong>MMU</strong>）即时完成的</li>\n<li>如果页面未被引用，操作系统将加载丢失的空间</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>页码(Pagination)</p>\n<ul>\n<li>虚拟地址空间分为页面</li>\n<li>物理内存中相应的单元称为页框。</li>\n<li>页面和框架大小相同（示例中为 4kb）</li>\n<li>RAM 和磁盘之间的传输总是整页<br><a href=\"https://postimg.cc/gXSJrJDX\"><img src=\"https://i.postimg.cc/vTM6qxxz/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n<li><p>页面置换的算法</p>\n<p>选择不常用的页面框架；写入所选帧的磁盘；将新引用的页面转移到释放的框架中；修改页表；继续操作</p>\n</li>\n<li><p>转换后备缓冲区(TLB)</p>\n<ul>\n<li>问题：页面匹配需要快速并且是性能限制</li>\n<li>观察：程序访问少量页面</li>\n<li>应用：MMU 的本地内存，带有一个小型查找表（Translation Lookaside Buffer，TLB）。TLB 记住与处理器必须访问的最后一页相对应的最后对（页、帧），这大大提高了内存访问时间</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"CPU-GPU内存传递\"><a href=\"#CPU-GPU内存传递\" class=\"headerlink\" title=\"CPU-GPU内存传递\"></a>CPU-GPU内存传递</h3><p>对 cudaMemcpy() 的调用使用直接内存访问 (DMA) 硬件单元以提高效率。</p>\n<ul>\n<li>DMA使用物理内存：在每次 DMA 传输开始时，对所有源和目标区域的页面进行地址转换并检查是否存在。同一DMA传输的其余部分不进行地址转换，从而效率高。<blockquote>\n<p>操作系统可能会意外删除 DMA 读取或写入的数据，并在同一物理位置插入另一个虚拟页面。</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"固定内存的使用-Pinned\"><a href=\"#固定内存的使用-Pinned\" class=\"headerlink\" title=\"固定内存的使用(Pinned)\"></a>固定内存的使用(Pinned)</h3><p>cudaMemcpy() 使用的 DMA 要求将主机内存中的任何源或目标分配为固定内存。</p>\n<ul>\n<li>固定内存(Pinned)&#x2F;Page Locked Memory&#x2F;Lock Pages<ul>\n<li>固定内存由标记为无法调出的虚拟内存页组成；通过特殊系统API函数调用分配</li>\n<li>作为 DMA 传输源或目标的 CPU 内存必须分配为固定内存</li>\n<li>调用 cudaMemcpy() 时，如果主机内存中的源或目标未分配到固定内存中，则必须首先将其复制到固定内存（额外开销）</li>\n<li>如果主机内存源或目标是在固定内存中分配的，则 cudaMemcpy() 会更快，因为不需要额外的副本。</li>\n</ul>\n</li>\n<li>在CUDA中使用固定内存<ul>\n<li>cudaHostAlloc()：指向已分配内存的指针地址(&amp;A)；分配的内存大小(以字节为单位)；使用cudaHoatAllocDefault选项</li>\n<li>以与 malloc() 和 free() 返回的方式相同的方式使用在主机上分配的内存及其指针，唯一的区别是分配的内存不能被操作系统调出</li>\n<li>cudaMemcpy() 函数在固定内存的情况下应该快大约 2 倍<blockquote>\n<p>固定内存是一种有限的资源。 过度使用会产生严重的后果。</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"统一内存-Unified-Memory\"><a href=\"#统一内存-Unified-Memory\" class=\"headerlink\" title=\"统一内存 Unified Memory\"></a>统一内存 Unified Memory</h3><p>统一内存在 CUDA 6.0 中首次引入。它是一个公共内存空间，主机和设备将其视为具有公共地址空间的单个内存。</p>\n<p>硬件&#x2F;软件自动处理主机和设备之间的数据迁移，保持它们之间的一致性，而无需进行显式内存复制调用（消除对 cudaMemcpy 例程的调用）</p>\n<p>统一内存允许：通过以一致的方式统一内存空间来简化 GPU 编程；通过将数据透明地迁移到使用它的处理器来最大化数据访问速度。</p>\n<ul>\n<li>系统要求：GPU框架3.0或更高；64位的主机和合适的操作系统</li>\n<li>使用：不再需要主机和设备之间的显式内存传输。在内存空间中创建的任何分配都会自动迁移到需要的地方。<ul>\n<li>程序按以下两种方式分配内存：<ul>\n<li>通过<code>cudaMallocManaged()</code>,相似于<code>cudamalloc</code></li>\n<li>通过定义全局变量<code>__managed__</code>,在语义上类似于<code>__device__</code><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">AplusB</span> <span class=\"params\">( <span class=\"type\">int</span> *ret , <span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span> &#123;</span><br><span class=\"line\">    ret [ threadIdx .x] = a + b + threadIdx .x;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span> <span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> *ret ;</span><br><span class=\"line\">    cudaMalloc(&amp; ret,<span class=\"number\">1000</span>*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    AplusB &lt;&lt;&lt;<span class=\"number\">1</span>,<span class=\"number\">1000</span>&gt;&gt;&gt;(ret,<span class=\"number\">10</span>,<span class=\"number\">100</span>) ;</span><br><span class=\"line\">    <span class=\"type\">int</span> * host_ret=(<span class=\"type\">int</span>*)<span class=\"built_in\">malloc</span>(<span class=\"number\">1000</span>*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    cudaMemcpy(host_ret,ret,<span class=\"number\">1000</span>*<span class=\"keyword\">sizeof</span> (<span class=\"type\">int</span> ),cudaMemcpyDefault);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">1000</span>; i++)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span> (<span class=\"string\">&quot;%d: A+B = %d\\n&quot;</span>, i, host_ret [i]) ;</span><br><span class=\"line\">    <span class=\"built_in\">free</span> ( host_ret ) ;</span><br><span class=\"line\">    cudaFree (ret );</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">AplusB</span> <span class=\"params\">( <span class=\"type\">int</span> *ret , <span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span> &#123;</span><br><span class=\"line\">    ret [ threadIdx .x] = a + b + threadIdx .x;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span> <span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> *ret ;</span><br><span class=\"line\">    cudaMallocManaged(&amp;&amp; ret,<span class=\"number\">1000</span>*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    AplusB&lt;&lt;&lt;<span class=\"number\">1</span>,<span class=\"number\">1000</span>&gt;&gt;&gt;(ret,<span class=\"number\">10</span>,<span class=\"number\">100</span>);</span><br><span class=\"line\">    cudaDeviceSynchronize();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">1000</span>; i++)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span> (<span class=\"string\">&quot;%d: A+B = %d\\n&quot;</span>, i, ret [i]) ;</span><br><span class=\"line\">    cudaFree (ret );</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>cudaMallocManaged()</code> 例程为主机代码和设备代码返回一个有效指针。 这允许在没有单独的 host_ret 副本的情况下使用 ret，从而简化并减小了程序的大小</li>\n<li>在非统一示例中，<code>cudaMemcpy()</code>例程既用于同步内核（即等待它完成运行），也用于将数据传输到主机。 因此，在主机程序可以安全地使用 GPU 输出之前，使用统一内存需要一个显式的 <code>cudaDeviceSynchronize()</code>。<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__device__ __managed__ <span class=\"type\">int</span> ret [<span class=\"number\">1000</span>];</span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">AplusB</span> <span class=\"params\">( <span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span> &#123;</span><br><span class=\"line\">ret [ threadIdx .x] = a + b + threadIdx .x;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span> <span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    AplusB&lt;&lt;&lt;<span class=\"number\">1</span>,<span class=\"number\">1000</span>&gt;&gt;&gt;(ret,<span class=\"number\">10</span>,<span class=\"number\">100</span>);</span><br><span class=\"line\">    cudaDeviceSynchronize();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( <span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">1000</span>; i++)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span> (<span class=\"string\">&quot;%d: A+B = %d\\n&quot;</span>, i, ret [i]) ;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>使用全局变量时进一步简化程序。</li>\n<li>缺少 cudaMemcpy() 调用和 ret 返回数组在 CPU 和 GPU 上都是可见的。</li>\n</ul>\n<h3 id=\"数据迁移和一致性\"><a href=\"#数据迁移和一致性\" class=\"headerlink\" title=\"数据迁移和一致性\"></a>数据迁移和一致性</h3><ul>\n<li>统一内存尝试通过将数据移动到主机内存（如果被 CPU 访问）和设备内存（如果被 GPU 访问）来优化内存性能</li>\n<li>数据迁移对程序是透明的。 系统尝试将数据放在访问效率最高的地方，而不违反一致性</li>\n<li>数据的物理位置对程序来说是不可见的，并且可以随时更改，但是对数据虚拟地址的访问将保持有效并从任何处理器保持一致，无论位置如何。 在性能之前，一致性是主要要求</li>\n<li>计算能力低于 6.x 的 GPU 架构不支持按需将托管数据精细移动到 GPU。 每当启动 GPU 内核时，通常需要将所有托管内存转移到 GPU 内存中，以避免内存访问错误</li>\n</ul>\n"},{"title":"原子指令","url":"/2023/02/19/GPU/%E5%8E%9F%E5%AD%90%E6%8C%87%E4%BB%A4/","content":"<h2 id=\"原子指令\"><a href=\"#原子指令\" class=\"headerlink\" title=\"原子指令\"></a>原子指令</h2><ul>\n<li>分段切片导致内存访问效率低下<ul>\n<li>线程不访问相邻数据</li>\n<li>访问未分组（合并）</li>\n<li>DRAM 带宽未得到充分利用<br><a href=\"https://postimg.cc/sM2x55M6\"><img src=\"https://i.postimg.cc/htLmWrcB/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n<li>优化：将sections交织<ul>\n<li>线程在连续的部分中进行</li>\n<li>访问因此被分组（合并）<br><a href=\"https://postimg.cc/LgR65PdV\"><img src=\"https://i.postimg.cc/x8NksGtB/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n<li>原子操作原理<ul>\n<li>读取-修改-写入操作在单个硬件指令中执行</li>\n<li>硬件保证在当前原子操作完成之前，没有其他线程可以在同一位置执行另一个读-修改-写操作。<ul>\n<li>任何其他试图在同一个地方执行原子操作的线程都将被放入队列中</li>\n<li>所有线程对相同的内存地址串行执行它们的原子操作</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>CUDA 中的原子算术运算<ul>\n<li>通过调用翻译成单个语句的函数来执行：加法、减法、递增、递减、最小值、最大值、交换、CAS（比较和交换）</li>\n<li>原子加法：<code>int atomicAdd(int* adress, int val)</code>在这一条指令中包括3个操作：<ul>\n<li>从全局或共享内存中地址指向的位置读取32位数据</li>\n<li>计算加法：读取数据+val</li>\n<li>返回结果到相同地址</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>性能<ul>\n<li>原子操作从 DRAM 中的读取开始，延迟为几百个周期；原子操作以在同一个地方写入结束，有几百个周期的延迟；在此期间，没有其他人可以访问该地址</li>\n<li>每个读取-修改-写入都会引起两次内存访问延迟</li>\n<li>一个变量（DRAM 位置）上的所有原子操作都被序列化<blockquote>\n<p>优化: 使用privasation，共享内存记录结果，统一进行读取和写入</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"共享内存优化\"><a href=\"#共享内存优化\" class=\"headerlink\" title=\"共享内存优化\"></a>共享内存优化</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__ <span class=\"type\">void</span> <span class=\"title function_\">histo_kernel</span> <span class=\"params\">( <span class=\"type\">unsigned</span> <span class=\"type\">char</span> * buffer , <span class=\"type\">long</span> size , <span class=\"type\">unsigned</span> <span class=\"type\">int</span> * histo )</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    __shared__ <span class=\"type\">unsigned</span> <span class=\"type\">int</span> histo_private [<span class=\"number\">7</span>];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ( threadIdx .x &lt; <span class=\"number\">7</span>) histo_private [threadidx .x] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    __syncthreads () ;</span><br><span class=\"line\">    <span class=\"type\">int</span> i = threadIdx .x + blockIdx .x * blockDim .x;</span><br><span class=\"line\">    <span class=\"comment\">// stride 是线程总数</span></span><br><span class=\"line\">    <span class=\"type\">int</span> stride = blockDim .x * gridDim .x;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt; size ) &#123;</span><br><span class=\"line\">        atomicAdd ( &amp;( private_histo [ buffer [i ]/<span class=\"number\">4</span>) , <span class=\"number\">1</span>) ;</span><br><span class=\"line\">        i += stride ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 等待块中所有有线程完成</span></span><br><span class=\"line\">    __syncthreads () ;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ( threadIdx .x &lt; <span class=\"number\">7</span>) &#123;</span><br><span class=\"line\">        atomicAdd (&amp;( histo [ threadIdx .x]) , private_histo [ threadIdx .x] );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"title":"合并内存","url":"/2023/02/19/GPU/%E5%90%88%E5%B9%B6%E5%86%85%E5%AD%98/","content":"<p>CUDA 设备的全局内存是用 DRAM 实现的；DRAM 很慢</p>\n<ul>\n<li>DRAM突发<ul>\n<li>现代 DRAM 使用并行性来提高其数据访问速率，通常称为内存访问吞吐量。</li>\n<li>每次访问 DRAM 插槽时，都会访问包含所请求插槽的一系列连续插槽，并且在每个 DRAM 芯片中提供多个传感器并并行操作，每个传感器检测这些连续位置中的位内容。</li>\n<li>一旦被传感器检测到，来自所有这些连续位置的数据可以以非常高的速度传输到处理器。 这些被访问和交付的连续插槽称为 DRAM 突发。</li>\n</ul>\n</li>\n<li>合并内存条件：<ul>\n<li>当所有线程执行一条加载指令时，如果所有访问的插槽都落在<strong>同一个突发段</strong>中，则只会发出一个 DRAM 请求，并且访问完全合并</li>\n<li>当可访问地址在多个突发段上时：合并失败，发出多个DRAM请求；访问未完全合并，访问和传输的某些字节未被线程使用</li>\n</ul>\n</li>\n<li>应用：矩阵相乘时，左边的矩阵(按行处理)满足进行合并的条件。右边的矩阵(按列处理)不满足条件。</li>\n<li>左&#x2F;右边的矩阵<br><a href=\"https://postimg.cc/BtXBq1Z0\"><img src=\"https://i.postimg.cc/xjPx3KWc/image.png\" alt=\"matrix\"></a></li>\n<li>矩阵数据的读取<ul>\n<li>左边：<a href=\"https://postimg.cc/D8fJ29Pb\"><img src=\"https://i.postimg.cc/kXKxy9Vw/image.png\" alt=\"image.png\"></a></li>\n<li>右边：<br><a href=\"https://postimg.cc/k6X3Vv1w\"><img src=\"https://i.postimg.cc/TwnR4kr8/image.png\" alt=\"image.png\"></a></li>\n<li>优化方法：使用共享内存</li>\n</ul>\n</li>\n</ul>\n"},{"title":"并行缩减算法","url":"/2023/02/19/GPU/%E5%B9%B6%E8%A1%8C%E7%BC%A9%E5%87%8F%E7%AE%97%E6%B3%95/","content":"<ul>\n<li>处理大量输入数据的常用策略<ul>\n<li>数据集中的处理元素没有要求的顺序</li>\n<li>将数据集拆分成更小的块</li>\n<li>有一个线程来处理一个块</li>\n<li>构建归约树以聚合每个块的结果</li>\n</ul>\n</li>\n<li>缩减N个数的树，平均并行数：(N-1)&#x2F;Log(N); 最大并行数：N&#x2F;2</li>\n<li>使用共享内存的缩减：<ul>\n<li>初始向量在设备的全局内存中</li>\n<li>共享内存包含部分和</li>\n<li>最初，共享内存包含初始向量</li>\n<li>每一步都使部分和更接近总和</li>\n<li>最后的总和将在共享向量的元素 0 中</li>\n<li>通过读取和写入值减少整体内存流量</li>\n<li>最大块大小必须小于或等于 2048<br><a href=\"https://postimg.cc/rDQkCHBx\"><img src=\"https://i.postimg.cc/mrGgRfVX/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n<li>初始代码：<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">redux</span> <span class=\"params\">( <span class=\"type\">float</span> * input , <span class=\"type\">float</span> * ouput )</span> &#123;</span><br><span class=\"line\">    __shared__ <span class=\"type\">float</span> max [<span class=\"number\">2</span>* BLOCK_SIZE ];  <span class=\"comment\">//每块给2×blockDim.x个元素</span></span><br><span class=\"line\">    <span class=\"type\">int</span> t = threadIdx .x;</span><br><span class=\"line\">    <span class=\"type\">int</span> start = <span class=\"number\">2</span>* blockIdx .x* blockDim .x;</span><br><span class=\"line\">    max [t] = input [ start + t];</span><br><span class=\"line\">    max [ blockDim +t] = input [ start + blockDim .x+t];</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> stride = <span class=\"number\">1</span>; stride &lt;= blockDim .x; stride *= <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        __syncthreads () ;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t % stride == <span class=\"number\">0</span>) <span class=\"comment\">//每个线程每次处理两个元素</span></span><br><span class=\"line\">            max [<span class=\"number\">2</span>* t] = ( ( max [<span class=\"number\">2</span>* t] &gt; max [<span class=\"number\">2</span>* t+ stride ]) ? max[<span class=\"number\">2</span>* t] : max [<span class=\"number\">2</span>* t+ stride ]) ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ouput [ blockIdx .x] = max [<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>代码优化：每次只保留有效的线程，避免不执行的线程消耗资源<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">redux</span> <span class=\"params\">( <span class=\"type\">float</span> * input , <span class=\"type\">float</span> * ouput )</span> &#123;</span><br><span class=\"line\">    __shared__ <span class=\"type\">float</span> max [<span class=\"number\">2</span>* BLOCK_SIZE ];</span><br><span class=\"line\">    <span class=\"type\">int</span> t = threadIdx .x;</span><br><span class=\"line\">    <span class=\"type\">int</span> start = <span class=\"number\">2</span>* blockIdx .x* blockDim .x;</span><br><span class=\"line\">    max [t] = input [ start + t];</span><br><span class=\"line\">    max [ blockDim +t] = input [ start + blockDim .x+t];</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> stride = blockDim .x; stride &gt; <span class=\"number\">0</span>; stride /= <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#123;  <span class=\"comment\">//stride的初始值和迭代方式改变</span></span><br><span class=\"line\">        __syncthreads () ;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t &lt; stride )</span><br><span class=\"line\">            max [<span class=\"number\">2</span>* t] = ( ( max [<span class=\"number\">2</span>* t] &gt; max [<span class=\"number\">2</span>* t+ stride ]) ? max [<span class=\"number\">2</span>* t] : max [<span class=\"number\">2</span>* t+ stride ]) ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ouput [ blockIdx .x] = max [<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"线程与块","url":"/2023/02/19/GPU/%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%9D%97/","content":"<p>作者：nililili（nililili7876.cn）</p>\n<h2 id=\"Allocation-des-donnees-et-execution-d’un-kernel\"><a href=\"#Allocation-des-donnees-et-execution-d’un-kernel\" class=\"headerlink\" title=\"Allocation des données et exécution d’un kernel\"></a>Allocation des données et exécution d’un kernel</h2><p>正常的一个C</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Compute vector sum C = A + B</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vecAdd</span> <span class=\"params\">( <span class=\"type\">float</span> * h_A , <span class=\"type\">float</span> *h_B , <span class=\"type\">float</span> * h_C , <span class=\"type\">int</span> n)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i&lt;n; i ++) &#123;</span><br><span class=\"line\">\t\th_C [i] = h_A [i] + h_B [i];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>如果用cuda写</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># <span class=\"keyword\">include</span> <span class=\"string\">&lt; cuda .h &gt;</span></span></span><br><span class=\"line\">__global__</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vecAddKernel</span> <span class=\"params\">( <span class=\"type\">float</span> * A, <span class=\"type\">float</span> * B, <span class=\"type\">float</span> * C, <span class=\"type\">int</span> n)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = threadIdx .x + blockDim .x* blockIdx .x;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(i&lt;n) C[i] = A[i] + B[i];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">vecAdd</span> <span class=\"params\">( <span class=\"type\">float</span> * h_A , <span class=\"type\">float</span> * h_B , <span class=\"type\">float</span> * h_C , <span class=\"type\">int</span> n)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> size = n* <span class=\"keyword\">sizeof</span> ( <span class=\"type\">float</span> ) ;</span><br><span class=\"line\">\t<span class=\"type\">float</span> *d_A , *d_B , * d_C;</span><br><span class=\"line\">\t<span class=\"comment\">//Part 1: Allocate device memory for A, B, and C</span></span><br><span class=\"line\">\tcudaMalloc (( <span class=\"type\">void</span> **) &amp;d_A , size ) ;</span><br><span class=\"line\">\tcudaMalloc (( <span class=\"type\">void</span> **) &amp;d_B , size ) ;</span><br><span class=\"line\">\tcudaMalloc (( <span class=\"type\">void</span> **) &amp;d_C , size ) ;</span><br><span class=\"line\">\t<span class=\"comment\">// Part 1: copy A and B to device memory</span></span><br><span class=\"line\">    cudaMemcpy (d_A , h_A , size , cudaMemcpyHostToDevice );</span><br><span class=\"line\">\tcudaMemcpy (d_B , h_B , size , cudaMemcpyHostToDevice );</span><br><span class=\"line\">\t<span class=\"comment\">// Part 2: Kernel launch code - the device performs the actual vector addition</span></span><br><span class=\"line\">    <span class=\"type\">int</span> DimGrid = <span class=\"built_in\">ceil</span> (n /<span class=\"number\">256.0</span>) ;</span><br><span class=\"line\">\t<span class=\"type\">int</span> DimBlock = <span class=\"number\">256</span>;</span><br><span class=\"line\">\tvecAddKernel &lt;&lt; &lt; DimGrid , DimBlock &gt; &gt; &gt;(d_A , d_B , d_C , n);</span><br><span class=\"line\">\t<span class=\"comment\">// Part 3: copy C from the device memory</span></span><br><span class=\"line\">    cudaMemcpy (h_C , d_C , size , cudaMemcpyDeviceToHost );</span><br><span class=\"line\">\t<span class=\"comment\">// Free</span></span><br><span class=\"line\">    cudaFree( d_A ); cudaFree (d_B) ; cudaFree ( d_C ) ;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>程序的三部分示意图：</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.15484o9dwdpc.png\" alt=\"yaimage\"></p>\n<h3 id=\"Kernel\"><a href=\"#Kernel\" class=\"headerlink\" title=\"Kernel\"></a>Kernel</h3><p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.5671ve9tpn40.png\" alt=\"yaimage\"></p>\n<ul>\n<li>同个block里面的threads通过共享内存、原子操作、barrier synchronization[阻碍同步]合作，不同block里面的线程不合作</li>\n<li>一个线程可以被看作是一个冯诺依曼类型的处理器，一个块里面的所有线程执行同一个操作[SIMD]</li>\n<li><strong>Thread index</strong>: 每个线程都有一个索引，可以用来操作内存地址并做出执行选择&#x3D;&#x3D;&gt;<code>i = blockIdx.x*blockDimx.x + threadIdx.x</code></li>\n<li>DimGrid:对应块&#x3D;<code>ceil(n/256.0)</code>，DimBlock:对应块里面的线程数&#x3D;256</li>\n</ul>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221214/yaimage.4xmreiua28w0.png\" alt=\"yaimage\"></p>\n<h3 id=\"Key-words\"><a href=\"#Key-words\" class=\"headerlink\" title=\"Key words\"></a>Key words</h3><table>\n<thead>\n<tr>\n<th></th>\n<th>执行</th>\n<th>使用</th>\n<th>注意</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__device__</code></td>\n<td>device&#x2F;GPU</td>\n<td>device&#x2F;GPU</td>\n<td>可以与<code>__host__</code>一起使用</td>\n</tr>\n<tr>\n<td><code>__global__</code></td>\n<td>device&#x2F;GPU</td>\n<td>host&#x2F;CPU</td>\n<td>定义kernel函数，返回值必须是void</td>\n</tr>\n<tr>\n<td><code>__host__</code></td>\n<td>host&#x2F;CPU</td>\n<td>host&#x2F;CPU</td>\n<td>可以与<code>__device__</code>一起使用</td>\n</tr>\n</tbody></table>\n<p>这里有个奇怪的话：<code>__device__</code> et<code> __host__</code> peuvent être utilisés conjointement pour avoir deux versions objet de la même fonction[已获得两个函数的相同版本].</p>\n<h3 id=\"blockIdx\"><a href=\"#blockIdx\" class=\"headerlink\" title=\"blockIdx\"></a>blockIdx</h3><p>blockIdx</p>\n<p>threadIdx</p>\n<p>blockDim: 线程块的大小，本质是3维的blockDim.x，blockDim.y,blockDim.z，但我们一般用1或者2维。</p>\n<p>gridDim: 整个网格的分块情况，也是有3个。</p>\n<h2 id=\"Multidim\"><a href=\"#Multidim\" class=\"headerlink\" title=\"Multidim\"></a>Multidim</h2><p>Les index de block et de thread sont des variables à trois dimensions</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.1cq5hwu3wun4.png\" alt=\"yaimage\"></p>\n<h3 id=\"Pseudo-algo-CPU\"><a href=\"#Pseudo-algo-CPU\" class=\"headerlink\" title=\"Pseudo-algo CPU\"></a>Pseudo-algo CPU</h3><ul>\n<li><p>RGB和灰度图的转换：Pour chaque pixel (r, v, b) en position (i, j)：$grayPixel[i,j]&#x3D;0.21<em>r+0.71</em>g+0.07*b$</p>\n</li>\n<li><p>Row-Major Order en C&#x2F;C++：$Row*Width+Col$</p>\n<p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.63d4i06rs9w0.png\" alt=\"yaimage\"></p>\n</li>\n</ul>\n<p>&#x3D;&#x3D;&gt;Pseudo-algo CPU</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">for ii from 0 to height do</span><br><span class=\"line\">\tfor jj from 0 to width do</span><br><span class=\"line\">\t\tidx = ii * width + jj</span><br><span class=\"line\">\t\tr = input [3* idx ]</span><br><span class=\"line\">\t\tg = input [3* idx + 1]</span><br><span class=\"line\">\t\tb = input [3* idx + 2]</span><br><span class=\"line\">\t\tgrayImage [ idx ] = (0.21* r + 0.71* g + 0.07* b)</span><br><span class=\"line\">\tend</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Kernel-1\"><a href=\"#Kernel-1\" class=\"headerlink\" title=\"Kernel\"></a>Kernel</h3><p><img src=\"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230108/yaimage.7v02me85gw8.png\" alt=\"yaimage\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># <span class=\"keyword\">define</span> CHANNELS 3 <span class=\"comment\">// we have 3 channels corresponding to RGB</span></span></span><br><span class=\"line\"><span class=\"comment\">// The input image is encoded as unsigned characters [0 , 255]</span></span><br><span class=\"line\">__global__<span class=\"comment\">//device执行，host用</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">colorConvert</span> <span class=\"params\">( <span class=\"type\">unsigned</span> <span class=\"type\">char</span> * grayImage ,<span class=\"type\">unsigned</span> <span class=\"type\">char</span> * rgbImage ,<span class=\"type\">int</span> width , <span class=\"type\">int</span> height )</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> x = threadIdx .x + blockIdx .x * blockDim .x; <span class=\"comment\">// Col</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> y = threadIdx .y + blockIdx .y * blockDim .y; <span class=\"comment\">// Row</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (x &lt; width &amp;&amp; y &lt; height )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// get 1D coordinate for the grayscale image</span></span><br><span class=\"line\">\t\t<span class=\"type\">int</span> grayOffset = y* width + x;</span><br><span class=\"line\">\t\t<span class=\"comment\">// one can think of the RGB image having CHANNEL times columns than the gray scale image</span></span><br><span class=\"line\">\t\t<span class=\"type\">int</span> rgbOffset = grayOffset * CHANNELS ;</span><br><span class=\"line\">\t\t<span class=\"type\">unsigned</span> <span class=\"type\">char</span> r = rgbImage [ rgbOffset ]; <span class=\"comment\">// red value for pixel</span></span><br><span class=\"line\">\t\t<span class=\"type\">unsigned</span> <span class=\"type\">char</span> g = rgbImage [ rgbOffset + <span class=\"number\">2</span>]; <span class=\"comment\">// green value for pixel</span></span><br><span class=\"line\">\t\t<span class=\"type\">unsigned</span> <span class=\"type\">char</span> b = rgbImage [ rgbOffset + <span class=\"number\">3</span>]; <span class=\"comment\">// blue value for pixel</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// perform the rescaling and store it</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// We multiply by floating point constants</span></span><br><span class=\"line\">\t\tgrayImage [ grayOffset ] = <span class=\"number\">0.21</span> f*r + <span class=\"number\">0.71</span> f*g + <span class=\"number\">0.07</span> f*b;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//调用部分：图片的大小是m*n</span></span><br><span class=\"line\"><span class=\"comment\">//...</span></span><br><span class=\"line\">dim3 <span class=\"title function_\">DimGrid</span> <span class=\"params\">((n <span class=\"number\">-1</span>) /<span class=\"number\">16</span> + <span class=\"number\">1</span> , (m <span class=\"number\">-1</span>) /<span class=\"number\">16</span>+<span class=\"number\">1</span> , <span class=\"number\">1</span>)</span> ;</span><br><span class=\"line\">dim3 <span class=\"title function_\">DimBlock</span> <span class=\"params\">(<span class=\"number\">16</span> , <span class=\"number\">16</span> , <span class=\"number\">1</span>)</span> ;</span><br><span class=\"line\">colorConvert &lt;&lt;&lt; DimGrid , DimBlock &gt; &gt; &gt;( grayImage , rgbImage , width , height ) ;</span><br><span class=\"line\"><span class=\"comment\">//...</span></span><br></pre></td></tr></table></figure>\n"},{"title":"Git","url":"/2023/02/20/QCM/Git/","content":"<table>\n<thead>\n<tr>\n<th>作用</th>\n<th>命令</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>创建空目录</td>\n<td>mkdir repo</td>\n</tr>\n<tr>\n<td>进入目录</td>\n<td>cd repo</td>\n</tr>\n<tr>\n<td>将目录初始化为git仓库</td>\n<td>git init</td>\n</tr>\n<tr>\n<td>添加文件到仓库</td>\n<td>git add fiche.txt<br>git add –all</td>\n</tr>\n<tr>\n<td>提交文件到仓库</td>\n<td>git commit [-m “commente”]</td>\n</tr>\n<tr>\n<td>查看仓库状态</td>\n<td>git status</td>\n</tr>\n<tr>\n<td>查看不同</td>\n<td>git diff [fiche.txt]</td>\n</tr>\n<tr>\n<td>查看提交日志(查找回退版本)</td>\n<td>git log [–pretty&#x3D;oneline]</td>\n</tr>\n<tr>\n<td>回退到上一版本<br>退回前N个的版本<br>来到版本号[80e0a…]对应的版本</td>\n<td>git reset –hard HEAD^<br>git reset –hard HEAD~N<br>git reset –hard 80e0a</td>\n</tr>\n<tr>\n<td>查看文件内容</td>\n<td>cat fiche.txt</td>\n</tr>\n<tr>\n<td>查看命令历史（查找未来版本）</td>\n<td>git reflog</td>\n</tr>\n<tr>\n<td>查看文件在工作区和仓库最新版间的区别</td>\n<td>git diff HEAD – fiche.txt</td>\n</tr>\n<tr>\n<td>回到文件最后一次git add或git commit的状态</td>\n<td>git checkout – fiche.txt</td>\n</tr>\n<tr>\n<td>撤销暂存区的修改，放回工作区</td>\n<td>git reset HEAD fiche.txt</td>\n</tr>\n<tr>\n<td>从版本库中删除文件</td>\n<td>git rm fiche.txt<br>git commit -m ‘remove fiche.txt’</td>\n</tr>\n<tr>\n<td>关联远程仓库</td>\n<td>git  remote add origin SSH</td>\n</tr>\n<tr>\n<td>推送本地当前分支master到远程</td>\n<td>git push -u origin master</td>\n</tr>\n<tr>\n<td>查看远程库信息</td>\n<td>git remote -v</td>\n</tr>\n<tr>\n<td>删除远程库(关联关系)</td>\n<td>git remote rm <name></td>\n</tr>\n<tr>\n<td>从远程克隆一个本地仓库</td>\n<td>git clone SSH</td>\n</tr>\n<tr>\n<td>创建分支</td>\n<td>git branch newbranch</td>\n</tr>\n<tr>\n<td>切换分支(到master)</td>\n<td>git checkout master<br>git switch master</td>\n</tr>\n<tr>\n<td>创建并切换到新分支</td>\n<td>git checkout -b newbranch<br>git switch -c newbranch</td>\n</tr>\n<tr>\n<td>查看分支信息</td>\n<td>git branch</td>\n</tr>\n<tr>\n<td>将另一个分支的信息合并到当前分支上</td>\n<td>git merge autrebranch</td>\n</tr>\n<tr>\n<td>删除分支</td>\n<td>git branch -d branchname</td>\n</tr>\n<tr>\n<td>不实用快速合并，合并分支</td>\n<td>git merge –no-ff -m “comment” branchname</td>\n</tr>\n<tr>\n<td>存储当前分支工作状态</td>\n<td>git stash</td>\n</tr>\n<tr>\n<td>查看stash列表<br>恢复最新的stash，在列表中删除该stash<br>恢复指定的stash，不改变列表</td>\n<td>git stash list<br>git stash pop<br>git stash apply stash@{0}</td>\n</tr>\n</tbody></table>\n<h2 id=\"版本回退\"><a href=\"#版本回退\" class=\"headerlink\" title=\"版本回退\"></a>版本回退</h2><ol>\n<li>Git 中HEAD表示当前版本，上一个版本HEAD<del>, 上上一个版本HEAD~~ ，也可以写作HEAD</del>2。</li>\n</ol>\n<h2 id=\"工作区和暂存区\"><a href=\"#工作区和暂存区\" class=\"headerlink\" title=\"工作区和暂存区\"></a>工作区和暂存区</h2><ul>\n<li>工作区：电脑中能看到的目录</li>\n<li>版本库：.git目录是git的版本库。<ol>\n<li>git add将文件添加到暂存区</li>\n<li>git commit将暂存区所有内容提交到当前分支<br><a href=\"https://postimg.cc/9DSyQyV9\"><img src=\"https://i.postimg.cc/SKQdVGvg/image.png\" alt=\"image.png\"></a></li>\n</ol>\n</li>\n</ul>\n<h2 id=\"管理修改\"><a href=\"#管理修改\" class=\"headerlink\" title=\"管理修改\"></a>管理修改</h2><ul>\n<li>第一次修改-&gt;git add-&gt;第二次修改-&gt;git commit: 只提交第一次修改。第二次修改因为没有添加到暂存区，所以不会被提交</li>\n<li>git checkout – fiche.txt:把fiche.txt文件在工作区的修改全部撤销。如果文件自修改后还未被放到暂存区，则会返回到和版本库一模一样的状态；如果文件添加到暂存区后又做了修改，则会回到添加到暂存区后的状态。<ol>\n<li>当不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD <file>，从暂存区撤销该文件；第二步用命令git checkout – file丢掉工作区的修改</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"远程仓库\"><a href=\"#远程仓库\" class=\"headerlink\" title=\"远程仓库\"></a>远程仓库</h2><ol>\n<li>在github上创建空仓库</li>\n<li>在本地仓库下运行命令如下，来关联远程仓库。添加后远程库的名字是origin<br>git remote add origin <a href=\"mailto:&#x67;&#105;&#x74;&#x40;&#103;&#105;&#x74;&#104;&#117;&#98;&#46;&#99;&#111;&#x6d;\">&#x67;&#105;&#x74;&#x40;&#103;&#105;&#x74;&#104;&#117;&#98;&#46;&#99;&#111;&#x6d;</a>&#x2F;JeanneSHen&#x2F;git-etude.git</li>\n<li>将本地仓库的所有内容推送到远程仓库上：<br>git push -u origin master 把当前分支master推送到远程。<br>-u:第一次推送master分支时，加上该参数，Git会将本地和远程的master分支关联起来，方便后续操作。</li>\n</ol>\n<h2 id=\"从远程库克隆\"><a href=\"#从远程库克隆\" class=\"headerlink\" title=\"从远程库克隆\"></a>从远程库克隆</h2><p>从零开始开发时，最好先创建远程库，然后从远程库克隆</p>\n<ol>\n<li>在github上创建远程库</li>\n<li>用命令git clone克隆一个本地库<br> git clone <a href=\"mailto:&#x67;&#x69;&#116;&#64;&#103;&#x69;&#x74;&#x68;&#117;&#98;&#46;&#99;&#111;&#109;\">&#x67;&#x69;&#116;&#64;&#103;&#x69;&#x74;&#x68;&#117;&#98;&#46;&#99;&#111;&#109;</a>…</li>\n</ol>\n<h2 id=\"分支管理\"><a href=\"#分支管理\" class=\"headerlink\" title=\"分支管理\"></a>分支管理</h2><p>在master分支上创建新的分支，在新分支上进行修改提交，工作完成后将新分支合并到master上，然后删除dev分支</p>\n<ul>\n<li>分支的创建与合并<ol>\n<li>git checkout -b newbranch 中-b参数表示创建并切换或者使用git switch -c newbranch</li>\n<li>git merge newbranch：合并newbranch到当前分支，使用‘快进模式’</li>\n<li>git branch -d branchname : 删除分支指针</li>\n</ol>\n</li>\n<li>解决冲突<ol>\n<li>当master和newbranch都有修改时，git无法执行快速合并，只能试图把各自的修改合并起来。<blockquote>\n<p>Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内</p>\n</blockquote>\n</li>\n</ol>\n</li>\n<li>分支管理策略</li>\n</ul>\n<ol>\n<li>git merge –no-ff -m “comment” branchname: 普通模式合并分支，合并后的历史显示分支，能看处曾经做过合并</li>\n</ol>\n<ul>\n<li>Bug 分支<br>对每个bug，都可以通过一个新的临时分支来修复。但是，当目前的工作任务尚未完成，没办法提交，有需要紧急处理bug时，使用stash功能<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ git stash</span><br><span class=\"line\">Saved working directory and index state WIP on 当前分支</span><br><span class=\"line\">$ git checout master</span><br><span class=\"line\">$ git switch -c issue-101</span><br><span class=\"line\">$ git add readme.txt</span><br><span class=\"line\">$ git commit -m <span class=\"string\">&quot;fix bug 101&quot;</span></span><br><span class=\"line\">$ git switch master</span><br><span class=\"line\">$ git merge --no-ff -m <span class=\"string\">&quot;merged bug fix 101&quot;</span> issue-101</span><br><span class=\"line\">$ git switch dev</span><br><span class=\"line\">$ git stash list  <span class=\"comment\"># 查看stash列表</span></span><br><span class=\"line\">$ git stash pop  <span class=\"comment\"># 恢复最新的stash，并在列表中删除</span></span><br><span class=\"line\">$ git stash apply stash@&#123;0&#125;  <span class=\"comment\"># 回复指定的stash，但不改变列表</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"Pytest","url":"/2023/02/20/QCM/Pytest/","content":"<h2 id=\"assert-断言\"><a href=\"#assert-断言\" class=\"headerlink\" title=\"assert 断言\"></a>assert 断言</h2><p>进行条件判断</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># pytest tests/test1.py</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_passing</span>():</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>) == (<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用pytest 进行测试函数<br><code>$ pytest tests/test1.py</code><blockquote>\n<p>Pytest 使用.标识测试成功，使用F标识测试失败. 可以使用-v选项，显示测试具体信息. 使用pytest -h查看pytest的所有选项</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"pytest-raise-捕获异常\"><a href=\"#pytest-raise-捕获异常\" class=\"headerlink\" title=\"pytest.raise()捕获异常\"></a>pytest.raise()捕获异常</h2><p>在测试过程中，经常需要测试是否如期抛出预期的异常，以确定异常处理模块生效。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_raises.py</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_raises</span>():</span><br><span class=\"line\">    <span class=\"keyword\">with</span> pytest.raises(TypeError) <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        connect(<span class=\"string\">&#x27;localhost&#x27;</span>, <span class=\"string\">&#x27;6379&#x27;</span>)</span><br><span class=\"line\">    exec_msg = e.value.args[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> exec_msg == <span class=\"string\">&#x27;port type must be int&#x27;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"标记函数\"><a href=\"#标记函数\" class=\"headerlink\" title=\"标记函数\"></a>标记函数</h2><p>默认情况下，pytest 会递归查找当前目录下所有以 test 开始或结尾的 Python 脚本，并执行文件内的所有以 test 开始或结束的函数和方法。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_no_mark.py</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_func1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">1</span> == <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_func2</span>():</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">1</span> != <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ pytest tests/test-function/test_no_mark.py</span><br><span class=\"line\">默认两个函数都执行</span><br></pre></td></tr></table></figure>\n<p>若我们只想执行指定的测试函数：</p>\n<ul>\n<li>显式指定函数名，通过 <code>::</code>标记<br> <code>$ pytest tests/test-function/test_no_mark.py::test_func1</code></li>\n<li>使用模糊匹配，-k选项标识<br> <code>pytest -k func1 tests/test-function/test_no_mark.py</code></li>\n<li>使用 pytest.mark 在函数上进行标记。测试时使用-m选择标记的测试函数<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_with mark.py</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.finished</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_func1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">1</span> == <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.unfinished</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_func2</span>():</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">1</span> != <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<code>$ pytest -m finished tests/test-function/test_with_mark.py</code></li>\n</ul>\n<h2 id=\"跳过测试\"><a href=\"#跳过测试\" class=\"headerlink\" title=\"跳过测试\"></a>跳过测试</h2><p>Pytest 使用特定的标记 <code>@pytest.mark.skip</code>指定要跳过的测试.<br>Pytest 还支持使用 <code>@pytest.mark.skipif</code> 为测试函数指定被忽略的条件</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_skip.py</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.skip(<span class=\"params\">reason=<span class=\"string\">&#x27;out-of-date api&#x27;</span></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_connect</span>():</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.skipif(<span class=\"params\">conn.__version__ &lt; <span class=\"string\">&#x27;0.2.0&#x27;</span>,reason=<span class=\"string\">&#x27;not supported until v0.2.0&#x27;</span></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_api</span>():</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"预见的错误\"><a href=\"#预见的错误\" class=\"headerlink\" title=\"预见的错误\"></a>预见的错误</h2><p>  Pytest 使用 <code>pytest.mark.xfail</code> 实现预见错误功能, 即事先知道测试函数会执行失败，但又不想直接跳过，而是希望显示的提示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_xfail.py</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.xfail(<span class=\"params\">gen.__version__ &lt; <span class=\"string\">&#x27;0.2.0&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                   reason=<span class=\"string\">&#x27;not supported until v0.2.0&#x27;</span></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_api</span>():</span><br><span class=\"line\">    id_1 = gen.unique_id()</span><br><span class=\"line\">    id_2 = gen.unique_id()</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> id_1 != id_2</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Pytest 使用x表示预见的失败。如果预见的是失败，但是实际运行测试却通过，pytest使用x进行标记</p>\n</blockquote>\n<h2 id=\"参数化\"><a href=\"#参数化\" class=\"headerlink\" title=\"参数化\"></a>参数化</h2><ul>\n<li>在 pytest 中，使用pytest.mark.parametrize(argnames, argvalues)进行参数化测试，即每组参数都独立执行一次测试。<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_parametrize.py</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.parametrize(<span class=\"params\"><span class=\"string\">&#x27;passwd&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        [<span class=\"string\">&#x27;123456&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        <span class=\"string\">&#x27;abcdefdfs&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        <span class=\"string\">&#x27;as52345fasdf4&#x27;</span>]</span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_passwd_length</span>(<span class=\"params\">passwd</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(passwd) &gt;= <span class=\"number\">8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 多参数的例子，用于校验用户密码</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.parametrize(<span class=\"params\"><span class=\"string\">&#x27;user, passwd&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        [(<span class=\"params\"><span class=\"string\">&#x27;jack&#x27;</span>, <span class=\"string\">&#x27;abcdefgh&#x27;</span></span>),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        (<span class=\"params\"><span class=\"string\">&#x27;tom&#x27;</span>, <span class=\"string\">&#x27;a123456a&#x27;</span></span>)]</span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_passwd_md5</span>(<span class=\"params\">user, passwd</span>):</span><br><span class=\"line\">    db = &#123;</span><br><span class=\"line\">          <span class=\"string\">&#x27;jack&#x27;</span>: <span class=\"string\">&#x27;e8dc4081b13434b45189a720b77b6818&#x27;</span>,</span><br><span class=\"line\">          <span class=\"string\">&#x27;tom&#x27;</span>: <span class=\"string\">&#x27;1702a132e769a623c1adb78353fc9503&#x27;</span>&#125;</span><br><span class=\"line\">          <span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\">          <span class=\"keyword\">assert</span> hashlib.md5(passwd.encode()).hexdigest() == db[user]</span><br></pre></td></tr></table></figure></li>\n<li>用<code>-v</code>进行测试<br><code>$ pytest -v tests/test-function/test_parametrize.py::test_passwd_md5</code></li>\n<li>如果觉得每组测试的默认参数显示不清晰，我们可以使用 pytest.param 的 id 参数进行自定义。<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_parametrize.py</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.mark.parametrize(<span class=\"params\"><span class=\"string\">&#x27;user, passwd&#x27;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                        [pytest.param(<span class=\"params\"><span class=\"string\">&#x27;jack&#x27;</span>, <span class=\"string\">&#x27;abcdefgh&#x27;</span></span>),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">                         pytest.param(<span class=\"params\"><span class=\"string\">&#x27;tom&#x27;</span>, <span class=\"string\">&#x27;a123456a&#x27;</span></span>)]</span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_passwd_md5</span>(<span class=\"params\">user, passwd</span>):</span><br><span class=\"line\">    db = &#123;</span><br><span class=\"line\">          <span class=\"string\">&#x27;jack&#x27;</span>: <span class=\"string\">&#x27;e8dc4081b13434b45189a720b77b6818&#x27;</span>,</span><br><span class=\"line\">          <span class=\"string\">&#x27;tom&#x27;</span>: <span class=\"string\">&#x27;1702a132e769a623c1adb78353fc9503&#x27;</span>&#125;</span><br><span class=\"line\">          <span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\">          <span class=\"keyword\">assert</span> hashlib.md5(passwd.encode()).hexdigest() == db[user]</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"跟踪测试的时间\"><a href=\"#跟踪测试的时间\" class=\"headerlink\" title=\"跟踪测试的时间\"></a>跟踪测试的时间</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># content of test_some_are_slow.py</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_funcfast</span>():</span><br><span class=\"line\">    time.sleep(<span class=\"number\">0.1</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_funcslow1</span>():</span><br><span class=\"line\">    time.sleep(<span class=\"number\">0.2</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_funcslow2</span>():</span><br><span class=\"line\">    time.sleep(<span class=\"number\">0.3</span>)</span><br></pre></td></tr></table></figure>\n<p>返回运行最慢的3个测试函数<br><code>$ pytest --durations=3 test_some_are_slow.py</code><br>返回运行时间长于1s的5个函数<br><code>$ pytest --durations=5 --durations-min=1.0 test_some_are_slow.py</code></p>\n<h2 id=\"固件\"><a href=\"#固件\" class=\"headerlink\" title=\"固件\"></a>固件</h2><p>固件（Fixture）是一些函数，pytest 会在执行测试函数之前（或之后）加载运行它们。最常见的可能就是数据库的初始连接和最后关闭操作。<br>传入函数名称作为参数，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_postcode.py</span></span><br><span class=\"line\"><span class=\"comment\"># 只返回北京邮编</span></span><br><span class=\"line\"><span class=\"meta\">@pytest.fixture()</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">postcode</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;010&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_postcode</span>(<span class=\"params\">postcode</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> postcode == <span class=\"string\">&#x27;010&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>为方便固件的复用，Pytest 使用文件 conftest.py 集中管理固件。<br>不要自己显式调用 conftest.py，pytest 会自动调用，可以把 conftest 当做插件来理解。</p>\n<h2 id=\"预处理和后处理\"><a href=\"#预处理和后处理\" class=\"headerlink\" title=\"预处理和后处理\"></a>预处理和后处理</h2><p>fixture以yield为界限，在这之前即是执行测试用例之前需要执行的语句(setup)，在这之后是测试用例执行完成后继续执行的语句(teardown)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># test_fixture.py</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pytest</span><br><span class=\"line\"><span class=\"meta\">@pytest.fixture()</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">connection</span>():</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start connection&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">yield</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;connection closed&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#固件可以通过测试函数名称传入</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_one</span>(<span class=\"params\">connection</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">1</span>==<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;test_one&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_two</span>(<span class=\"params\">connection</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">2</span>==<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;test_two&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p><code>$ pytest -s test_fixture.py</code><br>使用 -s 显示隐藏信息</p>\n<ul>\n<li>作用域</li>\n<li>自动执行</li>\n<li>重命名</li>\n<li>参数化</li>\n<li>内置固件</li>\n</ul>\n"},{"title":"SQL","url":"/2023/02/19/QCM/SQL/","content":"<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><ol>\n<li>问题建模<br>  a. 理解问题：定义场景, 找出信息，确定关系<br>  b.   建立模型 ：现实世界 – 概念模型 – 物理模型 – sql语言 – 数据库<br>  <a href=\"https://postimg.cc/hfBdwCjf\"><img src=\"https://i.postimg.cc/15gr821c/image.png\" alt=\"image.png\"></a></li>\n<li>定义<br>  a. Base de données rationnelles : 确定问题，提出可能的solution，系统建模，实施solution，测试提出的solution，维护和评测系统<br>  b. Système de gestion de bases de données (SGBD) : 描述BD的结构，处理数据，使用数据，确保数据的完整性和安全性，优化数据访问</li>\n</ol>\n<h1 id=\"数据的概念模型\"><a href=\"#数据的概念模型\" class=\"headerlink\" title=\"数据的概念模型\"></a>数据的概念模型</h1><h2 id=\"Modele-Entites-Association\"><a href=\"#Modele-Entites-Association\" class=\"headerlink\" title=\"Modèle Entités-Association\"></a>Modèle Entités-Association</h2><ul>\n<li>组成：objet &lt;&#x3D;&gt; entité; lien &lt;&#x3D;&gt; association; propriété &lt;&#x3D;&gt; attribut<ol>\n<li>Entités: 其属性不能简单地随时间变化，避免无用的属性<br>Occurence d’une propriété &#x3D; couple ( propriété, sa valeur )<br>Occurence d’une entité &#x3D; couple ( entité, Occurence des propriétés )<br>Population &#x3D; { Occurences de l’entité }</li>\n<li>Association: entite之间的联系，无方向，只与entite相连<br>association之间不相连<br>entite之间不能直接相连</li>\n<li>Cardinalites: 可能的取值 0:1, 0:n, 1:1, 1:n. 表示entite中的一个occurence可以在association之中出现多少次。</li>\n<li>Identifiants: Entite中每个occurence独自拥有的性质，可由多列组成。entite都必须有一个Identifiant。association的id由所连接的entite的id组成，不用再特意表示出来。<blockquote>\n<p>为了避免 “一个人和一种东西只能在购买中出现一次” 的问题，可以将购买从association变成entite.<br><a href=\"https://postimg.cc/Wq31LJQY\"><img src=\"https://i.postimg.cc/tJdZ63Db/image.png\" alt=\"image.png\"></a></p>\n</blockquote>\n</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"数据的关系模型、物理模型\"><a href=\"#数据的关系模型、物理模型\" class=\"headerlink\" title=\"数据的关系模型、物理模型\"></a>数据的关系模型、物理模型</h2><h3 id=\"性质：\"><a href=\"#性质：\" class=\"headerlink\" title=\"性质：\"></a>性质：</h3><ol>\n<li>Atomicite 原子性: 交易完全会根本没有</li>\n<li>Coherence 一致性: BD从一个有效状态变为另一个有效状态</li>\n<li>Isolation 绝缘性:  交易就像单独进行一样</li>\n<li>Durabilite 可持续性: 一旦确认，记录交易结果</li>\n</ol>\n<h3 id=\"Modele-Relationnel\"><a href=\"#Modele-Relationnel\" class=\"headerlink\" title=\"Modele Relationnel\"></a>Modele Relationnel</h3><ol>\n<li>一个Attribut是一个信息；attribut的Domaine de Valeurs是其能取的值组成的有穷或无穷的集合。一个Relation是attribut的非空集合。<br><a href=\"https://postimg.cc/dkhxbs5m\"><img src=\"https://i.postimg.cc/C17pfdDt/image.png\" alt=\"image.png\"></a></li>\n<li>一个tuple的某attribut未知，则记作NULL</li>\n<li>用下划线标明identifiant的列名，可以是一列，也可以是多列。id的列不能有NULL的值。</li>\n<li>Lien externe是连接两个relations的有向箭头。指向目标relation的identifiant。</li>\n<li>完备性约束：tuple的值在值域中；标识符的唯一性；外部链接必须指向关键candidature,默认为标识符；<br><a href=\"https://postimg.cc/LgzNSnLf\"><img src=\"https://i.postimg.cc/fyPFxdbH/image.png\" alt=\"image.png\"></a></li>\n</ol>\n<h3 id=\"Modele-Physique\"><a href=\"#Modele-Physique\" class=\"headerlink\" title=\"Modele Physique\"></a>Modele Physique</h3><ul>\n<li>Modele entite-association 转化为 物理模型 的方法：<ol>\n<li>Entite: 所有的entite变成relation，它的propriete是relation的attributs， 他的identifiant也是relation的标识符。</li>\n<li>二元association的cardinalite最大值为1时：Carnalite最大值不等1的entite的标识符被复制到基数最大值为1的entite中(aPourStatut); max都为1,选更有代表性的标识符复制(estEleve)。在两个relation之间创造外部链接，指向标识符被复制的relation。</li>\n<li>其他的associations: 创建同名的relation，复制连接的entites的标识符为新relation的标识符，创建指向被复制标识符的relations的外部链接。</li>\n<li>加上数据类型。<br><a href=\"https://postimg.cc/nC2G18kH\"><img src=\"https://i.postimg.cc/x1DpnnNb/image.png\" alt=\"image.png\"></a></li>\n</ol>\n</li>\n</ul>\n<h1 id=\"SQL\"><a href=\"#SQL\" class=\"headerlink\" title=\"SQL\"></a>SQL</h1><ul>\n<li>常用词对照</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>关系模型</th>\n<th>SGBD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Relation</td>\n<td>Table</td>\n</tr>\n<tr>\n<td>Attribut</td>\n<td>Colone</td>\n</tr>\n<tr>\n<td>Identifiant</td>\n<td>Cle primaire</td>\n</tr>\n<tr>\n<td>Lien externe</td>\n<td>Cle etrangere</td>\n</tr>\n<tr>\n<td>Tuple</td>\n<td>Ligne</td>\n</tr>\n</tbody></table>\n<h2 id=\"SELECT\"><a href=\"#SELECT\" class=\"headerlink\" title=\"SELECT\"></a>SELECT</h2><ul>\n<li><code>SELECT liste_de_colonnes FROM table</code></li>\n<li><code>SELECT liste_de_colonnes, &#39;valeur_fixes&#39; FROM table</code>  在选出的列表中的colones的同时，加上有valeur_fixes组成的列</li>\n<li><code>SELECT DISTINCT liste_de_colonnes FROM table</code> 保证选出的行不重复</li>\n<li><code>SELECT liste_de_colonnes FROM table WHERE Conditions</code> 选择符合Conditions的行<ol>\n<li>可以比较一列和一个值，或者比较两列</li>\n<li>操作符：&#x3D;, &lt;&gt;, &lt;, &gt;, &lt;&#x3D;, &gt;&#x3D;, AND, OR, NOT</li>\n<li>Colone BETWEEN x AND y<br>  等价与：( Colonne &gt;&#x3D; x AND Colonne &lt;&#x3D; y )</li>\n<li>Colone IN (v1,…,vn)<br>  等价与：( Colonne&#x3D;v1 OR … ORColonne&#x3D;vn )<br>  Colone NOT IN (v1,…,vn)</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h3><p>用<strong>单引号</strong>表示，单引号之间的字符串的单引号，用双引号表示。</p>\n<ol>\n<li>% &#x3D; 所有字符串</li>\n</ol>\n<ul>\n<li><code>A%</code> &#x3D; 以A开始的字符串</li>\n<li><code>%N</code> &#x3D; 以N结束的字符串</li>\n<li><code>A%T%N</code> &#x3D;以A开始，有一个T且以N结尾的字符串</li>\n</ul>\n<ol start=\"2\">\n<li>_ &#x3D; 所有字符</li>\n</ol>\n<ul>\n<li><code>A_ION</code> &#x3D; A,然后一个任意字符，然够ION</li>\n</ul>\n<ol start=\"3\">\n<li><code>LIKE</code> : 比较两个字符串，区分大小写</li>\n<li><code>ILIKE</code> : 比较两个字符串，不区分大小写</li>\n</ol>\n<h3 id=\"日期，时间和时间戳\"><a href=\"#日期，时间和时间戳\" class=\"headerlink\" title=\"日期，时间和时间戳\"></a>日期，时间和时间戳</h3><ol>\n<li><p>DATE &#x3D; 日期，’AAAA-MM-DD‘</p>\n<p>TIME &#x3D; 时间，’HH:MM:SS’</p>\n<p>TIMESTAMP &#x3D; 日期+时间’AAAA-MM-DD HH:MM:SS’</p>\n</li>\n<li><p>经典比较操作（&gt;,&lt;,&#x3D;…）</p>\n<ul>\n<li><p>TO_DATE (unedata,format) : 将日期转换为指定格式</p>\n</li>\n<li><p>EXTRACT( unedonnée FROM unedate) : 从日期中选择想要的数据。CENTURY, DECADE; YEAR,MONTH,DAY(几号); DOW(周几); HOUR,MINUTE,SECOND; MICROSECONDS; TIMEZONE.</p>\n</li>\n<li><p>AddDate（）：增加一个日期（天、周等）</p>\n</li>\n<li><p>AddTime（）：增加一个是时间（时、分等）</p>\n</li>\n<li><p>Now（）：返回当前日期和时间</p>\n</li>\n<li><p>CurDate（）：返回当前日期</p>\n</li>\n<li><p>CurTime（）：返回当前时间</p>\n</li>\n<li><p>Date（）：返回日期时间的日期部分</p>\n</li>\n<li><p>DateDiff（）：计算两个日期之差</p>\n</li>\n<li><p>Date_Add（）：高度灵活的日期运算函数</p>\n</li>\n<li><p>Date_Format（）：返回一个格式化的日期或时间串</p>\n</li>\n<li><p>DOW（）：对于一个日期，返回对应的星期几</p>\n</li>\n<li><p>Time（）：返回一个日期时间的时间部分</p>\n</li>\n<li><p>Year（）：返回一个日期的年份部分</p>\n</li>\n<li><p>Month（）：返回一个日期的月份部分</p>\n</li>\n<li><p>Day（）：返回一个日期的天数部分</p>\n</li>\n<li><p>Hour（）：返回一个时间的小时部分</p>\n</li>\n<li><p>Minute（）：返回一个时间的分钟部分</p>\n</li>\n<li><p>Second（）：返回一个时间的秒数部分</p>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"NULL\"><a href=\"#NULL\" class=\"headerlink\" title=\"NULL\"></a>NULL</h3><p>表示缺少信息</p>\n<ul>\n<li>Colone IS NULL</li>\n<li>Colone IS NOT NULL</li>\n</ul>\n<h2 id=\"Jointures\"><a href=\"#Jointures\" class=\"headerlink\" title=\"Jointures\"></a>Jointures</h2><p>对两个表的行进行笛卡尔乘积</p>\n<ol>\n<li>SELECT liste_colonnes FROM liste_tables WHERE liste_contraintes<blockquote>\n<p>SELECT * FROM Eleve, Membre WHERE Eleve_ID&#x3D;Membre_ID<br>SELECT * FROM Eleve INNER JOIN Membre ON (Eleve_ID&#x3D;Membre_ID)表示将Eleve和Membre两个table按照各自ID的对应关系join起来</p>\n</blockquote>\n</li>\n<li><code>CROSS JOIN</code> 笛卡尔乘积<ul>\n<li>[INNER] JOIN table ON (condition_jointure) : 按照ON的连接条件，join一个新的table.</li>\n<li>table1 INNER JOIN table2 USING (une_colonne)</li>\n<li>NATURAL JOIN : 当两表有相同列名时，按照同名列的相等join，否则进行笛卡尔乘积。</li>\n<li>FULL | LEFT | RIGHT [OUTER] JOIN table ON (condition_jointure) : 全连接，左连接，右连接</li>\n<li>FULL | LEFT | RIGHT NATURAL JOIN table</li>\n</ul>\n</li>\n<li>区分两表的相同列名时，可在列名前加上前缀。<code>Eleve.Eleve_ID</code>, <code>Membre.Eleve_ID</code></li>\n<li>在一个requete中多次使用一个table时，对表格进行重命名。<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> Club_Nom,Eleve_Resp.Eleve_Nom,Eleve_Tres.Eleve_Nom</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Club</span><br><span class=\"line\"><span class=\"keyword\">INNER</span> <span class=\"keyword\">JOIN</span> Eleve Eleve_Resp <span class=\"keyword\">ON</span> (Club_Resp<span class=\"operator\">=</span>Eleve_Resp.Eleve_ID)</span><br><span class=\"line\"><span class=\"keyword\">INNER</span> <span class=\"keyword\">JOIN</span> Eleve Eleve_Tres <span class=\"keyword\">ON</span> (Club_Tres<span class=\"operator\">=</span>Eleve_Tres.Eleve_ID)</span><br></pre></td></tr></table></figure></li>\n</ol>\n<h2 id=\"集合操作\"><a href=\"#集合操作\" class=\"headerlink\" title=\"集合操作\"></a>集合操作</h2><ol>\n<li>UNION : concat 两个表。</li>\n<li>EXCEPT : 删除第一个表中和第二个表相同的行。A&#x2F;B</li>\n<li>INTERSECT : 取两表中相同的行。</li>\n</ol>\n<h2 id=\"嵌套查询\"><a href=\"#嵌套查询\" class=\"headerlink\" title=\"嵌套查询\"></a>嵌套查询</h2><ol>\n<li>一个requete的结果可以在另一个requete中使用，只要使用<code>IN</code>.<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> Club_Nom, Eleve.Eleve_ID</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Club <span class=\"keyword\">INNER</span> <span class=\"keyword\">JOIN</span> Eleve <span class=\"keyword\">ON</span> (Club_Resp<span class=\"operator\">=</span>Eleve.Eleve_ID)</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> Eleve.Personne_ID <span class=\"keyword\">IN</span></span><br><span class=\"line\">(<span class=\"keyword\">SELECT</span> Personne_ID <span class=\"keyword\">FROM</span> Personne <span class=\"keyword\">WHERE</span> Personne_Nom <span class=\"keyword\">LIKE</span> ’A<span class=\"operator\">%</span>’)</span><br></pre></td></tr></table></figure></li>\n<li><code>ANY</code>: 比较attribut的一个值和一个requete的所有结果。一次比较为真，则整个条件都为真。<code>ALL</code>: 所有比较都为真时，才为真。<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> Fourniture_Nom</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Fourniture</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> Fourniture_Prix <span class=\"operator\">&lt;</span> <span class=\"keyword\">ANY</span></span><br><span class=\"line\"><span class=\"comment\">-- WHERE Fourniture_Prix &lt; ALL</span></span><br><span class=\"line\"><span class=\"comment\">-- WHERE EXISTS</span></span><br><span class=\"line\">(<span class=\"keyword\">SELECT</span> Fourniture_Prix <span class=\"keyword\">FROM</span> Fourniture <span class=\"keyword\">WHERE</span> Fourniture_Type <span class=\"operator\">=</span> ’BUREAU’)</span><br></pre></td></tr></table></figure></li>\n<li><code>EXISTS</code>:当嵌套的requete有至少一个结果时 为真。</li>\n</ol>\n<h2 id=\"GROUP-BY\"><a href=\"#GROUP-BY\" class=\"headerlink\" title=\"GROUP BY\"></a>GROUP BY</h2><p>根据一列或多列的值，对表的行进行聚合。方便对行的子集合进行操作。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> informations <span class=\"keyword\">FROM</span> liste_de_tables</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> conditions</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> critère</span><br></pre></td></tr></table></figure>\n<p>对于行的子集合进行的操作：AVG(均值)、COUNT、MAX、MIN、SUM</p>\n<h2 id=\"ORDER-BY\"><a href=\"#ORDER-BY\" class=\"headerlink\" title=\"ORDER BY\"></a>ORDER BY</h2><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> informations <span class=\"keyword\">FROM</span> liste_de_tables</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> conditions</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> list_colones <span class=\"keyword\">ASC</span> <span class=\"comment\">--DESC</span></span><br></pre></td></tr></table></figure>\n<p>排序的参照critere可以是列名的列表，第一列相同的行在比较第二列……</p>\n<ul>\n<li>排序的规则：ASC, DESC, 默认升序asc排序。</li>\n<li>几个函数：<ul>\n<li><code>UPPER</code> : 将一个attribut或者valeur变成大写</li>\n<li><code>LOWER</code> : 将一个attribut或者valeur变成小写</li>\n<li><code>CAPS</code> : 将一个attribut或者valeur每个词的首字母变成大写</li>\n<li><code>CONCAT</code> - &#96;||&#96;&#96; : concatenation</li>\n<li><code>SUBSTRING</code> - <code>SUBSTR</code> : 提取子串</li>\n<li><code>TRIM</code> : 删除字符串开头和结尾的空格。LTRIM，RTRIM</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"类型转换CAST\"><a href=\"#类型转换CAST\" class=\"headerlink\" title=\"类型转换CAST\"></a>类型转换CAST</h2><p>将表达式转换成指定类型，注意这里的类型必须兼容。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> Nom, <span class=\"built_in\">CAST</span> ( DateNaissance <span class=\"keyword\">AS</span> <span class=\"type\">VARCHAR</span> ) <span class=\"keyword\">FROM</span> Personne</span><br></pre></td></tr></table></figure>\n<h2 id=\"INSERT\"><a href=\"#INSERT\" class=\"headerlink\" title=\"INSERT\"></a>INSERT</h2><p>添加，每次只做用在一个表上。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> tab(attrib1, attrib2, ... attribn) <span class=\"keyword\">VALUES</span></span><br><span class=\"line\">(val1, val2, ... valn)</span><br><span class=\"line\"><span class=\"comment\">--2</span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> tab <span class=\"keyword\">VALUES</span> (val1, val2, ... valn)</span><br><span class=\"line\"><span class=\"comment\">--3</span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> tab(attrib1, attrib2, ... attribn) <span class=\"keyword\">VALUES</span></span><br><span class=\"line\">(val11, val12, ... val1n) ,</span><br><span class=\"line\">...</span><br><span class=\"line\">(valm1, valm2, ... valmn)</span><br><span class=\"line\"><span class=\"comment\">--4</span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> tab(liste_attributs) <span class=\"keyword\">SELECT</span> liste_attributs <span class=\"keyword\">FROM</span> ..</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> critiers</span><br><span class=\"line\"><span class=\"comment\">--5</span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> tab(col1, col2, ... coln) <span class=\"keyword\">VALUES</span> (val1, val2, ... valn)</span><br><span class=\"line\">RETURNING uneColonne</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Requete 中的attribut和valeur对应，未提到的attribut赋为默认值，无默认值则为NULL。当未遵守完整性约束，插入产生错误，则所有插入都不会进行。</li>\n<li>方式二中，对table的所有attribut逐个赋值。</li>\n<li>将val1,…valm 逐个插入，当一个出现错误，所有插入都不执行</li>\n<li>select选出的所有行都被插入表中，select选出的列要匹配插入的格式。</li>\n<li>可以使用RETURNING返回插入创造的一列的值。</li>\n</ol>\n<h2 id=\"UPDATE\"><a href=\"#UPDATE\" class=\"headerlink\" title=\"UPDATE\"></a>UPDATE</h2><p>更新，每次只做用在一个表上。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">UPDATE</span> tab <span class=\"keyword\">SET</span> ListeAffectation [<span class=\"keyword\">WHERE</span> ListeConditions]</span><br><span class=\"line\"><span class=\"comment\">--例子：Alexis DELILLE 不在club JdR，而在club Manga.</span></span><br><span class=\"line\"><span class=\"keyword\">UPDATE</span> Membre <span class=\"keyword\">SET</span> Club_Nom<span class=\"operator\">=</span>’Manga’</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> Personne_ID <span class=\"keyword\">IN</span> (<span class=\"keyword\">SELECT</span> Personne_ID <span class=\"keyword\">FROM</span> Personne</span><br><span class=\"line\">                        <span class=\"keyword\">WHERE</span> Nom<span class=\"operator\">=</span>’DELILLE’ <span class=\"keyword\">AND</span> Prenom<span class=\"operator\">=</span>’Alexis’)</span><br><span class=\"line\"><span class=\"keyword\">AND</span> Club_Nom <span class=\"operator\">=</span> ’JdR’</span><br></pre></td></tr></table></figure>\n<ol>\n<li>每个符合条件的行都按照listeaffection进行更新。违反一个完整性约束，所有更新都不执行。</li>\n</ol>\n<h2 id=\"DELETE\"><a href=\"#DELETE\" class=\"headerlink\" title=\"DELETE\"></a>DELETE</h2><p>删除，每次只做用在一个表上。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DELETE</span> <span class=\"keyword\">FROM</span> tab [<span class=\"keyword\">WHERE</span> ListeConditions]</span><br><span class=\"line\"><span class=\"comment\">--例子：Alexis DELILLE 不是club JdR 成员。</span></span><br><span class=\"line\"><span class=\"keyword\">DELETE</span> <span class=\"keyword\">FROM</span> Membre</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> Personne_ID <span class=\"keyword\">IN</span> (<span class=\"keyword\">SELECT</span> Personne_ID <span class=\"keyword\">FROM</span> PERSONNE</span><br><span class=\"line\">                      <span class=\"keyword\">WHERE</span> Nom<span class=\"operator\">=</span>’DELILLE’ <span class=\"keyword\">AND</span> Prenom<span class=\"operator\">=</span>’Alexis)</span><br><span class=\"line\"><span class=\"keyword\">AND</span> Club_Nom <span class=\"operator\">=</span> ’JdR’</span><br></pre></td></tr></table></figure>\n<ol>\n<li>删除每个符合条件的行。违反完整性约束是，所有删除都不会执行。</li>\n</ol>\n<h2 id=\"Transcation\"><a href=\"#Transcation\" class=\"headerlink\" title=\"Transcation\"></a>Transcation</h2><p>当多人同时操作一个数据库，由于requete执行顺序，可能会引起错误。为确保一个人的代码快执行完后在进行另一个代码块，使用transcation</p>\n<table>\n<thead>\n<tr>\n<th>BEGIN；…; COMMIT；</th>\n<th>BEGIN；…; ROLLBACK；</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>执行requete并保存</td>\n<td>执行requete并取消</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">BEGIN</span>;</span><br><span class=\"line\"><span class=\"keyword\">UPDATE</span> Personne <span class=\"keyword\">SET</span> Nom<span class=\"operator\">=</span>‘ALBAN’ <span class=\"keyword\">WHERE</span> Prenom<span class=\"operator\">=</span>‘Alexis’;</span><br><span class=\"line\"><span class=\"keyword\">UPDATE</span> Personne <span class=\"keyword\">SET</span> Nom<span class=\"operator\">=</span>‘DELILLE’ <span class=\"keyword\">WHERE</span> Prenom<span class=\"operator\">=</span>‘Roger’;</span><br><span class=\"line\"><span class=\"keyword\">COMMIT</span>;</span><br></pre></td></tr></table></figure>\n<h2 id=\"CREATE\"><a href=\"#CREATE\" class=\"headerlink\" title=\"CREATE\"></a>CREATE</h2><p><code>CREATE DATABASE</code> : 创建数据库，定义处理器，编译方式<br><code>CREATE TABLE</code> : 创建表，定义表的schema：列、cle primaire，etrangere等</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> nom_table (</span><br><span class=\"line\">nom_colonne type_colonne [Contraintes],</span><br><span class=\"line\">...</span><br><span class=\"line\">contraintes sur la <span class=\"keyword\">table</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>不同SGBD的数据类型表示</th>\n<th>PostgreSQL</th>\n<th>MySQL</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>整数</td>\n<td>INTEGER</td>\n<td>int</td>\n</tr>\n<tr>\n<td>实数</td>\n<td>NUMERIC</td>\n<td>float</td>\n</tr>\n<tr>\n<td>字符</td>\n<td>CHARACTER</td>\n<td>char</td>\n</tr>\n<tr>\n<td>字符串</td>\n<td>CHARACTER VARYING</td>\n<td>varchar</td>\n</tr>\n<tr>\n<td>文本</td>\n<td>TEXT</td>\n<td>text</td>\n</tr>\n<tr>\n<td>日期</td>\n<td>DATA</td>\n<td>data</td>\n</tr>\n<tr>\n<td>时间</td>\n<td>TIME</td>\n<td>time</td>\n</tr>\n<tr>\n<td>时间戳</td>\n<td>TIMESTAMP</td>\n<td>timestamp</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>列的限制条件有：</p>\n<ol>\n<li><code>NOT NULL</code></li>\n</ol>\n<p>  该列中不能有NULL。当没有此限制。默认为NULL。<br>  当我们尝试在一个声明为NOT NULL的列添加NULL时会出错。</p>\n<ol start=\"2\">\n<li><p><code>PRIMARY KEY</code></p>\n<p><code>PRIMARY KEY (liste_de_colonnes)</code><br>有这个条件的列是主键（identifiant）。当主键有多列组成时，必须在表的最后声明（使用liste de colones) 。该条件在一个表中必须且只能出现一次。该条件与NOT NULL同时使用。</p>\n</li>\n<li><p><code>UNIQUE</code></p>\n<p>UNIQUE (liste_de_colonnes) : 允许同时声明多列候选主键。</p>\n</li>\n</ol>\n<p>  有这个条件的列，不能有重复的值，该列是一个候选键，但不是标识符。当唯一键由多列组成时，必须在表的最后声明。此条件与NOT NULL同时使用。一个表中可以有许多个唯一键，也可以没有。</p>\n<ol start=\"4\">\n<li><p><code>REFERENCES</code> nom-table [(nom-col)] [action]<br><code>FOREIGN KEY</code> (liste_de_colonnes) <code>REFERENCES</code> nom-table[(Autre_liste_de_colonnes)] [action] : 用于声明外部链接</p>\n<p>默认指向另一个表的主键，不是主键的话，也是唯一键。某列有盖条件时，那么该列的值取决于他们在参考列中的存在。如果参考多列免责应该在表的最后声明。参考的表必须已经存在。</p>\n</li>\n<li><p><code>CHECK (condition)</code></p>\n<p>用于在插入或修改的情况下检查一列或一组列是否遵守预定义的约束。如果某列定义了该限制，则该条件只作用在该列。当在表的末尾定义该限制，该条件可以左右在任意一列或者整个表上。</p>\n</li>\n<li><p><code>DEFAULT</code></p>\n<p>用于设定默认值。当一列在insertion时未被提及则取默认值。</p>\n</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> Eleve (</span><br><span class=\"line\">    id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">    nom <span class=\"type\">CHARACTER</span> <span class=\"type\">VARYING</span>(<span class=\"number\">120</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> inscription(</span><br><span class=\"line\">    id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> REFERENCE Eleve,</span><br><span class=\"line\">    Annee <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">    <span class=\"keyword\">PRIMARY</span> KEY (id,Annee)</span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> InscriptionMatiere(</span><br><span class=\"line\">    id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">REFERENCES</span> Eleve,</span><br><span class=\"line\">    Annee <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">    Matiere <span class=\"type\">CHARACTER</span> <span class=\"type\">VARYING</span>(<span class=\"number\">24</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">    <span class=\"keyword\">PRIMARY</span> KEY (id, Annee, Matiere)</span><br><span class=\"line\">    <span class=\"keyword\">FOREIGN</span> KEY (id, Annee) <span class=\"keyword\">REFERENCES</span> Inscription</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>CREATE VIEW</code> : 创建由select选出的数据的特殊vue</li>\n</ul>\n<h2 id=\"DROP\"><a href=\"#DROP\" class=\"headerlink\" title=\"DROP\"></a>DROP</h2><ul>\n<li><code>DROP DATABASE</code> : 删除数据库，schema、数据等</li>\n<li><code>DROP TABLE</code> : 删除表，schema、数据等<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DROP</span> <span class=\"keyword\">TABLE</span> tab <span class=\"comment\">-- 外部连接仍然有效</span></span><br></pre></td></tr></table></figure></li>\n<li>DROP VIEW : 删除vue</li>\n</ul>\n<h2 id=\"ALTER\"><a href=\"#ALTER\" class=\"headerlink\" title=\"ALTER\"></a>ALTER</h2><ul>\n<li><code>ALTER DATABASE</code>：修改数据库，处理器、名字等</li>\n<li><code>ALTER  TABLE</code>：修改表，改变处理器、名字、列等</li>\n<li><code>ALTER TABLE tab</code><ol>\n<li><code>RENAME TO</code> nouveau-nom-table<br>更改表的名字</li>\n<li><code>ADD COLUMN</code> nom-col type-col [contraintes]<br>添加列</li>\n<li><code>MODIFY COLUMN</code> nom-col type-col [contraints]<br>更改列的名字、类型</li>\n<li><code>DROP COLUMN</code> nom-col [CASCADE CONSTRAINTS]<br>删除某列</li>\n<li><code>RENAME COLUMN</code> old-name TO new-name  将某列的名字改为</li>\n</ol>\n</li>\n<li><code>ALTER  VIEW</code>：修改vue</li>\n</ul>\n<h2 id=\"完整性参照\"><a href=\"#完整性参照\" class=\"headerlink\" title=\"完整性参照\"></a>完整性参照</h2><ul>\n<li>添加一行数据时：<ol>\n<li>主键不能已经存在</li>\n<li>唯一键的唯一性</li>\n<li>有外部链接的列的值要在对应列中存在</li>\n<li>（CHECK）的条件要满足</li>\n</ol>\n</li>\n<li>更改一行数据时：<ol>\n<li>主键与唯一键的唯一性</li>\n<li>外部链接的列的值在参照列中存在。外键的目标列在被更改时，不能打断参考</li>\n<li>check的限制要满足</li>\n</ol>\n</li>\n<li>删除一列时：<ol>\n<li>外键的目标列在被删除时，不能打断参考</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"触发\"><a href=\"#触发\" class=\"headerlink\" title=\"触发\"></a>触发</h2><table>\n<thead>\n<tr>\n<th>2种情况</th>\n<th>4个动作</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ON UPDATE; ON DELETE</td>\n<td>RESTRICT; CASCADE; SET DEFAULT; SET NULL</td>\n</tr>\n</tbody></table>\n<p>当外键的目标收到莫命令时，也将该命令作用在外键的来源。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> Eleve (</span><br><span class=\"line\">    Eleve_ID <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">PRIMARY</span> KEY</span><br><span class=\"line\">    Personne_ID <span class=\"type\">INTEGER</span> <span class=\"keyword\">REFERENCES</span> Personne</span><br><span class=\"line\">                        <span class=\"keyword\">ON</span> <span class=\"keyword\">UPDATE</span> CASCADE</span><br><span class=\"line\">                        <span class=\"keyword\">ON</span> <span class=\"keyword\">DELETE</span> <span class=\"keyword\">SET</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>Personne</th>\n<th>Personne</th>\n<th>Personne</th>\n<th>Eleves</th>\n<th>Eleves</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>Nom</td>\n<td>Prenom</td>\n<td>Num</td>\n<td>ID</td>\n</tr>\n<tr>\n<td>1</td>\n<td>SHEN</td>\n<td>Anran</td>\n<td>1824</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>NI</td>\n<td>LI</td>\n<td>1924</td>\n<td>2</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>因为不会Markdowm合并单元格，所以画了这个chun表格!!!</p>\n</blockquote>\n<ul>\n<li>当我们想将personne中anran的id变为4，或者删除anran时，会破环eleve中1824向personne的1的外键。<ol>\n<li>当有ON UPDATE CASCADE时，eleves的ID&#x3D;1也会被ID&#x3D;4替换。</li>\n<li>当有ON DELETE SET NULL，eleve的ID&#x3D;1会变成NULL</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"创建使用者\"><a href=\"#创建使用者\" class=\"headerlink\" title=\"创建使用者\"></a>创建使用者</h2><ul>\n<li>CREATE ROLE un_utilisateur … &#x2F; DROP ROLE un_utilisateur</li>\n<li>CREATE USER un_utilisateur … &#x2F; DROP USER un_utilisateur</li>\n<li>使用者权限<ul>\n<li>LOGIN &#x2F; NOLOGIN</li>\n<li>SUPERUSER &#x2F; NOSUPERUSER</li>\n<li>CREATEDB &#x2F; NOCREATEDB</li>\n<li>VALID UNTIL ’date et heure</li>\n<li>PASSWORD ‘mot de passe’<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">USER</span> prweb <span class=\"keyword\">WITH</span> LOGIN NOSUPERUSER CREATEDB PASSWORD</span><br><span class=\"line\">’info2019prweb’;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li>改变数据库创建者</li>\n<li>ALTER DATABASE ma_base OWNER TO un_utilisateur</li>\n<li>ALTER TABLE ma_table OWNER TO un_utilisateur</li>\n<li>分配权限<ul>\n<li>GRANT operation ON objet TO utilisateur [WITH GRANT OPTION]</li>\n<li>权限有：SELECT，INSERT，UPDATE，DELETE，TRUNCATE (vider une table)<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> DATABASE personnel OWNER <span class=\"keyword\">TO</span> persouser;</span><br><span class=\"line\"><span class=\"keyword\">GRANT</span> <span class=\"keyword\">SELECT</span> <span class=\"keyword\">ON</span> <span class=\"keyword\">ALL</span> TABLES <span class=\"keyword\">TO</span> autreperso;</span><br><span class=\"line\"><span class=\"keyword\">GRANT</span> <span class=\"keyword\">ALL</span> PRIVILEGES <span class=\"keyword\">ON</span> <span class=\"keyword\">TABLE</span> personne <span class=\"keyword\">TO</span> autreperso;</span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> personne OWNER <span class=\"keyword\">TO</span> altuser;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"导入csv文件\"><a href=\"#导入csv文件\" class=\"headerlink\" title=\"导入csv文件\"></a>导入csv文件</h2><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">COPY</span> tab(liste_de_colonne)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> ‘chemin_acces_fichier’</span><br><span class=\"line\">DELIMITER ‘délimiteur’ CSV;</span><br></pre></td></tr></table></figure>\n<p>先创建表tab，在拥有权限的情况下，按照copy后面的格式从csv文件路径读入数据，delimiter定义分隔符。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> public.test(</span><br><span class=\"line\">    id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">    name <span class=\"type\">CHARACTER</span> <span class=\"type\">VARYING</span>(<span class=\"number\">20</span>) <span class=\"keyword\">COLLATE</span> pg_catalog.&quot;default&quot;,</span><br><span class=\"line\">    val <span class=\"type\">CHARACTER</span> <span class=\"type\">VARYING</span>(<span class=\"number\">20</span>)<span class=\"keyword\">COLLATE</span> pg_catalog.&quot;default&quot;,</span><br><span class=\"line\">    <span class=\"keyword\">CONSTRAINT</span> test_pkey <span class=\"keyword\">PRIMARY</span> KEY (id)</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">COPY</span> test(id, name, <span class=\"keyword\">value</span>)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> ’chemin_absolu’ DELIMITER ’;’ CSV;</span><br></pre></td></tr></table></figure>\n"},{"title":"UML","url":"/2023/02/19/QCM/UML/","content":"<h1 id=\"不同视图\"><a href=\"#不同视图\" class=\"headerlink\" title=\"不同视图\"></a>不同视图</h1><ul>\n<li>用例视图(vue des cas d’utilisation)：这是系统参与者“看到”的系统描述。 它对应于每个参与者的预期需求（即 WHAT 和 WHO）</li>\n<li>逻辑视图(vue logique)：这是从内部看到的系统定义。 它解释了如何满足参与者的需求（这就是 HOW）</li>\n<li>实现视图(vue d’implementation)：该视图定义了模块之间的依赖关系。</li>\n<li>过程视图(vue des processus)：这是时间和技术视图，它实现了并发任务、刺激、控制、同步等概念。</li>\n<li>部署视图(vue de deploiement)：这个视图描述了系统每个元素的地理位置和物理架构（这是WHERE）</li>\n</ul>\n<h1 id=\"不同图的分类\"><a href=\"#不同图的分类\" class=\"headerlink\" title=\"不同图的分类\"></a>不同图的分类</h1><ul>\n<li>结构图&#x2F;静态图<ol>\n<li>部署图(Deployment diagram)：用于表示硬件元素(计算机、外设、网络、存储系统等)以及系统组件在这些硬件元素上的分布方式和交互方式</li>\n<li>包图(Package Diagram)：表示系统的包</li>\n<li>复合结构图(Composite Structure Diagram)：表示了class的内部结构以及其与系统其他部分的交互点</li>\n</ol>\n</li>\n<li>行为图&#x2F;动态图<ol>\n<li>用例图(Use case diagram)：描述系统与参与者之间交互的可能性，即系统必须提供的所有功能。</li>\n<li>状态转换图(Diagramme États-Transitions)：显示了系统的状态如何根据系统时间进行修改</li>\n<li>活动图(Activity Diagram)：状态转换图的变体，允许根据系统状态表示事件的触发，也可以模拟可并行化的行为（多线程或多进程）</li>\n</ol>\n</li>\n<li>交互图<ol>\n<li>序列图(Sequence Diagram)：系统元素和&#x2F;或参与者之间处理和交互过程的顺序表示</li>\n<li>通信图(Interaction Overview Diagram)：序列图的简化表示，重点关注对象之间的消息交换</li>\n<li>时序图(Timing Diagram)：突出显示时间方面的交互表示<br><a href=\"https://postimg.cc/0MhRyWjS\"><img src=\"https://i.postimg.cc/gj2Y41Pg/image.png\" alt=\"image.png\"></a></li>\n</ol>\n</li>\n</ul>\n<h1 id=\"用例图\"><a href=\"#用例图\" class=\"headerlink\" title=\"用例图\"></a>用例图</h1><p>定义：表示用例、参与者以及它们之间的关系。从用户的角度用动作和反应的形式描述系统的行为。帮助定义系统边界以及系统与环境之间的关系</p>\n<ul>\n<li>用户：存在于系统之外，所有可以与系统交换信息的人、机器、其他系统等。子用户、父用户。<ul>\n<li>主要用户：主要使用系统的人</li>\n<li>次要用户：管理或维护的人员</li>\n<li>外部硬件：属于范围的一部分且必须使用的硬件设备</li>\n<li>其他系统：系统必须与之交互的系统</li>\n</ul>\n</li>\n<li>用例：描述一组动作序列，系统执行这些动作以提供对参与者有价值的结果。<ul>\n<li>提取常见行为：子用户的行为一定也是父用户的行为</li>\n<li>包括：一个行为包括在另一个用例中，分解复杂的行为。《include》</li>\n<li>扩展：在某些点添加复杂的备用路径,可以添加条件。《extende》<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Tant que condition faire:    Si condition:</span><br><span class=\"line\">&lt;&lt;action&gt;&gt;                   alors &lt;&lt;action&gt;&gt;</span><br><span class=\"line\">&lt;&lt;action&gt;&gt;                   sinon &lt;&lt;action&gt;&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">Repeter n fois:           Choix:</span><br><span class=\"line\">&lt;&lt;action&gt;&gt;                condition:&lt;&lt;action&gt;&gt;condition:</span><br><span class=\"line\">&lt;&lt;action&gt;&gt;                &lt;&lt;action&gt;&gt;,&lt;&lt;action&gt;&gt;</span><br></pre></td></tr></table></figure>\n<a href=\"https://postimg.cc/dLKpNTyJ\"><img src=\"https://i.postimg.cc/vH1yBr8x/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"封装图\"><a href=\"#封装图\" class=\"headerlink\" title=\"封装图\"></a>封装图</h1><p>定义：显示包的组织及其元素，表示类包的命名空间、逻辑或物理截断。</p>\n<ul>\n<li>可以具有嵌套包的层次结构<br><a href=\"https://postimg.cc/RWpPm3N6\"><img src=\"https://i.postimg.cc/JhMSbZ8q/image.png\" alt=\"image.png\"></a></li>\n<li>包的遏制：有两种等效方式</li>\n<li>依赖：<ol>\n<li>&lt;<merge>&gt;：一个包导入其他包的功能</li>\n<li>&lt;<import>&gt;：一个包需要其他包的功能帮助<br><a href=\"https://postimg.cc/HjbPMsPR\"><img src=\"https://i.postimg.cc/h4YFHX8D/image.png\" alt=\"image.png\"></a></li>\n</ol>\n</li>\n</ul>\n<h1 id=\"类图\"><a href=\"#类图\" class=\"headerlink\" title=\"类图\"></a>类图</h1><p>定义：描述系统的通用模型，类使用包含类名、属性(field) 和方法(method) 且带有分割线的矩形来表示。</p>\n<ul>\n<li>属性：对应类的属性，属性名、类型、初始值等</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Employee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-name : String <br> - age : Int<br>- email : String</td>\n</tr>\n<tr>\n<td>+ modifyInfo() : void<br># Method() : String</td>\n</tr>\n</tbody></table>\n<ul>\n<li>操作：指定对象的行为，类的创造器、类的摧毁器、返回类的全部或部分状态的选择器、改变类的全部或部分状态的修改器，返回类的全部或部分状态的迭代器<ol>\n<li>操作参数的方向:</li>\n<li>In：只传入参数，执行中不修改参数</li>\n<li>Out：只传出参数，调用该操作可以获得信息</li>\n<li>InOut：输入输出参数</li>\n<li>适用于操作的预定义属性：</li>\n</ol>\n<ul>\n<li>{requete}: 不改变相关实例状态的操作</li>\n<li>{concurrence&#x3D;valeur}: 其中值是顺序的</li>\n<li>{abstrait}: 操作未在类中实现</li>\n<li>{estFeuille}: 不能在类中重新定义的操作</li>\n<li>{estRacine}: 在类层次中首次定义的操作<br>  <a href=\"https://postimg.cc/N5m9YZF1\"><img src=\"https://i.postimg.cc/CL6jCgV6/image.png\" alt=\"image.png\"></a></li>\n</ul>\n</li>\n<li>描述：默认对象的属性值被封装在对象内部，不能直接被其他对象直接操作。可见性规则有：私有(-)、受保护(#)、公共(+)   可见性 名称：类型 [&#x3D; 缺省值]</li>\n<li>Stereotype：UML的主要扩展机制。可以创造新的类型，书名号表示<ol>\n<li><code>&lt;&lt;Displayable Object&gt;&gt;</code>: 创建可显示对象</li>\n<li><code>&lt;&lt;class implementation&gt;&gt;</code>: 一个类在编程语言中实现</li>\n<li><code>&lt;&lt;enumeration&gt;&gt;</code>: 定义一个类型的值域的标识符集合的类</li>\n<li><code>&lt;&lt;metaclass&gt;&gt;</code>: 类的类</li>\n<li><code>&lt;&lt;powertype&gt;&gt;</code>:一个类是元类型,即他的实例都是一个类型的子类性</li>\n</ol>\n</li>\n<li>关键词：出现在元模型中，不是UML语言的扩展<ol>\n<li><code>&lt;&lt;actor&gt;&gt;</code>: 该类模拟了一组系统中用户的角色，</li>\n<li><code>&lt;&lt;interface&gt;&gt;</code>:只包含可见操作描述的类</li>\n<li><code>&lt;&lt;signal&gt;&gt;或&lt;&lt;exception&gt;&gt;</code>: 模拟信号类型的元素</li>\n</ol>\n</li>\n<li>参数化类：类模型，不能直接使用，必须实例化形式参数已给出参数化类  </li>\n<li>实用类：在不构建完整类的情况下对模块的元素进行分组，使用<code>&lt;&lt;Utility&gt;&gt;stereotype</code></li>\n</ul>\n<h2 id=\"类之间的关联关系\"><a href=\"#类之间的关联关系\" class=\"headerlink\" title=\"类之间的关联关系\"></a>类之间的关联关系</h2><p>两个类之间的关联：由名称、相关的两个类和multiplicite定义，关联后面跟方向箭头(实心箭头)表方向</p>\n<ul>\n<li>Multiplicite: 1 只有一, 0..1 零或一, M..N 从M到N, * ou 0..* 从零到多个, 1..* 从一到多个<br><a href=\"https://postimg.cc/p5NW6XD3\"><img src=\"https://i.postimg.cc/J00stst7/image.png\" alt=\"image.png\"></a></li>\n<li>关联关系<ol>\n<li>单项关联：单向箭头表示</li>\n<li>双向关联：默认情况下是双向关联。用直线连接</li>\n<li>自关联：在系统中可能会存在一些类的属性对象类型为该类本身，如两人结婚等</li>\n</ol>\n</li>\n<li>聚合关系：非对称关联，用空心菱形端表示 ’整体‘，强调 ‘整体’ 包含 ‘部分’，但部分可脱离整体单独存在。</li>\n<li>组合关系：这里的“部分”脱离了“整体”便不复存在，暗示对聚合侧的多重性值约束。组合和属性在语义上是等价的。<br><a href=\"https://postimg.cc/3dTrjNrv\"><img src=\"https://i.postimg.cc/SNXXhYwD/image.png\" alt=\"image.png\"></a></li>\n<li>依赖关系：是一种使用关系，常体现在某个类的方法使用另一个类的对象作为参数。虚线表示，由依赖的一方指向被依赖的一方。<br><a href=\"https://postimg.cc/HJRdrsPj\"><img src=\"https://i.postimg.cc/sXDX0MYP/image.png\" alt=\"image.png\"></a></li>\n<li>继承关系&#x2F;泛化关系：描述父类与子类之间的关系。用带空心三角形的直线来表示。<br>泛化关系可以添加约束：<ol>\n<li>{disjoint} ou {Exclusif}: 子类只能是super-class</li>\n<li>{chevauchement} ou {Inclusif} : 子类可以是多个super-class的子类</li>\n<li>{complète} ： 泛化结束，不能在添加新的子类</li>\n<li>{Incomplète}：还可以添加新的子类</li>\n</ol>\n</li>\n<li>接口&#x2F;抽象类：<br>接口和类之间存在一种实现(realization)关系，用代空心三角的虚线箭头表示<ol>\n<li>抽象类：它的接口是它所有非私有属性的签名，使用斜体名称或{abstract}表示</li>\n<li>接口：使用类型来描述类的可见行为，无需实现。可以提供一组服务的局部或全局视图。可以用圆圈表示接口。</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"关联的约束\"><a href=\"#关联的约束\" class=\"headerlink\" title=\"关联的约束\"></a>关联的约束</h2><ol>\n<li>{ordered} : ⼀个顺序约束描述了放置在集合中的对象，顺序必须保持</li>\n<li>{sous-ensemble} : 一个集合包含在另一个集合中</li>\n<li>{ou-exclusif} : 对于给定的对象，只有一个有效的关联<br>  <a href=\"https://postimg.cc/xJGfBVtC\"><img src=\"https://i.postimg.cc/MTNnMWBy/image.png\" alt=\"image.png\"></a></li>\n</ol>\n<h1 id=\"对象图\"><a href=\"#对象图\" class=\"headerlink\" title=\"对象图\"></a>对象图</h1><p>对象图<br>定义：描述类图的特定实例。不包含操作</p>\n<ul>\n<li>对象：对象是类的实例，是一个封装了状态和行为的实体。<ol>\n<li>对象名：在矩形框的顶端显示。</li>\n<li>类型：具体的类目</li>\n<li>状态：由对象的所有属性以及运行时的当前值组成。</li>\n<li>表示法：在对象名后跟一个冒号加上类型名，并且使用下划线与类进行区分</li>\n</ol>\n</li>\n<li>链：两个或多个对象之间的独立连接。类似于类图中的关联关系<br><a href=\"https://postimg.cc/KRpZ6Nrn\"><img src=\"https://i.postimg.cc/W1ThJYdf/image.png\" alt=\"image.png\"></a></li>\n</ul>\n<h2 id=\"类图与对象图对比\"><a href=\"#类图与对象图对比\" class=\"headerlink\" title=\"类图与对象图对比\"></a>类图与对象图对比</h2><p><a href=\"https://postimg.cc/qtsHvXnD\"><img src=\"https://i.postimg.cc/FKqrGTKv/image.png\" alt=\"image.png\"></a><br><a href=\"https://postimg.cc/bDLXQymQ\"><img src=\"https://i.postimg.cc/vTwQwD9S/image.png\" alt=\"image.png\"></a></p>\n<h1 id=\"时序图\"><a href=\"#时序图\" class=\"headerlink\" title=\"时序图\"></a>时序图</h1><p>定义：通过描述对象间发送消息的时间顺序显示多个对象之间的动态协作状态。元素有角色（Actors）、对象（Object）、生命线（Lifetime）、消息（Message）、激活（Focus of Control）等等</p>\n<ul>\n<li><p>生命线：给每个角色和对象加上一条生命线，就是从角色（或对象）引出向下延伸的虚线，表示时序图存在的时间轴。如果对象被创建或销毁，可以在序列图中开始和&#x2F;或停⽌ (X) 此图定义的持续时间。<br><a href=\"https://postimg.cc/YhZ5Nyzy\"><img src=\"https://i.postimg.cc/bwwqh7D8/image.png\" alt=\"image.png\"></a></p>\n</li>\n<li><p>消息：发送出去的消息叫 Request（请求，用实心箭头表示并加注说明），反馈的消息是 Response（响应，用虚线箭头表示并加返回内容）。</p>\n<ol>\n<li>发送消息有三大类：</li>\n</ol>\n<ul>\n<li>扁平化控制流程：知识下一步进展的发送；异步消息</li>\n<li>过程调用或嵌套控制流：嵌套序列必须结束；封闭序列才能重获控制权</li>\n<li>从过程中返回调用：激活结束时隐含；发送异步和并⾏消息的情况下，可以指⽰执⾏结束<br><a href=\"https://postimg.cc/9DyndXMr\"><img src=\"https://i.postimg.cc/VkhPcdPB/image.png\" alt=\"image.png\"></a></li>\n</ul>\n<ol start=\"2\">\n<li>消息类型</li>\n</ol>\n<ul>\n<li>递归消息：对象多次处于激活状态</li>\n<li>反射消息：对象向自己发送消息<br><a href=\"https://postimg.cc/rRzHxS0k\"><img src=\"https://i.postimg.cc/PJMkGydP/image.png\" alt=\"image.png\"></a></li>\n</ul>\n<ol start=\"3\">\n<li>时间限制：用斜箭头表示不可忽视的时间延迟，可以用时间表达式表示具体的时间</li>\n</ol>\n</li>\n</ul>\n<p><a href=\"https://postimg.cc/0K6tbPF5\"><img src=\"https://i.postimg.cc/2yGg95xW/image.png\" alt=\"image.png\"></a><br>  4. 结构控制</p>\n"}]